{
  "hash": "92d1b475b1e3e7ee014c2ac63da6cf74",
  "result": {
    "markdown": "---\ntitle: \"Time Series Analysis\"\nsubtitle: \"Predicting Value of Unknown Variables\"\nauthor: \"Zach Palmore\"\ndate: \"2022-08-28\"\ncategories: [predictive, modeling, data science]\nimage: \"bullversusbear_image.svg\"\nformat:\n  html:\n    code-fold: true\n    code-tools: true\n    code-link: true\n    highlight-style: pygments\n    html-math-method: katex\n    df-print: paged\n    cache: true\n    theme:\n      light: flatly\n      dark: darkly\n---\n\n\n\n\nA good forecast is a blessing while the wrong forecast could prove to be dangerous\n\n## Introduction\n\nGiven an unknown data source with several groups, we attempt to predict the next 140 values of a times series data set based on 1622 entries provided on multiple events. Our predictions will be fine-tuned to reduce the mean absolute percentage error (MAPE) as much as possible. The packages we will be using and all associated code to produce the models can be found in the attached markdown file. The data with its first five rows, are shown below.  \n\n\n\n\n::: {.cell hash='arima_cache/html/unnamed-chunk-2_3f62647b13cec38924aee01000903691'}\n\n```{.r .cell-code}\n# Data source\ndata <- read.csv(\"https://raw.githubusercontent.com/palmorezm/msds/main/Predictive%20Analytics/Projects/Project1/project1data.csv\")\n# data <- data %>% \n#   rename(SeriesInd = ï..SeriesInd) \nhead(data, 5)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"SeriesInd\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"group\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Var01\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Var02\"],\"name\":[4],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Var03\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Var05\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Var07\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"40669\",\"2\":\"S03\",\"3\":\"30.64286\",\"4\":\"123432400\",\"5\":\"30.34\",\"6\":\"30.49\",\"7\":\"30.57286\",\"_rn_\":\"1\"},{\"1\":\"40669\",\"2\":\"S02\",\"3\":\"10.28000\",\"4\":\"60855800\",\"5\":\"10.05\",\"6\":\"10.17\",\"7\":\"10.28000\",\"_rn_\":\"2\"},{\"1\":\"40669\",\"2\":\"S01\",\"3\":\"26.61000\",\"4\":\"10369300\",\"5\":\"25.89\",\"6\":\"26.20\",\"7\":\"26.01000\",\"_rn_\":\"3\"},{\"1\":\"40669\",\"2\":\"S06\",\"3\":\"27.48000\",\"4\":\"39335700\",\"5\":\"26.82\",\"6\":\"27.02\",\"7\":\"27.32000\",\"_rn_\":\"4\"},{\"1\":\"40669\",\"2\":\"S05\",\"3\":\"69.26000\",\"4\":\"27809100\",\"5\":\"68.19\",\"6\":\"68.72\",\"7\":\"69.15000\",\"_rn_\":\"5\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nWe create forecasts for two preselected variables within each of six predetermined groups. These groups are denoted S01, S02, S03, S04, S05, and S06 respectively. There are five variables within each group that we have to work with. They are Var01, Var02, Var03, Var05, and Var07 respectively. Our date variable ‘SeriesInd,’ is displayed in its numeric serial number form calculated with Excel. Although we do not know what the variables stand for, we can develop models to try and forecast their behavior. This chart contains a breakdown of which variables are forecast in each group.\n\n\n::: {.cell hash='arima_cache/html/unnamed-chunk-3_a2aee77d89dd2b17e2408e80fdb10628'}\n\n```{.r .cell-code}\n# Chart\nvarsbygroup <- data.frame(matrix(c(\"S01\", \"S02\", \"S03\",\n                                   \"S04\", \"S05\", \"S06\", \n                                   \"Var01\", \"Var02\", \"Var05\",\n                                   \"Var01\", \"Var02\", \"Var05\",\n                                   \"Var02\", \"Var03\", \"Var07\",\n                                   \"Var02\", \"Var03\", \"Var07\"),\n                                 nrow = 6, ncol=3))\ncolnames(varsbygroup) <- c(\"Group\", \"Variable1\", \"Variable2\")\nvarsbygroup %>% \n  kbl(booktabs = T) %>% \n  kable_styling(latex_options = c(\"striped\", \"HOLD_position\", \"scale_down\"), full_width = T)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Group </th>\n   <th style=\"text-align:left;\"> Variable1 </th>\n   <th style=\"text-align:left;\"> Variable2 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> S01 </td>\n   <td style=\"text-align:left;\"> Var01 </td>\n   <td style=\"text-align:left;\"> Var02 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> S02 </td>\n   <td style=\"text-align:left;\"> Var02 </td>\n   <td style=\"text-align:left;\"> Var03 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> S03 </td>\n   <td style=\"text-align:left;\"> Var05 </td>\n   <td style=\"text-align:left;\"> Var07 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> S04 </td>\n   <td style=\"text-align:left;\"> Var01 </td>\n   <td style=\"text-align:left;\"> Var02 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> S05 </td>\n   <td style=\"text-align:left;\"> Var02 </td>\n   <td style=\"text-align:left;\"> Var03 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> S06 </td>\n   <td style=\"text-align:left;\"> Var05 </td>\n   <td style=\"text-align:left;\"> Var07 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n```{.r .cell-code}\n# Grouping\nS01 <- data %>% \n  filter(group == \"S01\")\nS02 <- data %>% \n  filter(group == \"S02\")\nS03 <- data %>% \n  filter(group == \"S03\")\nS04 <- data %>% \n  filter(group == \"S04\")\nS05 <- data %>% \n  filter(group == \"S05\")\nS06 <- data %>% \n  filter(group == \"S06\")\n\n# Imputation by function - missing something? lapply/sapply may work \nsoximp <- function(df){\n  for (i in colnames(df)){\n    if (sum(is.na(df[[i]])) !=0){\n      df[[i]][is.na(df[[i]])] <- median(df[[i]], na.rm=TRUE)\n    }\n  }\n}\n\n# Imputation loops for each group by median \nfor (i in colnames(S01)){\n  if (sum(is.na(S01[[i]])) != 0){\n    S01[[i]][is.na(S01[[i]])] <- median(S01[[i]], na.rm = TRUE)\n  } \n}\nfor (i in colnames(S02)){\n  if (sum(is.na(S02[[i]])) != 0){\n    S02[[i]][is.na(S02[[i]])] <- median(S02[[i]], na.rm = TRUE)\n  } \n}\nfor (i in colnames(S03)){\n  if (sum(is.na(S03[[i]])) != 0){\n    S03[[i]][is.na(S03[[i]])] <- median(S03[[i]], na.rm = TRUE)\n  } \n}\nfor (i in colnames(S04)){\n  if (sum(is.na(S04[[i]])) != 0){\n    S04[[i]][is.na(S04[[i]])] <- median(S04[[i]], na.rm = TRUE)\n  } \n}\nfor (i in colnames(S05)){\n  if (sum(is.na(S05[[i]])) != 0){\n    S05[[i]][is.na(S05[[i]])] <- median(S05[[i]], na.rm = TRUE)\n  } \n}\nfor (i in colnames(S06)){\n  if (sum(is.na(S06[[i]])) != 0){\n    S06[[i]][is.na(S06[[i]])] <- median(S06[[i]], na.rm = TRUE)\n  } \n}\n```\n:::\n\n\nBefore we begin, the data is filtered to extract each time series by group. This isolates the Var01, Var02, Var03, Var05, and Var07 variables associated with groups S01, S02, and so on. Then, with each group and its respective variables’ behavior isolated, we clean and adjust the data to make use of it in the analysis. Once we determine the most appropriate models to forecast the proper variable in each group, we evaluate the results of our predictions. Our final forecasts are captured in the excel spreadsheet attached. \n\n## Analysis\n\nWe began by addressing missing values. Given 10,572 observations, about 8% of each variable was missing. Several methods were tried to address this but the best were Kalman smoothing and simple imputation by the median of each ‘Var0X’ variable to fill in where appropriate. The ‘SeriesInd’ numeric date was also converted from its serial number form to a common date-time series. We then examined each group’s variables separately.\n\n\n::: {.cell hash='arima_cache/html/unnamed-chunk-4_27625962b2afd75c13c15d8b97fc1a5b'}\n\n```{.r .cell-code}\n# library(fpp2)\n\n#S01\nS01<-subset(data, group == \"S01\", select = c(SeriesInd, Var01, Var02))%>%\n  mutate(date=as.Date(SeriesInd, origin = \"1905-01-01\"))\nsummary(S01)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   SeriesInd         Var01           Var02               date           \n Min.   :40669   Min.   :23.01   Min.   : 1339900   Min.   :2016-05-07  \n 1st Qu.:41304   1st Qu.:29.85   1st Qu.: 5347550   1st Qu.:2018-01-31  \n Median :41946   Median :35.66   Median : 7895050   Median :2019-11-05  \n Mean   :41945   Mean   :39.41   Mean   : 8907092   Mean   :2019-11-03  \n 3rd Qu.:42586   3rd Qu.:48.70   3rd Qu.:11321675   3rd Qu.:2021-08-06  \n Max.   :43221   Max.   :62.31   Max.   :48477500   Max.   :2023-05-03  \n                 NA's   :142     NA's   :140                            \n```\n:::\n\n```{.r .cell-code}\n# Subset Var01 and Var02 from S01.\nS01_Var01<-S01 %>%select(Var01)\nS01_Var01<-S01_Var01[1:1625,]\n\n\nS01_Var02<-S01 %>%select(Var02)\nS01_Var02<-S01_Var02[1:1625,]\n\n\n#S02\nS02<-subset(data, group == \"S02\", select = c(SeriesInd, Var02, Var03))%>%\n  mutate(date=as.Date(SeriesInd, origin = \"1905-01-01\"))\nsummary(S02)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   SeriesInd         Var02               Var03            date           \n Min.   :40669   Min.   :  7128800   Min.   : 8.82   Min.   :2016-05-07  \n 1st Qu.:41304   1st Qu.: 27880300   1st Qu.:11.82   1st Qu.:2018-01-31  \n Median :41946   Median : 39767500   Median :13.76   Median :2019-11-05  \n Mean   :41945   Mean   : 50633098   Mean   :13.68   Mean   :2019-11-03  \n 3rd Qu.:42586   3rd Qu.: 59050900   3rd Qu.:15.52   3rd Qu.:2021-08-06  \n Max.   :43221   Max.   :480879500   Max.   :38.28   Max.   :2023-05-03  \n                 NA's   :140         NA's   :144                         \n```\n:::\n\n```{.r .cell-code}\n# Subset Var02 and Var03 from S02.\nS02_Var02<-S02 %>%select(Var02)\nS02_Var02<-S02_Var02[1:1625,]\n\n\nS02_Var03<-S02 %>%select(Var03)\nS02_Var03<-S02_Var03[1:1625,]\n\n\n\n#S03\nS03<-subset(data, group == \"S03\", select = c(SeriesInd, Var05, Var07))%>%\n  mutate(date=as.Date(SeriesInd, origin = \"1905-01-01\"))\nsummary(S03)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   SeriesInd         Var05            Var07             date           \n Min.   :40669   Min.   : 27.48   Min.   : 27.44   Min.   :2016-05-07  \n 1st Qu.:41304   1st Qu.: 53.30   1st Qu.: 53.46   1st Qu.:2018-01-31  \n Median :41946   Median : 75.59   Median : 75.71   Median :2019-11-05  \n Mean   :41945   Mean   : 76.90   Mean   : 76.87   Mean   :2019-11-03  \n 3rd Qu.:42586   3rd Qu.: 98.55   3rd Qu.: 98.61   3rd Qu.:2021-08-06  \n Max.   :43221   Max.   :134.46   Max.   :133.00   Max.   :2023-05-03  \n                 NA's   :144      NA's   :144                          \n```\n:::\n\n```{.r .cell-code}\n# Subset Var05 and Var07 from S03.\nS03_Var05<-S03 %>%select(Var05)\nS03_Var05<-S03_Var05[1:1625,]\n\n\nS03_Var07<-S03 %>%select(Var07)\nS03_Var07<-S03_Var07[1:1625,]\n```\n:::\n\n\n\n\nStatistical summaries, box plots, and histograms were run on each group to evaluate where the average value of each variable was, if its distribution was skewed, determine whether outliers were present, and provide other descriptors of the data. These informed us that the average value (mean) of the variables are similar but their range varies widely with Var05 at 186.01 while Var02 covers a range of 479 million. Our analysis solves this potential problem by focusing on variables of the same scales as the intended target.\n\n\n\n\n::: {.cell hash='arima_cache/html/unnamed-chunk-6_836de3a48c46be37c92f2b1a9d91ff6d'}\n\n```{.r .cell-code}\nsummary(S01_Var01)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  23.01   29.85   35.72   39.47   48.76   62.38 \n```\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1,2))\nhist(ts_S01_Var01)\nboxplot(ts_S01_Var01)\n```\n\n::: {.cell-output-display}\n![](arima_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\nautoplot(ts_S01_Var01)\n```\n\n::: {.cell-output-display}\n![](arima_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n:::\n\n\n\nAdditionally, all but group S03 of the histograms exhibited right skewness, and Var02 and Var03 had outliers. These were replaced using Friedman’s super smoothing method. Due to the randomness of these variables, determining outliers was difficult and there is a presence of additional overly influential points as determined using Cook's distance formula. We acknowledge the presence of these points but are unable to alter them as they are likely intentional based on the patterns in the data. For reference, the observations are shown in the scatter plot with color coding by each group. \n\n\n\n::: {.cell hash='arima_cache/html/unnamed-chunk-7_beb6c22858d2fbd07d84f47dc3d4a8f3'}\n\n```{.r .cell-code}\ndata[c(1:7)]%>%\n  gather(variable, value, -SeriesInd, -group) %>%\n  ggplot(., aes(value, SeriesInd, color = group)) + \n  geom_point(fill = \"white\",\n             size=1, \n             shape=21, \n             alpha = 0.75) + \n  coord_flip() + \n   facet_wrap(~variable, \n             scales =\"free\") + \n  labs(title = \"Variable Patterns\", \n       subtitle = \"Color Coded by Group\", \n       x=\"Value\", \n       y=\"Time\", \n       caption = \"Contains all non-null observations of the given data set\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust=0.5), \n        plot.subtitle = element_text(hjust=0.5),\n        legend.position = \"bottom\", \n        axis.ticks.x=element_blank(),\n        axis.text.x=element_blank(), \n        plot.caption = element_text(hjust=0.5)\n        )\n```\n\n::: {.cell-output-display}\n![](arima_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nSeasonality was also considered. It is possible this data follows a weak seasonal trend that increases during summer months but there is not a lot of evidence to support regular fluctuations. Regular gaps were noticed in the time series on a weekly basis and several methods were used in attempts to fix this. However, the data appears randomly distributed and as such, acts randomly. For this reason, we left the gaps alone and any further adjustments made were minimal to avoid disturbing any existing patterns in the data. \n\n\n\n\n\n\nWe determined that the best model type was an Auto Regressive Integrated Moving Average (ARIMA) with drift. Unfortunately, all variables required differencing to achieve stationarity. This indicates that any predictions made with these variables may be unrealistic because of inherent random changes in statistics like the mean and variance of these variables over time. We transform the data in our attempts to achieve stationarity but it should be noted that our review of stationarity is only a rough estimate using the aforementioned summary statistics so that we may apply this ARIMA method. Otherwise, we would have to conclude this data is inherently unpredictable and as such, render model forecasts useless. Rather, we focus on forecasting each variable individually and try to keep it simple.\n\n\n\n## Prediction\n\n\n\n\n\n\n::: {.cell hash='arima_cache/html/unnamed-chunk-11_504adfdaf18db74481a10239653de8fc'}\n\n```{.r .cell-code}\nfcast0101 %>%\n  as.data.frame() %>%\n  ggplot(aes(x = seq(1:length(`Hi 95`)))) + \n  geom_ribbon(aes(ymin=`Lo 95`,ymax=`Hi 95`), fill=\"forest green\", alpha=0.5) + \n  geom_ribbon(aes(ymin=`Lo 80`,ymax=`Hi 80`), fill=\"dark green\", alpha=0.5) +\n  geom_line(aes(y = `Hi 95`), color = \"grey\", size=2, alpha = .5) + \n  geom_line(aes(y = `Point Forecast`), color = \"#000000\", size=1, lty = 1, alpha = .5) + \n  geom_line(aes(y = `Lo 95`), color = \"grey\", size=2, alpha = .5) + \n  geom_vline(xintercept = b1, lty = 3) +\n  geom_vline(xintercept = b2, lty = 3) +\n  geom_vline(xintercept = b3, lty = 3) +\n  geom_segment(aes(x = 0, y = extract[which(extract$Day == b1),]$`Hi 95`, \n                   xend = b1, yend = extract[which(extract$Day == b1),]$`Hi 95`), lty = 3) +\n  geom_segment(aes(x = 0, y = extract[which(extract$Day == b1),]$`Lo 95`, \n                   xend = b1, yend = extract[which(extract$Day == b1),]$`Lo 95`), lty = 3) +\n  geom_segment(aes(x = 0, y = extract[which(extract$Day == b2),]$`Hi 95`, \n                   xend = b2, yend = extract[which(extract$Day == b2),]$`Hi 95`), lty = 3) +\n  geom_segment(aes(x = 0, y = extract[which(extract$Day == b2),]$`Lo 95`, \n                   xend = b2, yend = extract[which(extract$Day == b2),]$`Lo 95`), lty = 3) +\n  geom_segment(aes(x = 0, y = extract[which(extract$Day == b3),]$`Hi 95`, \n                   xend = b3, yend = extract[which(extract$Day == b3),]$`Hi 95`), lty = 3) +\n  geom_segment(aes(x = 0, y = extract[which(extract$Day == b3),]$`Lo 95`, \n                   xend = b3, yend = extract[which(extract$Day == b3),]$`Lo 95`), lty = 3) +\n  geom_segment(aes(x = 0, y = extract[which(extract$Day == b4),]$`Hi 95`, \n                   xend = b4, yend = extract[which(extract$Day == b4),]$`Hi 95`), lty = 3) +\n  geom_segment(aes(x = 0, y = extract[which(extract$Day == b4),]$`Lo 95`, \n                   xend = b4, yend = extract[which(extract$Day == b4),]$`Lo 95`), lty = 3) +\n  scale_x_continuous(expand = c(0, 0), \n                     limits=c(0,100), \n                     breaks = c(0, b1, b2, b3, b4)) +\n  scale_y_continuous(expand = c(0, 0), \n                     limits=c(min(fcast0101$lower), max(fcast0101$upper)), \n                     breaks = c(round(fcast0101$mean[[1]], digits = 2),\n                                round(extract[which(extract$Day == b1),]$`Hi 95`, 2), \n                                round(extract[which(extract$Day == b1),]$`Lo 95`, 2),\n                                round(extract[which(extract$Day == b2),]$`Hi 95`, 2),\n                                round(extract[which(extract$Day == b2),]$`Lo 95`, 2), \n                                round(extract[which(extract$Day == b3),]$`Hi 95`, 2),\n                                round(extract[which(extract$Day == b3),]$`Lo 95`, 2), \n                                round(extract[which(extract$Day == b4),]$`Hi 95`, 2),\n                                round(extract[which(extract$Day == b4),]$`Lo 95`, 2))) +\n  annotate(\"text\", x = 7.5, y =53.9, label = paste0(\"MAPE =\", signif(mape0101, 4))) +\n  annotate(\"label\",x=txtx90,y=extract[which(extract$Day == txtx90),]$`Point Forecast` + 3.50, \n           label = \"95% Confidence\" ) + \n  annotate(\"segment\", x = txtx90, y = mean(extract$`Point Forecast`) + 4.75, \n           xend = txtx90, \n           yend = extract[which(extract$Day == txtx90),]$`Hi 95`,\n         arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\"))) +\n  annotate(\"segment\", x = txtx90, y = mean(extract$`Point Forecast`) + 3, \n           xend = txtx90, \n           yend = extract[which(extract$Day == txtx90),]$`Lo 95`,\n         arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\"))) + \n  annotate(\"label\",x=txtx80, y=extract[which(extract$Day == txtx90),]$`Point Forecast` - 3.01, \n           label = \"80% Confidence\" ) +\n  annotate(\"segment\", x = txtx80, y = mean(extract$`Point Forecast`) - 1.75, \n           xend = txtx80, \n           yend = extract[which(extract$Day == txtx80),]$`Hi 80`,\n         arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\"))) +\n  annotate(\"segment\", x = txtx80, y = mean(extract$`Point Forecast`) -3.5, \n           xend = txtx80, \n           yend = extract[which(extract$Day == txtx80),]$`Lo 80`,\n         arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\"))) +\n  annotate(\"label\",x=b1 - 7.05, y=mean(extract$`Point Forecast`) + 6, \n           label = paste(\"Day 15:\", signif(extract[which(extract$Day == b1),]$`Point Forecast`, 4))) +\n  annotate(\"segment\", x = b1 - 7, y = mean(extract$`Point Forecast`) + 5, \n           yend = extract[which(extract$Day == b1),]$`Point Forecast`,\n           xend = b1, \n         arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\")))  +\n  annotate(\"label\",x=b2 - 5, y=mean(extract$`Point Forecast`) + 9, \n           label = paste(\"Day 30:\", signif(extract[which(extract$Day == b2),]$`Point Forecast`, 4))) +\n  annotate(\"segment\", x = b2 - 4, y = mean(extract$`Point Forecast`) + 8.0, \n           yend = extract[which(extract$Day == b2),]$`Point Forecast`,\n           xend = b2, \n         arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\")))  +\n  annotate(\"label\",x=b3 - 10, y=mean(extract$`Point Forecast`) + 11, \n           label = paste(\"Day 60:\", signif(extract[which(extract$Day == b3),]$`Point Forecast`, 4))) +\n  annotate(\"segment\", x = b3 - 10, y = mean(extract$`Point Forecast`) + 9.75, \n           yend = extract[which(extract$Day == b3),]$`Point Forecast`,\n           xend = b3, \n         arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\"))) +\n  labs(x = \"Day\", y = \"Value\", \n       title = \"Value Estimation is Less Certain with Time\", \n       subtitle = \"Stock Market Day-Value Pairs over 100 Days Shows Growth of Uncertainty\") + \n  theme_minimal() + theme(plot.title = element_text(hjust = 0.5), \n                          plot.subtitle = element_text(hjust = 0.5))\n```\n\n::: {.cell-output-display}\n![](arima_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\r\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\r\n<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}