[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Zach Palmore is currently working as a Data Scientist in Rock County, WI. When not innovating data pipelines, he enjoys spending time gardening and making homemade pizzas.\nM.S. Data Science"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Risk Modeling and Prediction",
    "section": "",
    "text": "A Credit Risk Assessment for Lenders\n\n\n\n\npredictive\n\n\nmodeling\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2022\n\n\nZach Palmore\n\n\n\n\n\n\n  \n\n\n\n\n\nPredicting Value of Unknown Variables\n\n\n\n\npredictive\n\n\nmodeling\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2022\n\n\nZach Palmore\n\n\n\n\n\n\n  \n\n\n\n\n\nAn Auto Insurance Example\n\n\n\n\npredictive\n\n\nmodeling\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2022\n\n\nZach Palmore\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Insurance Claim Prediction/index.html#the-challenge",
    "href": "posts/Insurance Claim Prediction/index.html#the-challenge",
    "title": "Claim Estimation",
    "section": "The Challenge",
    "text": "The Challenge\nA key part of insurance is charging each customer the appropriate price for the risk they represent.\nThe challenge is knowing what is most likely to happen before it occurs. If we have confidence in what could occur, then we may better manage our risk. Data science offers a solution to this challenge. Developing advanced computational models with the capacity to crunch high volume data allows us to calculate probabilities of risk for many individuals at scale.\nThe business need is to predict the probability that a person will submit a claim and then estimate that claim amount. Multiple linear regression and binary logistic regression models were built to answer these questions. We explore, analyze and model with a data set containing records of customer behavior at an auto insurance company. A table with a selection of five variables that represent behavioral characteristics from customer records is shown below with a brief description of each for reference.\n\n\nCode\n# short descriptions of variables as table from matrix\nvardesc <- data.frame(matrix(c(\n'TARGET_FLAG',  'Was a claim submitted? 1 = Yes, 0 = No',\n'TARGET_AMT',   'Estimated amount of claim',\n'CLM_FREQ', 'Number of claims filed in past five years',\n'MVR_PTS',  'Motor vehicle inspection points',\n'TRAVETIME',    'Distance to work in minutes'\n),  byrow = TRUE, ncol = 2))\ncolnames(vardesc) <- c('Variable', 'Description')\nkbl(vardesc, booktabs = T, caption = \"Variable Descriptions\") %>%\n  kable_styling(latex_options = c(\"striped\", \"HOLD_position\"), full_width = F)\n\n\n\n\nVariable Descriptions\n \n  \n    Variable \n    Description \n  \n \n\n  \n    TARGET_FLAG \n    Was a claim submitted? 1 = Yes, 0 = No \n  \n  \n    TARGET_AMT \n    Estimated amount of claim \n  \n  \n    CLM_FREQ \n    Number of claims filed in past five years \n  \n  \n    MVR_PTS \n    Motor vehicle inspection points \n  \n  \n    TRAVETIME \n    Distance to work in minutes"
  },
  {
    "objectID": "posts/Insurance Claim Prediction/index.html#data-prep",
    "href": "posts/Insurance Claim Prediction/index.html#data-prep",
    "title": "Claim Estimation",
    "section": "Data Prep",
    "text": "Data Prep\nThere are over 8000 records in this data set, with 26 variables in each record. We begin by loading in the data and exploring.\nWhen exploring we look for missing values and abnormalities and try to find patterns in the records. We calculate some descriptive and inferential statistics that show the characteristics present in the data. These statistics inform us if imputations, transformations, or other adjustments are needed. For example, this violin plot with a box plot inlay, provides a wealth of information.\n\n\nKernel Density Estimation & Outliers\ntdata %>% \n  select_if(is.numeric) %>% \n  gather() %>% \n  ggplot(aes(value, key)) +\n  facet_wrap(~ key, scales = \"free\") +\n  geom_violin(aes(color = key, alpha = 1)) + \n  geom_boxplot(aes(fill = key, alpha = .5), notch = TRUE, size = .1, lty = 3) +  \n  stat_summary(fun.y = mean, geom = \"point\",\n               shape = 8, size = 1.5, color = \"#000000\") + \n  theme_minimal() + \n  theme(axis.text = element_blank(), \n        axis.title = element_blank(), \n        legend.position = \"none\", plot.title = element_text(hjust = 0.5)) + \n  labs(title = \"Kernel Density Estimation & Outliers\") \n\n\n\n\n\nPerhaps one of the easiest things to spot in this plot are outliers, shown as grey dots on the left and right sides of the variables. These points are located far from where the rest of the data is, and there are so many that some dots appear solid black due to repeated grey dots plotted in the same spot. Our target amount, the value we intend to predict, is a great example of a distribution with a lot of outliers. These points give a good idea of what ‘normal’ is for each variable but this plot offers some more.\nIn this plot the asterisk near the center of each boxplot is the median of the distribution for all records that contain the variable named, however, what exactly the median is less important than where it lies within the distribution. The top and bottom edges (the curvy colored lines) of the violin plot use the non-parametric method kernel density estimation (KDE) to smoothly estimate the probability of a value occurring anywhere along its range. Here again, we already notice problems with our target amount. Although not every variable follows this pattern. For these reasons, we need to be careful about how we handle this data before making any predictions.\nAnother method used to explore and prepare the data is a correlation plot. These examine the strength of relationships between variables, whether they are positive or negative, and how they compare to one another. We take a look at some selected variables in hopes that they confirm some expectations.\n\n\nCorrelation Plot\ntdata %>%\n  select_if(is.numeric) %>% \n  cor() %>% \n  ggcorrplot(method = \"circle\", type=\"upper\", \n             ggtheme = ggplot2::theme_minimal, legend.title = \"Influence\") + coord_flip() \n\n\n\n\n\nIf we are trying to predict the amount of a claim and we have a true or false variable indicating the presence or absences of claim submission (TARGET_FLAG) and the claim amount (TARGET_AMT), shouldn’t we expect the two to be correlated? I would hope so, given that, when an individual does not file a claim, the resultant claim amount is 0. We notice this in the big red circle towards the bottom left of the screen. This shows a strong positive correlation between the amount of a claim and presence of a claim. From this correlation plot we start to confirm these expectations and validate some conventional auto insurance knowledge.\nThese demonstrate only two ways to look at data before model building. However, to ensure that a model functions in the real world, a multitude of exploratory methods should be used to fully understand the data. To keep it brief, the data is split 70-30 into training and testing data sets, then cleaned up with using the multiple imputation by chained equations (MICE) method, perform a Yeo-Johnson transformation, and adjust other points as necessary to make the non-normal predictor appear normal enough for prediction. A quick look at the imputed summary statistics is shown for four numeric variables as a reference.\n\n\nImputed Summary Statistics\nimputed.stats.table <- data.frame(matrix(c(0,29707,54028,61469,83304,367030, \n         0,0,161160,155225,233352,885282,\n         1500,9280,14440,15710,20850,69740,\n         0,0,0,4037,4636,57037), ncol = 4, byrow = F)) \nnames(imputed.stats.table) <- c(\"Income\", \"Home Value\", \n                                \"Bluebook Value\", \"Old Claims\")\nimputed.stats.table <- imputed.stats.table %>% \n  mutate(Statistic = c(\"Min\", \"1st Quartile\", \"Median\", \n                       \"Mean\", \"3rd Qartile\", \"Max\")) %>%\n  dplyr::select(\"Statistic\", \"Income\", \"Home Value\", \n                \"Bluebook Value\", \"Old Claims\")\nkbl(imputed.stats.table,\n    booktabs = T,\n    caption = \"Imputed Summary Statistics\") %>%\n  kable_styling(latex_options = c(\"striped\", \"hold_position\"),\n                full_width = F)\n\n\n\n\nImputed Summary Statistics\n \n  \n    Statistic \n    Income \n    Home Value \n    Bluebook Value \n    Old Claims \n  \n \n\n  \n    Min \n    0 \n    0 \n    1500 \n    0 \n  \n  \n    1st Quartile \n    29707 \n    0 \n    9280 \n    0 \n  \n  \n    Median \n    54028 \n    161160 \n    14440 \n    0 \n  \n  \n    Mean \n    61469 \n    155225 \n    15710 \n    4037 \n  \n  \n    3rd Qartile \n    83304 \n    233352 \n    20850 \n    4636 \n  \n  \n    Max \n    367030 \n    885282 \n    69740 \n    57037"
  },
  {
    "objectID": "posts/Insurance Claim Prediction/index.html#model-building",
    "href": "posts/Insurance Claim Prediction/index.html#model-building",
    "title": "Claim Estimation",
    "section": "Model Building",
    "text": "Model Building\nOur objective was to build multiple linear regression and binomial logistic regression models to predict the amount of auto insurance claim. We explored, analyzed, and prepared the data as best we could in an attempt to improve the predictive outcome. The manner in which these were built is detailed in the Caret Model code chunk.\n\n\nCaret Models\n# Model 1: Establish Baseline\n# This models uses only previous accident as a predictor\nmodel1 <- glm(TARGET_FLAG ~ previous_accident, \n              family = binomial(link = \"logit\"), train)\n# Model 2: Experimentally Determine Best Features by Hand\n# These features were selected by alpha level and intuition\nmodel2 <- glm(TARGET_FLAG ~ previous_accident + \n                city + young + clean_rec + \n                educated, family = binomial(link = \"logit\"), train)\n# Model 3: Add Recommended Risk Predictors from III\n# This model takes another step towards improving accuracy \nmodel3 <- glm(TARGET_FLAG ~ previous_accident + \n                city + mstatus + income.values + \n                sex + car_use + educated + KIDSDRIV + \n                revoked, family = binomial(link = \"logit\"), \n              train)\n# Model 4: All in One\n# Examine results with all variables included, what worked?\nmodel4 <- lm(target_amt ~ ., train) \n# Model 5: Linear Regression with Dollar Estimators\n# This model leans heavy on the variables with specific dollar figures\nmodel5 <- lm(target_amt ~ income.values +\n               home.values + bluebook.values + \n               oldclaim.values + avg_claim, \n             train) \n# Model 6: Multidirectional StepAIC Regression\n# This uses a Stepwise Akaike Information Criteria to evaluate \n# and select predictors in the model with some special data prep\nmodel6 <- lm(target_amt ~ . -TARGET_AMT -TARGET_FLAG, train) \npm <- stepAIC(model6, trace = F, direction = \"both\")\n\n\nAnother model wherein everything but the kitchen sink was thrown at it (aptly known as the kitchen sink model), gave us insights into which variables were significant to use and what their effect on the model would probably be. Of course, with a model that contains over 30 variables, there is room for some complex interactions to occur. We rely on our exploration and analysis to guide us in the creation of additional models alongside the results from the kitchen sink model, historical model, and conventional wisdom from the auto insurance domain.\nSince we humans tend to poorly judge relationships represented as mathematical operations, a stepwise AIC model was created to do a lot of work for us. This model performs a check by cycling through the variables both forwards and backwards to pick the variables that are most likely to improve the predictive capacity of the model. The AIC just stands for Akaike Information Criterion which is an estimator of prediction error. In this model, when stepping (or cycling) through variables, we are using this criterion to select variables that reduce the amount of error present in the model. Ideally, this will improve model quality and output.\nWe finish with some general predictions of each model type. We created a historical model, kitchen sink model, a multidirectional stepwiseAIC, backward stepwiseAIC, high risk predictor model, and a conventional wisdom model using information from the Insurance Information Institute (III). These will be put to the test in the model selection process. Which one do you think will perform best?"
  },
  {
    "objectID": "posts/Insurance Claim Prediction/index.html#model-selection",
    "href": "posts/Insurance Claim Prediction/index.html#model-selection",
    "title": "Claim Estimation",
    "section": "Model Selection",
    "text": "Model Selection\nTo standardize the process of model selection, a function is created wherein all statistics are computed the same way. It begins by bringing in the holdout data we created in the data prep section. This is named the test data. With this we evaluate how accurately the model predicted the result. We then compute a confusion matrix which contains the rates of false positives, true positives, false negatives, and true negatives and estimate model specificity, sensitivity, precision, and recall with those values. These combined measures let us find the F1 score, and from them we can create a receiver operating characteristic (ROC) plot and find the area under the curve (AUC). These provide details about the accuracy and real-world effectiveness of a model.\nAs a quick reference, we show the results from one of the best models, that of the multidirectional stepwiseAIC. The receiver operating characteristic curve is plotted alongside text containing the accuracy, its bounds, how precise and sensitive it is as well as multiple significance values including McNemars p-value and an accuracy p-value. Each model went through this evaluation process.\n\n\nModel Summary\n# Create Function to Evaluate All Models\nmodstat <- function(model, test, \n                    target = \"TARGET_FLAG\", threshold = 0.5){\n  \n  # test model using predictions with test data\n  test$new <- ifelse(predict.glm(\n    model, test, \"response\") >= threshold, 1, 0) \n    \n    # create confusion matrix with stats\n    # shows true positive, false positive, and their inverse\n    cm <- confusionMatrix(factor(test$new), \n                          factor(test[[target]]), \"1\")\n    \n    # Organize information into data frame\n    df <- data.frame(obs = test$TARGET_FLAG, \n                     predicted = test$new, \n                     probs = predict(model, test))\n  \n  # Calculate performance and significance values\n  Pscores <- prediction(df$probs, \n                        df$obs)\n  \n  # AUC = \"Area Under the Curve\" \n  AUC <- performance(Pscores, \n                     measure = \"auc\")@y.values[[1]]\n    pscores <- performance(Pscores, \n                           \"tpr\", \"fpr\")\n  \n  # Plot the scores of true positive/ false positive\n    # This is a receiver operating characteristic (ROC) curve \n  plot(pscores,main=\"ROC Curve\", \n       sub = paste0(\"AUC: \", \n                    round(AUC, 3)))\n  \n  # Extract the F1 score \n    # place it below the plot for each model when run\n  results <- paste(cat(\"F1 = \", \n                       cm$byClass[7], \" \"), cm)\n  \n  # Output results with a ROC curve and all scores \n  return(results)\n}\n\n# Calculate and show ONLY model 6 for quick reference\nmodstat(model6, test)\n\n\n\n\n\nF1 =  0.4838057  \n\n\n[1] \" 1\"                                                                                                                                                                                                                                                                                                                                                                                             \n[2] \" c(1699, 113, 397, 239)\"                                                                                                                                                                                                                                                                                                                                                                        \n[3] \" c(Accuracy = 0.791666666666667, Kappa = 0.36653677545056, AccuracyLower = 0.775030346732191, AccuracyUpper = 0.807602177444679, AccuracyNull = 0.740196078431373, AccuracyPValue = 1.62575683811825e-09, McnemarPValue = 5.02353275353728e-36)\"                                                                                                                                                \n[4] \" c(Sensitivity = 0.375786163522013, Specificity = 0.937637969094923, `Pos Pred Value` = 0.678977272727273, `Neg Pred Value` = 0.810591603053435, Precision = 0.678977272727273, Recall = 0.375786163522013, F1 = 0.483805668016194, Prevalence = 0.259803921568627, `Detection Rate` = 0.0976307189542484, `Detection Prevalence` = 0.143790849673203, `Balanced Accuracy` = 0.656712066308468)\"\n[5] \" sens_spec\"                                                                                                                                                                                                                                                                                                                                                                                     \n[6] \" list()\""
  },
  {
    "objectID": "posts/Insurance Claim Prediction/index.html#conclusion",
    "href": "posts/Insurance Claim Prediction/index.html#conclusion",
    "title": "Claim Estimation",
    "section": "Conclusion",
    "text": "Conclusion\nRisk varies widely from customer to customer, and a deep understanding of different risk factors helps predict the likelihood and cost of insurance claims. Many factors contribute to the frequency and severity of car accidents including how, where and under what conditions people drive, as well as what they are driving. We developed 6 models. Half of these were multiple linear regression models and the other half were binomial logistic regression. Both have their benefits in the right context.\n\n\nComparison Table\nmod.stats.table <- data.frame(matrix(c(\n         \"Model 1\", 0.722, 0.757, 0.5, .01, \n         \"Model 2\", 0.732, 0.767, 0.58, 0.333,\n         \"Model 3\", 0.754, 0.788, 0.625, 0.422,\n         \"Model 4\", 0.998, 0.999, 0.999, 0.999, \n         \"Model 5\", 0.719, 0.753, 0.514, .090,\n         \"Model 6\", 0.775, 0.808, 0.657, 0.484),\n         nrow = 6, byrow = T))\ncolnames(mod.stats.table) <- c(\"ID\", \"Lower Bound\", \"Upper Bound\", \n                               \"Balanced Accuracy\", \"F1 Score\")\nmod.stats.table\n\n\n\n\n  \n\n\n\nWith an accuracy between 78 - 81%, the multidirectional stepwiseAIC model wins the contest between these two model types. It was also the most useful real-world model with a balanced accuracy at 65.7%. Its F1 score was 0.484, indicating the relationship between precision (how well it predicts true positives) and recall (ratio of correct positives in the predictions) is better than any other method of modeling.\nIn this table we reviewed the model’s lower and upper accuracy bounds, its balanced accuracy, and F1 scores. The results show Model 3, our other stepwiseAIC, is the runner up. Model 4 is the only model that appears to have been too effective, and is not realistic for a variety of reasons. Perhaps the most important being over-fitting.\nOf these factors in prediction, balanced accuracy is likely the best criteria to judge the models on in this scenario. This is because it finds arithmetic mean of sensitivity and specificity which tends to represent imbalanced data better than accuracy alone. Since our data set was highly imbalanced and the target class of claim amount appeared much less than the non-target class, this accuracy estimate helps balance expectations.\nOf course, these predictions could be improved. When building models, it may be a good idea to perform some feature engineering to better isolate the riskiest and least risky customers. Other model types may also offer new insights.\nFor more, view the full report on my GitHub page."
  },
  {
    "objectID": "posts/Loan Assessment/LendingApproval.html",
    "href": "posts/Loan Assessment/LendingApproval.html",
    "title": "Loan Approval",
    "section": "",
    "text": "The main source of income of lenders stems from their credit line. Can it be improved?"
  },
  {
    "objectID": "posts/Loan Assessment/LendingApproval.html#data-characteristics",
    "href": "posts/Loan Assessment/LendingApproval.html#data-characteristics",
    "title": "Loan Approval",
    "section": "Data Characteristics",
    "text": "Data Characteristics\nThere are 614 observations of 12 variables. Each observation is an applicant’s application for a loan with its corresponding variables of interest. Below is the description of the variables of interest in the data set.\n\n\n\nVARIABLE NAME\nDESCRIPTION\n\n\n\n\nLoan_ID\nUnique Loan ID\n\n\nGender\nMale/ Female\n\n\nMarried\nApplicant married (Y/N)\n\n\nDependents\nNumber of dependents\n\n\nEducation\nApplicant Education (Graduate/ Undergraduate)\n\n\nSelf_Employed\nSelf employed (Y/N)\n\n\nApplicantIncome\nApplicant income\n\n\nCoapplicantIncome\nCoapplicant income\n\n\nLoanAmount\nLoan amount in thousands\n\n\nLoan_Amount_Term\nTerm of loan in months\n\n\nCredit_History\ncredit history meets guidelines\n\n\nProperty_Area\nUrban/ Semi Urban/ Rural\n\n\nLoan_Status\nLoan approved (Y/N)\n\n\n\nThere are four numeric variables represented by loan amount, loan amount term, applicant and co-applicant income. Several of these variables appear to be factors with specific levels but are not coded as such. For example, Gender, Married, Dependents, Education, Self_Employed, Property_Area, Credit_History, and Loan_Status are character strings. We will need to fix this if we are to make use of them.\n\n\nCode\n# read data, change blank to NA and and remove loan_id\nloan_data <- read.csv('https://raw.githubusercontent.com/amit-kapoor/Data622Group2/main/Loan_approval.csv') %>% \n  na_if(\"\") %>%\n  dplyr::select(-1)\n\n# categorical columns as factors\nloan_data <- loan_data %>% \n  mutate(Gender=as.factor(Gender),\n         Married=as.factor(Married),\n         Dependents=as.factor(Dependents),\n         Education=as.factor(Education),\n         Self_Employed=as.factor(Self_Employed),\n         Property_Area=as.factor(Property_Area),\n         Credit_History=as.factor(Credit_History),\n         Loan_Status=as.factor(Loan_Status))"
  },
  {
    "objectID": "posts/Loan Assessment/LendingApproval.html#data-summary",
    "href": "posts/Loan Assessment/LendingApproval.html#data-summary",
    "title": "Loan Approval",
    "section": "Data summary",
    "text": "Data summary\nBelow is a summary of the loan approval dataset. For this process we have already adjusted the data types to their proper forms. This summarizing function quantifies each variable in a manner consistent with their types. We notice the levels of each factor in the ‘Stats/Values’ column, the frequency of valid (non-missing) observations per level of our factors, and the quantity and percent missing alongside them. We review these statistics to identify any issues with each variable.\n\n\nCode\ndfSummary(loan_data, style = 'grid', graph.col = FALSE)\n\n\n\n\n  \n\n\n\nThere are 7 columns that have missing values. The proportion of values for several columns shows significant differences and skew. For example, 97.9% of this dataset contains males applicants based on observations of the Gender variable, 99.5% of applicants are married people given the Married variable, and over 90% of our observations have longer Credit_History. Due to the disproportionate levels within the variables we should expect the data is not representative of a larger population unless that population happens to have similar proportions.\nOur numeric incomes variables show significant signs of skew through the differences in their mean and medians as well as their ranges. The lowest applicant income was 150, while the highest was 81000. A similar problem exists with our co-applicant income data having had individuals with 0 income on the lowest end of the range and 41667 on the highest.\nHowever, all of the observations contained an applicant and co-applicant income. Since some applicants may not have used a co-applicant on their applications, part of this skew could be caused by the data collection process. Additionally, we are only missing 3.6% of the observations of loan amount and 2.3% for loan terms.\nThere are regular intervals and commonality in the loan term amounts which indicates we may have been able to factorize their data types. We chose instead to leave it as a discrete numeric value since it represents the term length which could be any number of days or months. We note that 85.3% percent of these applicants applied for a loan term of 360 but we are unsure if that is due to the lending institutions standard practice or if applicants requested this specific term.\nFor exploratory purposes, we visualize the proportions to see just how skewed and disproportionate this dataset is. We include missing values to demonstrate their influence on the dataset as well. The chart below shows the distribution of all categorical variables, which includes the factors mentioned previously.\n\n\nCode\n# select categorical columns\ncat_cols = c()\nj <- 1\nfor (i in 1:ncol(loan_data)) {\n  if (class((loan_data[,i])) == 'factor') {\n      cat_cols[j]=names(loan_data[i])\n      j <- j+1\n  }\n}\n\nloan_fact <-  loan_data[cat_cols]\n# long format\nloan_factm <- melt(loan_fact, measure.vars = cat_cols, variable.name = 'metric', value.name = 'value')\n\n# plot categorical columns\nggplot(loan_factm, aes(x = value)) + \n  geom_bar() + \n  scale_fill_brewer(palette = \"Set1\") + \n  facet_wrap( ~ metric, nrow = 5L, scales = 'free') + coord_flip()\n\n\n\n\n\nFrom this chart, it is very clear we have a dataset with mostly married male graduates with no dependents, a long credit history, and who are not self-employed. There is a relatively even mix of urban, suburban, and rural applicants and a small number of missing values. Applicants tend to be accepted more often than not and there are no missing observations for our target variable ‘Loan_Status’ nor the applicant’s property area or education. These are all of our categorical variables.\nWe also generate histograms with the count of each observation to assess our numeric variable distributions. This will let us know more about the skewness, average values, and where potential outliers may be found for our numeric variables. The graph below shows their distributions.\n\n\nCode\nplot_histogram(loan_data, geom_histogram_args = list(\"fill\" = \"tomato4\"))\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe applicant income and co-applicant income variables are highly right skewed with a smaller number of individual applicants stretching the distribution towards higher incomes. For analysis purposes, we must keep in mind that only a handful of applicants had higher incomes while the bulk of applicants were concentrated at the lower end of the income distribution. The loan amount term has one spike at 360. Meanwhile, the loan amount is the closest to normal. These results are consistent with our summary table.\nNext we will review the impact of the categorical variables’ proportions on loan approval in more detail by isolating the factor levels individually. Here again, we visualize the proportions as a bar chart without missing values and expand the size of the chart to see the nuances of each. These are placed alongside each variable’s frequency table by level to visualize their proportions. The results are as follows:\n\n\nCode\nloan_ch <- with(loan_data, table(Credit_History, Loan_Status)) %>% \n  prop.table(margin = 1) %>% as.data.frame() %>% filter(Loan_Status == 'Y')\n\nloan_ch\n\n\n\n\n  \n\n\n\n\n\nCode\nggplot(loan_ch, aes(x=Credit_History, y=Freq, fill=Credit_History)) + geom_bar(stat='identity') + labs(title = 'Approved Loans by Credit History', y = \"Percentage\", x = \"Credit History\")\n\n\n\n\n\n\n\nCode\nloan_gen <- with(loan_data, table(Gender, Loan_Status)) %>% \n  prop.table(margin = 1) %>% as.data.frame() %>% filter(Loan_Status == 'Y')\n\nloan_gen\n\n\n\n\n  \n\n\n\n\n\nCode\nggplot(loan_gen, aes(x=Gender, y=Freq, fill=Gender)) + geom_bar(stat='identity') + labs(title = 'Approved Loans by Gender', y = \"Percentage\", x = \"Gender\")\n\n\n\n\n\n\n\nCode\nloan_ed <- with(loan_data, table(Education, Loan_Status)) %>% \n  prop.table(margin = 1) %>% as.data.frame() %>% filter(Loan_Status == 'Y')\n\nloan_ed\n\n\n\n\n  \n\n\n\n\n\nCode\nggplot(loan_ed, aes(x=Education, y=Freq, fill=Education)) + geom_bar(stat='identity') + labs(title = 'Approved Loans by Education', y = \"Percentage\", x = \"Education\")\n\n\n\n\n\n\n\nCode\nloan_mar <- with(loan_data, table(Married, Loan_Status)) %>% \n  prop.table(margin = 1) %>% as.data.frame() %>% filter(Loan_Status == 'Y')\n\nloan_mar\n\n\n\n\n  \n\n\n\n\n\nCode\nggplot(loan_mar, aes(x=Married, y=Freq, fill=Married)) + geom_bar(stat='identity') + labs(title = 'Approved Loans by Married', y = \"Percentage\", x = \"Married\")\n\n\n\n\n\n\n\nCode\nloan_dep <- with(loan_data, table(Dependents, Loan_Status)) %>% \n  prop.table(margin = 1) %>% as.data.frame() %>% filter(Loan_Status == 'Y')\n\nloan_dep\n\n\n\n\n  \n\n\n\n\n\nCode\nggplot(loan_dep, aes(x=Dependents, y=Freq, fill=Dependents)) + geom_bar(stat='identity') + labs(title = 'Approved Loans by Dependents', y = \"Percentage\", x = \"Dependents\")\n\n\n\n\n\nThese bar charts confirm our thoughts about the dataset’s disproportionalities. Missing values have little effect on the overall proportions and so they can be removed. It remains male dominated with applicants who are married, have no dependents, are highly educated, and have a long credit history."
  },
  {
    "objectID": "posts/Loan Assessment/LendingApproval.html#correlations",
    "href": "posts/Loan Assessment/LendingApproval.html#correlations",
    "title": "Loan Approval",
    "section": "Correlations",
    "text": "Correlations\nTo determine how well each variable is correlated with our target variable and with one another, we construct a correlation plot. This plot contains the values of all correlation between variables represented by colors and numbers. The row we review the most is our target variable, ‘Loan_Status.’\n\n\nCode\nG = cor(loan_data[6:(length(loan_data)-3)])\ncorrplot(G, method = 'number') # colorful number\n\n\n\n\n\nThe numeric features do not seem to be strongly correlated with another so that is a factor that does not have to be dealt with.\n\n\nCode\nG = cor(loan_data[6:(length(loan_data)-3)])\ncorrplot(G, method = 'number') # colorful number\n\n\n\n\n\nGiven that our numeric features have correlation values near 0, they do not seem to be strongly correlated with our target. They also do not seem to have any correlation with one another so this is a factor that does not have to be dealt with."
  },
  {
    "objectID": "posts/Loan Assessment/LendingApproval.html#handling-missing-values",
    "href": "posts/Loan Assessment/LendingApproval.html#handling-missing-values",
    "title": "Loan Approval",
    "section": "Handling missing values",
    "text": "Handling missing values\n\n\nCode\n# plot missing values\nplot_missing(loan_data)\n\n\n\n\n\nWe can see above credit_history contributes to 8% of missing data along with self_employed that accounts for more than 5% of missing data. All records having missing categorical predictors will be removed. Next we will impute numeric values using MICE (Multivariate Imputation by Chained Equations).\n\n\nCode\n# Filter out the data which has missing categorical predictors\nloan_data <- loan_data %>% filter(!is.na(Credit_History) &\n                                  !is.na(Self_Employed) &  \n                                  !is.na(Dependents) & \n                                  !is.na(Gender) & \n                                  !is.na(Married))\n\n\n\n\nCode\n# impute numeric predictors using mice\nloan_data <- complete(mice(data=loan_data, method=\"pmm\", print=FALSE))\n\n\n\n\nCode\ndim(loan_data)\n\n\n[1] 511  12\n\n\nFinally our clean dataset contains 511 rows and 12 columns."
  },
  {
    "objectID": "posts/Loan Assessment/LendingApproval.html#preprocess-using-transformation",
    "href": "posts/Loan Assessment/LendingApproval.html#preprocess-using-transformation",
    "title": "Loan Approval",
    "section": "Preprocess using transformation",
    "text": "Preprocess using transformation\nWe have seen above that numeric features are right skewed so in this step we will use caret preprocess method using box cox, center and scale transformation.\n\n\nCode\n# library(e1071) - where this was used\nset.seed(622)\nloan_data <- loan_data %>% \n  dplyr::select(c(\"ApplicantIncome\", \"CoapplicantIncome\", \"LoanAmount\", \"Loan_Amount_Term\")) %>%\n  preProcess(method = c(\"BoxCox\",\"center\",\"scale\")) %>% \n  predict(loan_data)"
  },
  {
    "objectID": "posts/Loan Assessment/LendingApproval.html#training-and-test-partition",
    "href": "posts/Loan Assessment/LendingApproval.html#training-and-test-partition",
    "title": "Loan Approval",
    "section": "Training and Test Partition",
    "text": "Training and Test Partition\nIn this step for data preparation we will partition the training dataset in training and validation sets using createDataPartition method from caret package. We will reserve 75% for training and rest 25% for validation purpose.\n\n\nCode\nset.seed(622)\npartition <- createDataPartition(loan_data$Loan_Status, p=0.75, list = FALSE)\n\ntraining <- loan_data[partition,]\ntesting <- loan_data[-partition,]\n\n# training/validation partition for independent variables\n#X.train <- ld.clean[partition, ] %>% dplyr::select(-Loan_Status)\n#X.test <- ld.clean[-partition, ] %>% dplyr::select(-Loan_Status)\n\n# training/validation partition for dependent variable Loan_Status\n#y.train <- ld.clean$Loan_Status[partition]\n#y.test <- ld.clean$Loan_Status[-partition]"
  },
  {
    "objectID": "posts/Loan Assessment/LendingApproval.html#linear-discriminant-analysis-lda",
    "href": "posts/Loan Assessment/LendingApproval.html#linear-discriminant-analysis-lda",
    "title": "Loan Approval",
    "section": "Linear Discriminant Analysis (LDA)",
    "text": "Linear Discriminant Analysis (LDA)\n\n\nCode\n# LDA model\nlda_model <- lda(Loan_Status~., data = loan_data)\nlda_model\n\n\nCall:\nlda(Loan_Status ~ ., data = loan_data)\n\nPrior probabilities of groups:\n        N         Y \n0.3209393 0.6790607 \n\nGroup means:\n  GenderMale MarriedYes Dependents1 Dependents2 Dependents3+\nN  0.7926829  0.5792683   0.1829268   0.1341463   0.09756098\nY  0.8357349  0.6801153   0.1585014   0.1902017   0.08069164\n  EducationNot Graduate Self_EmployedYes ApplicantIncome CoapplicantIncome\nN             0.2682927        0.1463415     0.003576320         0.0571435\nY             0.1902017        0.1325648    -0.001690249        -0.0270073\n   LoanAmount Loan_Amount_Term Credit_History1 Property_AreaSemiurban\nN  0.07966414      0.016992352       0.5548780              0.2682927\nY -0.03765106     -0.008030968       0.9798271              0.4409222\n  Property_AreaUrban\nN          0.3719512\nY          0.2997118\n\nCoefficients of linear discriminants:\n                                LD1\nGenderMale              0.185159211\nMarriedYes              0.375755462\nDependents1            -0.209004726\nDependents2             0.137509542\nDependents3+            0.007142953\nEducationNot Graduate  -0.294391997\nSelf_EmployedYes       -0.025905262\nApplicantIncome        -0.012085555\nCoapplicantIncome      -0.106529320\nLoanAmount             -0.099136040\nLoan_Amount_Term       -0.049820158\nCredit_History1         3.073804026\nProperty_AreaSemiurban  0.616732100\nProperty_AreaUrban      0.066231320\n\n\n\n\nCode\n# prediction from lda model\nlda_predict <- lda_model %>% \n  predict(testing)\n\n\n\n\nCode\n# accuracy\nmean(lda_predict$class==testing$Loan_Status)\n\n\n[1] 0.8110236\n\n\nCode\nconfusionMatrix(lda_predict$class, testing$Loan_Status)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  N  Y\n         N 19  2\n         Y 22 84\n                                        \n               Accuracy : 0.811         \n                 95% CI : (0.732, 0.875)\n    No Information Rate : 0.6772        \n    P-Value [Acc > NIR] : 0.0005479     \n                                        \n                  Kappa : 0.5046        \n                                        \n Mcnemar's Test P-Value : 0.0001052     \n                                        \n            Sensitivity : 0.4634        \n            Specificity : 0.9767        \n         Pos Pred Value : 0.9048        \n         Neg Pred Value : 0.7925        \n             Prevalence : 0.3228        \n         Detection Rate : 0.1496        \n   Detection Prevalence : 0.1654        \n      Balanced Accuracy : 0.7201        \n                                        \n       'Positive' Class : N             \n                                        \n\n\nLDA model accuracy comes out as ~81%"
  },
  {
    "objectID": "posts/Loan Assessment/LendingApproval.html#k-nearest-neighbor-knn",
    "href": "posts/Loan Assessment/LendingApproval.html#k-nearest-neighbor-knn",
    "title": "Loan Approval",
    "section": "K-nearest neighbor (KNN)",
    "text": "K-nearest neighbor (KNN)\n\n\nCode\n# KNN model\nset.seed(622)\ntrain.knn <- training[, names(training) != \"Direction\"]\nprep <- preProcess(x = train.knn, method = c(\"center\", \"scale\"))\nprep\n\n\nCreated from 384 samples and 12 variables\n\nPre-processing:\n  - centered (4)\n  - ignored (8)\n  - scaled (4)\n\n\nCode\ncl <- trainControl(method=\"repeatedcv\", repeats = 5) \nknn_model <- train(Loan_Status ~ ., data = training, \n                method = \"knn\", \n                trControl = cl, \n                preProcess = c(\"center\",\"scale\"), \n                tuneLength = 20)\nknn_model \n\n\nk-Nearest Neighbors \n\n384 samples\n 11 predictor\n  2 classes: 'N', 'Y' \n\nPre-processing: centered (14), scaled (14) \nResampling: Cross-Validated (10 fold, repeated 5 times) \nSummary of sample sizes: 346, 346, 346, 345, 346, 345, ... \nResampling results across tuning parameters:\n\n  k   Accuracy   Kappa    \n   5  0.7590209  0.3575145\n   7  0.7689406  0.3751117\n   9  0.7725985  0.3746036\n  11  0.7782524  0.3884713\n  13  0.7829615  0.3994494\n  15  0.7787908  0.3849571\n  17  0.7689528  0.3503238\n  19  0.7642294  0.3309567\n  21  0.7647551  0.3299468\n  23  0.7548097  0.2947001\n  25  0.7454298  0.2607665\n  27  0.7412733  0.2448000\n  29  0.7402746  0.2423939\n  31  0.7365911  0.2319391\n  33  0.7292901  0.2050164\n  35  0.7251066  0.1885755\n  37  0.7178596  0.1615955\n  39  0.7126768  0.1383143\n  41  0.7100317  0.1282468\n  43  0.7084798  0.1212857\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was k = 13.\n\n\n\n\nCode\n# prediction from knn model\nplot(knn_model)\n\n\n\n\n\nCode\nknn_predict <- predict(knn_model,newdata = testing)\nmean(knn_predict == testing$Loan_Status) # accuracy\n\n\n[1] 0.7952756\n\n\nCode\nconfusionMatrix(knn_predict, testing$Loan_Status)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  N  Y\n         N 17  2\n         Y 24 84\n                                          \n               Accuracy : 0.7953          \n                 95% CI : (0.7146, 0.8617)\n    No Information Rate : 0.6772          \n    P-Value [Acc > NIR] : 0.002202        \n                                          \n                  Kappa : 0.4553          \n                                          \n Mcnemar's Test P-Value : 3.814e-05       \n                                          \n            Sensitivity : 0.4146          \n            Specificity : 0.9767          \n         Pos Pred Value : 0.8947          \n         Neg Pred Value : 0.7778          \n             Prevalence : 0.3228          \n         Detection Rate : 0.1339          \n   Detection Prevalence : 0.1496          \n      Balanced Accuracy : 0.6957          \n                                          \n       'Positive' Class : N               \n                                          \n\n\nKNN model accuracy comes out as ~80%"
  },
  {
    "objectID": "posts/Loan Assessment/LendingApproval.html#decision-trees",
    "href": "posts/Loan Assessment/LendingApproval.html#decision-trees",
    "title": "Loan Approval",
    "section": "Decision Trees",
    "text": "Decision Trees\n\n\nCode\n# Decision Trees model\nset.seed(622)\ntree.loans = tree(Loan_Status~., data=training)\nsummary(tree.loans)\n\n\n\nClassification tree:\ntree(formula = Loan_Status ~ ., data = training)\nVariables actually used in tree construction:\n[1] \"Credit_History\"    \"Property_Area\"     \"CoapplicantIncome\"\nNumber of terminal nodes:  5 \nResidual mean deviance:  0.921 = 349 / 379 \nMisclassification error rate: 0.1849 = 71 / 384 \n\n\nCode\nplot(tree.loans)\ntext(tree.loans, pretty = 0)\n\n\n\n\n\n\n\nCode\n# prediction from decision tree model\ntree.predict<-predict(tree.loans, testing, type = 'class')\nmean(tree.predict == testing$Loan_Status) # accuracy\n\n\n[1] 0.7952756\n\n\nCode\nconfusionMatrix(tree.predict, testing$Loan_Status)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  N  Y\n         N 19  4\n         Y 22 82\n                                          \n               Accuracy : 0.7953          \n                 95% CI : (0.7146, 0.8617)\n    No Information Rate : 0.6772          \n    P-Value [Acc > NIR] : 0.0022025       \n                                          \n                  Kappa : 0.471           \n                                          \n Mcnemar's Test P-Value : 0.0008561       \n                                          \n            Sensitivity : 0.4634          \n            Specificity : 0.9535          \n         Pos Pred Value : 0.8261          \n         Neg Pred Value : 0.7885          \n             Prevalence : 0.3228          \n         Detection Rate : 0.1496          \n   Detection Prevalence : 0.1811          \n      Balanced Accuracy : 0.7085          \n                                          \n       'Positive' Class : N               \n                                          \n\n\nDecision Tree model accuracy comes out as ~80%"
  },
  {
    "objectID": "posts/Loan Assessment/LendingApproval.html#random-forests",
    "href": "posts/Loan Assessment/LendingApproval.html#random-forests",
    "title": "Loan Approval",
    "section": "Random Forests",
    "text": "Random Forests\n\n\nCode\nset.seed(622)\n# Random Forest model\nrf.loans <- randomForest(Loan_Status~., data = training)\nrf.loans\n\n\n\nCall:\n randomForest(formula = Loan_Status ~ ., data = training) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 3\n\n        OOB estimate of  error rate: 20.57%\nConfusion matrix:\n   N   Y class.error\nN 59  64  0.52032520\nY 15 246  0.05747126\n\n\n\n\nCode\n# prediction from random forest model\nrf.predict <- predict(rf.loans, testing,type='class')\nmean(rf.predict == testing$Loan_Status) # accuracy\n\n\n[1] 0.8110236\n\n\nCode\nconfusionMatrix(rf.predict, testing$Loan_Status)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  N  Y\n         N 22  5\n         Y 19 81\n                                        \n               Accuracy : 0.811         \n                 95% CI : (0.732, 0.875)\n    No Information Rate : 0.6772        \n    P-Value [Acc > NIR] : 0.0005479     \n                                        \n                  Kappa : 0.5254        \n                                        \n Mcnemar's Test P-Value : 0.0079635     \n                                        \n            Sensitivity : 0.5366        \n            Specificity : 0.9419        \n         Pos Pred Value : 0.8148        \n         Neg Pred Value : 0.8100        \n             Prevalence : 0.3228        \n         Detection Rate : 0.1732        \n   Detection Prevalence : 0.2126        \n      Balanced Accuracy : 0.7392        \n                                        \n       'Positive' Class : N             \n                                        \n\n\nRandom Forest model accuracy comes out as ~80%"
  },
  {
    "objectID": "posts/Stock Prediction/arima.html",
    "href": "posts/Stock Prediction/arima.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "A good forecast is a blessing while the wrong forecast could prove to be dangerous"
  },
  {
    "objectID": "posts/Stock Prediction/arima.html#introduction",
    "href": "posts/Stock Prediction/arima.html#introduction",
    "title": "Time Series Analysis",
    "section": "Introduction",
    "text": "Introduction\nGiven an unknown data source with several groups, we attempt to predict the next 140 values of a times series data set based on 1622 entries provided on multiple events. Our predictions will be fine-tuned to reduce the mean absolute percentage error (MAPE) as much as possible. The packages we will be using and all associated code to produce the models can be found in the attached markdown file. The data with its first five rows, are shown below.\n\n\nCode\n# Data source\ndata <- read.csv(\"https://raw.githubusercontent.com/palmorezm/msds/main/Predictive%20Analytics/Projects/Project1/project1data.csv\")\n# data <- data %>% \n#   rename(SeriesInd = ï..SeriesInd) \nhead(data, 5)\n\n\n\n\n  \n\n\n\nWe create forecasts for two preselected variables within each of six predetermined groups. These groups are denoted S01, S02, S03, S04, S05, and S06 respectively. There are five variables within each group that we have to work with. They are Var01, Var02, Var03, Var05, and Var07 respectively. Our date variable ‘SeriesInd,’ is displayed in its numeric serial number form calculated with Excel. Although we do not know what the variables stand for, we can develop models to try and forecast their behavior. This chart contains a breakdown of which variables are forecast in each group.\n\n\nCode\n# Chart\nvarsbygroup <- data.frame(matrix(c(\"S01\", \"S02\", \"S03\",\n                                   \"S04\", \"S05\", \"S06\", \n                                   \"Var01\", \"Var02\", \"Var05\",\n                                   \"Var01\", \"Var02\", \"Var05\",\n                                   \"Var02\", \"Var03\", \"Var07\",\n                                   \"Var02\", \"Var03\", \"Var07\"),\n                                 nrow = 6, ncol=3))\ncolnames(varsbygroup) <- c(\"Group\", \"Variable1\", \"Variable2\")\nvarsbygroup %>% \n  kbl(booktabs = T) %>% \n  kable_styling(latex_options = c(\"striped\", \"HOLD_position\", \"scale_down\"), full_width = T)\n\n\n\n\n \n  \n    Group \n    Variable1 \n    Variable2 \n  \n \n\n  \n    S01 \n    Var01 \n    Var02 \n  \n  \n    S02 \n    Var02 \n    Var03 \n  \n  \n    S03 \n    Var05 \n    Var07 \n  \n  \n    S04 \n    Var01 \n    Var02 \n  \n  \n    S05 \n    Var02 \n    Var03 \n  \n  \n    S06 \n    Var05 \n    Var07 \n  \n\n\n\n\n\nCode\n# Grouping\nS01 <- data %>% \n  filter(group == \"S01\")\nS02 <- data %>% \n  filter(group == \"S02\")\nS03 <- data %>% \n  filter(group == \"S03\")\nS04 <- data %>% \n  filter(group == \"S04\")\nS05 <- data %>% \n  filter(group == \"S05\")\nS06 <- data %>% \n  filter(group == \"S06\")\n\n# Imputation by function - missing something? lapply/sapply may work \nsoximp <- function(df){\n  for (i in colnames(df)){\n    if (sum(is.na(df[[i]])) !=0){\n      df[[i]][is.na(df[[i]])] <- median(df[[i]], na.rm=TRUE)\n    }\n  }\n}\n\n# Imputation loops for each group by median \nfor (i in colnames(S01)){\n  if (sum(is.na(S01[[i]])) != 0){\n    S01[[i]][is.na(S01[[i]])] <- median(S01[[i]], na.rm = TRUE)\n  } \n}\nfor (i in colnames(S02)){\n  if (sum(is.na(S02[[i]])) != 0){\n    S02[[i]][is.na(S02[[i]])] <- median(S02[[i]], na.rm = TRUE)\n  } \n}\nfor (i in colnames(S03)){\n  if (sum(is.na(S03[[i]])) != 0){\n    S03[[i]][is.na(S03[[i]])] <- median(S03[[i]], na.rm = TRUE)\n  } \n}\nfor (i in colnames(S04)){\n  if (sum(is.na(S04[[i]])) != 0){\n    S04[[i]][is.na(S04[[i]])] <- median(S04[[i]], na.rm = TRUE)\n  } \n}\nfor (i in colnames(S05)){\n  if (sum(is.na(S05[[i]])) != 0){\n    S05[[i]][is.na(S05[[i]])] <- median(S05[[i]], na.rm = TRUE)\n  } \n}\nfor (i in colnames(S06)){\n  if (sum(is.na(S06[[i]])) != 0){\n    S06[[i]][is.na(S06[[i]])] <- median(S06[[i]], na.rm = TRUE)\n  } \n}\n\n\nBefore we begin, the data is filtered to extract each time series by group. This isolates the Var01, Var02, Var03, Var05, and Var07 variables associated with groups S01, S02, and so on. Then, with each group and its respective variables’ behavior isolated, we clean and adjust the data to make use of it in the analysis. Once we determine the most appropriate models to forecast the proper variable in each group, we evaluate the results of our predictions. Our final forecasts are captured in the excel spreadsheet attached."
  },
  {
    "objectID": "posts/Stock Prediction/arima.html#analysis",
    "href": "posts/Stock Prediction/arima.html#analysis",
    "title": "Time Series Analysis",
    "section": "Analysis",
    "text": "Analysis\nWe began by addressing missing values. Given 10,572 observations, about 8% of each variable was missing. Several methods were tried to address this but the best were Kalman smoothing and simple imputation by the median of each ‘Var0X’ variable to fill in where appropriate. The ‘SeriesInd’ numeric date was also converted from its serial number form to a common date-time series. We then examined each group’s variables separately.\n\n\nCode\n# library(fpp2)\n\n#S01\nS01<-subset(data, group == \"S01\", select = c(SeriesInd, Var01, Var02))%>%\n  mutate(date=as.Date(SeriesInd, origin = \"1905-01-01\"))\nsummary(S01)\n\n\n   SeriesInd         Var01           Var02               date           \n Min.   :40669   Min.   :23.01   Min.   : 1339900   Min.   :2016-05-07  \n 1st Qu.:41304   1st Qu.:29.85   1st Qu.: 5347550   1st Qu.:2018-01-31  \n Median :41946   Median :35.66   Median : 7895050   Median :2019-11-05  \n Mean   :41945   Mean   :39.41   Mean   : 8907092   Mean   :2019-11-03  \n 3rd Qu.:42586   3rd Qu.:48.70   3rd Qu.:11321675   3rd Qu.:2021-08-06  \n Max.   :43221   Max.   :62.31   Max.   :48477500   Max.   :2023-05-03  \n                 NA's   :142     NA's   :140                            \n\n\nCode\n# Subset Var01 and Var02 from S01.\nS01_Var01<-S01 %>%select(Var01)\nS01_Var01<-S01_Var01[1:1625,]\n\n\nS01_Var02<-S01 %>%select(Var02)\nS01_Var02<-S01_Var02[1:1625,]\n\n\n#S02\nS02<-subset(data, group == \"S02\", select = c(SeriesInd, Var02, Var03))%>%\n  mutate(date=as.Date(SeriesInd, origin = \"1905-01-01\"))\nsummary(S02)\n\n\n   SeriesInd         Var02               Var03            date           \n Min.   :40669   Min.   :  7128800   Min.   : 8.82   Min.   :2016-05-07  \n 1st Qu.:41304   1st Qu.: 27880300   1st Qu.:11.82   1st Qu.:2018-01-31  \n Median :41946   Median : 39767500   Median :13.76   Median :2019-11-05  \n Mean   :41945   Mean   : 50633098   Mean   :13.68   Mean   :2019-11-03  \n 3rd Qu.:42586   3rd Qu.: 59050900   3rd Qu.:15.52   3rd Qu.:2021-08-06  \n Max.   :43221   Max.   :480879500   Max.   :38.28   Max.   :2023-05-03  \n                 NA's   :140         NA's   :144                         \n\n\nCode\n# Subset Var02 and Var03 from S02.\nS02_Var02<-S02 %>%select(Var02)\nS02_Var02<-S02_Var02[1:1625,]\n\n\nS02_Var03<-S02 %>%select(Var03)\nS02_Var03<-S02_Var03[1:1625,]\n\n\n\n#S03\nS03<-subset(data, group == \"S03\", select = c(SeriesInd, Var05, Var07))%>%\n  mutate(date=as.Date(SeriesInd, origin = \"1905-01-01\"))\nsummary(S03)\n\n\n   SeriesInd         Var05            Var07             date           \n Min.   :40669   Min.   : 27.48   Min.   : 27.44   Min.   :2016-05-07  \n 1st Qu.:41304   1st Qu.: 53.30   1st Qu.: 53.46   1st Qu.:2018-01-31  \n Median :41946   Median : 75.59   Median : 75.71   Median :2019-11-05  \n Mean   :41945   Mean   : 76.90   Mean   : 76.87   Mean   :2019-11-03  \n 3rd Qu.:42586   3rd Qu.: 98.55   3rd Qu.: 98.61   3rd Qu.:2021-08-06  \n Max.   :43221   Max.   :134.46   Max.   :133.00   Max.   :2023-05-03  \n                 NA's   :144      NA's   :144                          \n\n\nCode\n# Subset Var05 and Var07 from S03.\nS03_Var05<-S03 %>%select(Var05)\nS03_Var05<-S03_Var05[1:1625,]\n\n\nS03_Var07<-S03 %>%select(Var07)\nS03_Var07<-S03_Var07[1:1625,]\n\n\nStatistical summaries, box plots, and histograms were run on each group to evaluate where the average value of each variable was, if its distribution was skewed, determine whether outliers were present, and provide other descriptors of the data. These informed us that the average value (mean) of the variables are similar but their range varies widely with Var05 at 186.01 while Var02 covers a range of 479 million. Our analysis solves this potential problem by focusing on variables of the same scales as the intended target.\n\n\nCode\nsummary(S01_Var01)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  23.01   29.85   35.72   39.47   48.76   62.38 \n\n\nCode\npar(mfrow = c(1,2))\nhist(ts_S01_Var01)\nboxplot(ts_S01_Var01)\n\n\n\n\n\nCode\nautoplot(ts_S01_Var01)\n\n\n\n\n\nAdditionally, all but group S03 of the histograms exhibited right skewness, and Var02 and Var03 had outliers. These were replaced using Friedman’s super smoothing method. Due to the randomness of these variables, determining outliers was difficult and there is a presence of additional overly influential points as determined using Cook’s distance formula. We acknowledge the presence of these points but are unable to alter them as they are likely intentional based on the patterns in the data. For reference, the observations are shown in the scatter plot with color coding by each group.\n\n\nCode\ndata[c(1:7)]%>%\n  gather(variable, value, -SeriesInd, -group) %>%\n  ggplot(., aes(value, SeriesInd, color = group)) + \n  geom_point(fill = \"white\",\n             size=1, \n             shape=21, \n             alpha = 0.75) + \n  coord_flip() + \n   facet_wrap(~variable, \n             scales =\"free\") + \n  labs(title = \"Variable Patterns\", \n       subtitle = \"Color Coded by Group\", \n       x=\"Value\", \n       y=\"Time\", \n       caption = \"Contains all non-null observations of the given data set\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust=0.5), \n        plot.subtitle = element_text(hjust=0.5),\n        legend.position = \"bottom\", \n        axis.ticks.x=element_blank(),\n        axis.text.x=element_blank(), \n        plot.caption = element_text(hjust=0.5)\n        )\n\n\n\n\n\nSeasonality was also considered. It is possible this data follows a weak seasonal trend that increases during summer months but there is not a lot of evidence to support regular fluctuations. Regular gaps were noticed in the time series on a weekly basis and several methods were used in attempts to fix this. However, the data appears randomly distributed and as such, acts randomly. For this reason, we left the gaps alone and any further adjustments made were minimal to avoid disturbing any existing patterns in the data.\nWe determined that the best model type was an Auto Regressive Integrated Moving Average (ARIMA) with drift. Unfortunately, all variables required differencing to achieve stationarity. This indicates that any predictions made with these variables may be unrealistic because of inherent random changes in statistics like the mean and variance of these variables over time. We transform the data in our attempts to achieve stationarity but it should be noted that our review of stationarity is only a rough estimate using the aforementioned summary statistics so that we may apply this ARIMA method. Otherwise, we would have to conclude this data is inherently unpredictable and as such, render model forecasts useless. Rather, we focus on forecasting each variable individually and try to keep it simple."
  },
  {
    "objectID": "posts/Stock Prediction/arima.html#prediction",
    "href": "posts/Stock Prediction/arima.html#prediction",
    "title": "Time Series Analysis",
    "section": "Prediction",
    "text": "Prediction\n\n\nCode\nfcast0101 %>%\n  as.data.frame() %>%\n  ggplot(aes(x = seq(1:length(`Hi 95`)))) + \n  geom_ribbon(aes(ymin=`Lo 95`,ymax=`Hi 95`), fill=\"forest green\", alpha=0.5) + \n  geom_ribbon(aes(ymin=`Lo 80`,ymax=`Hi 80`), fill=\"dark green\", alpha=0.5) +\n  geom_line(aes(y = `Hi 95`), color = \"grey\", size=2, alpha = .5) + \n  geom_line(aes(y = `Point Forecast`), color = \"#000000\", size=1, lty = 1, alpha = .5) + \n  geom_line(aes(y = `Lo 95`), color = \"grey\", size=2, alpha = .5) + \n  geom_vline(xintercept = b1, lty = 3) +\n  geom_vline(xintercept = b2, lty = 3) +\n  geom_vline(xintercept = b3, lty = 3) +\n  geom_segment(aes(x = 0, y = extract[which(extract$Day == b1),]$`Hi 95`, \n                   xend = b1, yend = extract[which(extract$Day == b1),]$`Hi 95`), lty = 3) +\n  geom_segment(aes(x = 0, y = extract[which(extract$Day == b1),]$`Lo 95`, \n                   xend = b1, yend = extract[which(extract$Day == b1),]$`Lo 95`), lty = 3) +\n  geom_segment(aes(x = 0, y = extract[which(extract$Day == b2),]$`Hi 95`, \n                   xend = b2, yend = extract[which(extract$Day == b2),]$`Hi 95`), lty = 3) +\n  geom_segment(aes(x = 0, y = extract[which(extract$Day == b2),]$`Lo 95`, \n                   xend = b2, yend = extract[which(extract$Day == b2),]$`Lo 95`), lty = 3) +\n  geom_segment(aes(x = 0, y = extract[which(extract$Day == b3),]$`Hi 95`, \n                   xend = b3, yend = extract[which(extract$Day == b3),]$`Hi 95`), lty = 3) +\n  geom_segment(aes(x = 0, y = extract[which(extract$Day == b3),]$`Lo 95`, \n                   xend = b3, yend = extract[which(extract$Day == b3),]$`Lo 95`), lty = 3) +\n  geom_segment(aes(x = 0, y = extract[which(extract$Day == b4),]$`Hi 95`, \n                   xend = b4, yend = extract[which(extract$Day == b4),]$`Hi 95`), lty = 3) +\n  geom_segment(aes(x = 0, y = extract[which(extract$Day == b4),]$`Lo 95`, \n                   xend = b4, yend = extract[which(extract$Day == b4),]$`Lo 95`), lty = 3) +\n  scale_x_continuous(expand = c(0, 0), \n                     limits=c(0,100), \n                     breaks = c(0, b1, b2, b3, b4)) +\n  scale_y_continuous(expand = c(0, 0), \n                     limits=c(min(fcast0101$lower), max(fcast0101$upper)), \n                     breaks = c(round(fcast0101$mean[[1]], digits = 2),\n                                round(extract[which(extract$Day == b1),]$`Hi 95`, 2), \n                                round(extract[which(extract$Day == b1),]$`Lo 95`, 2),\n                                round(extract[which(extract$Day == b2),]$`Hi 95`, 2),\n                                round(extract[which(extract$Day == b2),]$`Lo 95`, 2), \n                                round(extract[which(extract$Day == b3),]$`Hi 95`, 2),\n                                round(extract[which(extract$Day == b3),]$`Lo 95`, 2), \n                                round(extract[which(extract$Day == b4),]$`Hi 95`, 2),\n                                round(extract[which(extract$Day == b4),]$`Lo 95`, 2))) +\n  annotate(\"text\", x = 7.5, y =53.9, label = paste0(\"MAPE =\", signif(mape0101, 4))) +\n  annotate(\"label\",x=txtx90,y=extract[which(extract$Day == txtx90),]$`Point Forecast` + 3.50, \n           label = \"95% Confidence\" ) + \n  annotate(\"segment\", x = txtx90, y = mean(extract$`Point Forecast`) + 4.75, \n           xend = txtx90, \n           yend = extract[which(extract$Day == txtx90),]$`Hi 95`,\n         arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\"))) +\n  annotate(\"segment\", x = txtx90, y = mean(extract$`Point Forecast`) + 3, \n           xend = txtx90, \n           yend = extract[which(extract$Day == txtx90),]$`Lo 95`,\n         arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\"))) + \n  annotate(\"label\",x=txtx80, y=extract[which(extract$Day == txtx90),]$`Point Forecast` - 3.01, \n           label = \"80% Confidence\" ) +\n  annotate(\"segment\", x = txtx80, y = mean(extract$`Point Forecast`) - 1.75, \n           xend = txtx80, \n           yend = extract[which(extract$Day == txtx80),]$`Hi 80`,\n         arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\"))) +\n  annotate(\"segment\", x = txtx80, y = mean(extract$`Point Forecast`) -3.5, \n           xend = txtx80, \n           yend = extract[which(extract$Day == txtx80),]$`Lo 80`,\n         arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\"))) +\n  annotate(\"label\",x=b1 - 7.05, y=mean(extract$`Point Forecast`) + 6, \n           label = paste(\"Day 15:\", signif(extract[which(extract$Day == b1),]$`Point Forecast`, 4))) +\n  annotate(\"segment\", x = b1 - 7, y = mean(extract$`Point Forecast`) + 5, \n           yend = extract[which(extract$Day == b1),]$`Point Forecast`,\n           xend = b1, \n         arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\")))  +\n  annotate(\"label\",x=b2 - 5, y=mean(extract$`Point Forecast`) + 9, \n           label = paste(\"Day 30:\", signif(extract[which(extract$Day == b2),]$`Point Forecast`, 4))) +\n  annotate(\"segment\", x = b2 - 4, y = mean(extract$`Point Forecast`) + 8.0, \n           yend = extract[which(extract$Day == b2),]$`Point Forecast`,\n           xend = b2, \n         arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\")))  +\n  annotate(\"label\",x=b3 - 10, y=mean(extract$`Point Forecast`) + 11, \n           label = paste(\"Day 60:\", signif(extract[which(extract$Day == b3),]$`Point Forecast`, 4))) +\n  annotate(\"segment\", x = b3 - 10, y = mean(extract$`Point Forecast`) + 9.75, \n           yend = extract[which(extract$Day == b3),]$`Point Forecast`,\n           xend = b3, \n         arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\"))) +\n  labs(x = \"Day\", y = \"Value\", \n       title = \"Value Estimation is Less Certain with Time\", \n       subtitle = \"Stock Market Day-Value Pairs over 100 Days Shows Growth of Uncertainty\") + \n  theme_minimal() + theme(plot.title = element_text(hjust = 0.5), \n                          plot.subtitle = element_text(hjust = 0.5))"
  }
]