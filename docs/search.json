[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Zach Palmore is currently working as a Data Scientist in Rock County, WI. When not innovating data pipelines, he enjoys spending time gardening and making homemade pizzas.\nM.S. Data Science"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Risk Modeling and Prediction",
    "section": "",
    "text": "A Credit Risk Assessment\n\n\n\n\npredictive\n\n\nmodeling\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2022\n\n\nZach Palmore\n\n\n\n\n\n\n  \n\n\n\n\n\nPredicting Value of Unknown Variables\n\n\n\n\npredictive\n\n\nmodeling\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2022\n\n\nZach Palmore\n\n\n\n\n\n\n  \n\n\n\n\n\nAn Auto Insurance Example\n\n\n\n\npredictive\n\n\nmodeling\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2022\n\n\nZach Palmore\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Insurance Claim Prediction/index.html#the-challenge",
    "href": "posts/Insurance Claim Prediction/index.html#the-challenge",
    "title": "Claim Estimation",
    "section": "The Challenge",
    "text": "The Challenge\nA key part of insurance is charging each customer the appropriate price for the risk they represent.\nThe challenge is knowing what is most likely to happen before it occurs. If we have confidence in what could occur, then we may better manage our risk. Data science offers a solution to this challenge. Developing advanced computational models with the capacity to crunch high volume data allows us to calculate probabilities of risk for many individuals at scale.\nThe business need is to predict the probability that a person will submit a claim and then estimate that claim amount. Multiple linear regression and binary logistic regression models were built to answer these questions. We explore, analyze and model with a data set containing records of customer behavior at an auto insurance company. A table with a selection of five variables that represent behavioral characteristics from customer records is shown below with a brief description of each for reference.\n\n\nCode\n# short descriptions of variables as table from matrix\nvardesc <- data.frame(matrix(c(\n'TARGET_FLAG',  'Was a claim submitted? 1 = Yes, 0 = No',\n'TARGET_AMT',   'Estimated amount of claim',\n'CLM_FREQ', 'Number of claims filed in past five years',\n'MVR_PTS',  'Motor vehicle inspection points',\n'TRAVETIME',    'Distance to work in minutes'\n),  byrow = TRUE, ncol = 2))\ncolnames(vardesc) <- c('Variable', 'Description')\nkbl(vardesc, booktabs = T, caption = \"Variable Descriptions\") %>%\n  kable_styling(latex_options = c(\"striped\", \"HOLD_position\"), full_width = F)\n\n\n\n\nVariable Descriptions\n \n  \n    Variable \n    Description \n  \n \n\n  \n    TARGET_FLAG \n    Was a claim submitted? 1 = Yes, 0 = No \n  \n  \n    TARGET_AMT \n    Estimated amount of claim \n  \n  \n    CLM_FREQ \n    Number of claims filed in past five years \n  \n  \n    MVR_PTS \n    Motor vehicle inspection points \n  \n  \n    TRAVETIME \n    Distance to work in minutes"
  },
  {
    "objectID": "posts/Insurance Claim Prediction/index.html#data-prep",
    "href": "posts/Insurance Claim Prediction/index.html#data-prep",
    "title": "Claim Estimation",
    "section": "Data Prep",
    "text": "Data Prep\nThere are over 8000 records in this data set, with 26 variables in each record. We begin by loading in the data and exploring.\nWhen exploring we look for missing values and abnormalities and try to find patterns in the records. We calculate some descriptive and inferential statistics that show the characteristics present in the data. These statistics inform us if imputations, transformations, or other adjustments are needed. For example, this violin plot with a box plot inlay, provides a wealth of information.\n\n\nKernel Density Estimation & Outliers\ntdata %>% \n  select_if(is.numeric) %>% \n  gather() %>% \n  ggplot(aes(value, key)) +\n  facet_wrap(~ key, scales = \"free\") +\n  geom_violin(aes(color = key, alpha = 1)) + \n  geom_boxplot(aes(fill = key, alpha = .5), notch = TRUE, size = .1, lty = 3) +  \n  stat_summary(fun.y = mean, geom = \"point\",\n               shape = 8, size = 1.5, color = \"#000000\") + \n  theme_minimal() + \n  theme(axis.text = element_blank(), \n        axis.title = element_blank(), \n        legend.position = \"none\", plot.title = element_text(hjust = 0.5)) + \n  labs(title = \"Kernel Density Estimation & Outliers\") \n\n\n\n\n\nPerhaps one of the easiest things to spot in this plot are outliers, shown as grey dots on the left and right sides of the variables. These points are located far from where the rest of the data is, and there are so many that some dots appear solid black due to repeated grey dots plotted in the same spot. Our target amount, the value we intend to predict, is a great example of a distribution with a lot of outliers. These points give a good idea of what ‘normal’ is for each variable but this plot offers some more.\nIn this plot the asterisk near the center of each boxplot is the median of the distribution for all records that contain the variable named, however, what exactly the median is less important than where it lies within the distribution. The top and bottom edges (the curvy colored lines) of the violin plot use the non-parametric method kernel density estimation (KDE) to smoothly estimate the probability of a value occurring anywhere along its range. Here again, we already notice problems with our target amount. Although not every variable follows this pattern. For these reasons, we need to be careful about how we handle this data before making any predictions.\nAnother method used to explore and prepare the data is a correlation plot. These examine the strength of relationships between variables, whether they are positive or negative, and how they compare to one another. We take a look at some selected variables in hopes that they confirm some expectations.\n\n\nCorrelation Plot\ntdata %>%\n  select_if(is.numeric) %>% \n  cor() %>% \n  ggcorrplot(method = \"circle\", type=\"upper\", \n             ggtheme = ggplot2::theme_minimal, legend.title = \"Influence\") + coord_flip() \n\n\n\n\n\nIf we are trying to predict the amount of a claim and we have a true or false variable indicating the presence or absences of claim submission (TARGET_FLAG) and the claim amount (TARGET_AMT), shouldn’t we expect the two to be correlated? I would hope so, given that, when an individual does not file a claim, the resultant claim amount is 0. We notice this in the big red circle towards the bottom left of the screen. This shows a strong positive correlation between the amount of a claim and presence of a claim. From this correlation plot we start to confirm these expectations and validate some conventional auto insurance knowledge.\nThese demonstrate only two ways to look at data before model building. However, to ensure that a model functions in the real world, a multitude of exploratory methods should be used to fully understand the data. To keep it brief, the data is split 70-30 into training and testing data sets, then cleaned up with using the multiple imputation by chained equations (MICE) method, perform a Yeo-Johnson transformation, and adjust other points as necessary to make the non-normal predictor appear normal enough for prediction. A quick look at the imputed summary statistics is shown for four numeric variables as a reference.\n\n\nImputed Summary Statistics\nimputed.stats.table <- data.frame(matrix(c(0,29707,54028,61469,83304,367030, \n         0,0,161160,155225,233352,885282,\n         1500,9280,14440,15710,20850,69740,\n         0,0,0,4037,4636,57037), ncol = 4, byrow = F)) \nnames(imputed.stats.table) <- c(\"Income\", \"Home Value\", \n                                \"Bluebook Value\", \"Old Claims\")\nimputed.stats.table <- imputed.stats.table %>% \n  mutate(Statistic = c(\"Min\", \"1st Quartile\", \"Median\", \n                       \"Mean\", \"3rd Qartile\", \"Max\")) %>%\n  dplyr::select(\"Statistic\", \"Income\", \"Home Value\", \n                \"Bluebook Value\", \"Old Claims\")\nkbl(imputed.stats.table,\n    booktabs = T,\n    caption = \"Imputed Summary Statistics\") %>%\n  kable_styling(latex_options = c(\"striped\", \"hold_position\"),\n                full_width = F)\n\n\n\n\nImputed Summary Statistics\n \n  \n    Statistic \n    Income \n    Home Value \n    Bluebook Value \n    Old Claims \n  \n \n\n  \n    Min \n    0 \n    0 \n    1500 \n    0 \n  \n  \n    1st Quartile \n    29707 \n    0 \n    9280 \n    0 \n  \n  \n    Median \n    54028 \n    161160 \n    14440 \n    0 \n  \n  \n    Mean \n    61469 \n    155225 \n    15710 \n    4037 \n  \n  \n    3rd Qartile \n    83304 \n    233352 \n    20850 \n    4636 \n  \n  \n    Max \n    367030 \n    885282 \n    69740 \n    57037"
  },
  {
    "objectID": "posts/Insurance Claim Prediction/index.html#model-building",
    "href": "posts/Insurance Claim Prediction/index.html#model-building",
    "title": "Claim Estimation",
    "section": "Model Building",
    "text": "Model Building\nOur objective was to build multiple linear regression and binomial logistic regression models to predict the amount of auto insurance claim. We explored, analyzed, and prepared the data as best we could in an attempt to improve the predictive outcome. The manner in which these were built is detailed in the Caret Model code chunk.\n\n\nCaret Models\n# Model 1: Establish Baseline\n# This models uses only previous accident as a predictor\nmodel1 <- glm(TARGET_FLAG ~ previous_accident, \n              family = binomial(link = \"logit\"), train)\n# Model 2: Experimentally Determine Best Features by Hand\n# These features were selected by alpha level and intuition\nmodel2 <- glm(TARGET_FLAG ~ previous_accident + \n                city + young + clean_rec + \n                educated, family = binomial(link = \"logit\"), train)\n# Model 3: Add Recommended Risk Predictors from III\n# This model takes another step towards improving accuracy \nmodel3 <- glm(TARGET_FLAG ~ previous_accident + \n                city + mstatus + income.values + \n                sex + car_use + educated + KIDSDRIV + \n                revoked, family = binomial(link = \"logit\"), \n              train)\n# Model 4: All in One\n# Examine results with all variables included, what worked?\nmodel4 <- lm(target_amt ~ ., train) \n# Model 5: Linear Regression with Dollar Estimators\n# This model leans heavy on the variables with specific dollar figures\nmodel5 <- lm(target_amt ~ income.values +\n               home.values + bluebook.values + \n               oldclaim.values + avg_claim, \n             train) \n# Model 6: Multidirectional StepAIC Regression\n# This uses a Stepwise Akaike Information Criteria to evaluate \n# and select predictors in the model with some special data prep\nmodel6 <- lm(target_amt ~ . -TARGET_AMT -TARGET_FLAG, train) \npm <- stepAIC(model6, trace = F, direction = \"both\")\n\n\nAnother model wherein everything but the kitchen sink was thrown at it (aptly known as the kitchen sink model), gave us insights into which variables were significant to use and what their effect on the model would probably be. Of course, with a model that contains over 30 variables, there is room for some complex interactions to occur. We rely on our exploration and analysis to guide us in the creation of additional models alongside the results from the kitchen sink model, historical model, and conventional wisdom from the auto insurance domain.\nSince we humans tend to poorly judge relationships represented as mathematical operations, a stepwise AIC model was created to do a lot of work for us. This model performs a check by cycling through the variables both forwards and backwards to pick the variables that are most likely to improve the predictive capacity of the model. The AIC just stands for Akaike Information Criterion which is an estimator of prediction error. In this model, when stepping (or cycling) through variables, we are using this criterion to select variables that reduce the amount of error present in the model. Ideally, this will improve model quality and output.\nWe finish with some general predictions of each model type. We created a historical model, kitchen sink model, a multidirectional stepwiseAIC, backward stepwiseAIC, high risk predictor model, and a conventional wisdom model using information from the Insurance Information Institute (III). These will be put to the test in the model selection process. Which one do you think will perform best?"
  },
  {
    "objectID": "posts/Insurance Claim Prediction/index.html#model-selection",
    "href": "posts/Insurance Claim Prediction/index.html#model-selection",
    "title": "Claim Estimation",
    "section": "Model Selection",
    "text": "Model Selection\nTo standardize the process of model selection, a function is created wherein all statistics are computed the same way. It begins by bringing in the holdout data we created in the data prep section. This is named the test data. With this we evaluate how accurately the model predicted the result. We then compute a confusion matrix which contains the rates of false positives, true positives, false negatives, and true negatives and estimate model specificity, sensitivity, precision, and recall with those values. These combined measures let us find the F1 score, and from them we can create a receiver operating characteristic (ROC) plot and find the area under the curve (AUC). These provide details about the accuracy and real-world effectiveness of a model.\nAs a quick reference, we show the results from one of the best models, that of the multidirectional stepwiseAIC. The receiver operating characteristic curve is plotted alongside text containing the accuracy, its bounds, how precise and sensitive it is as well as multiple significance values including McNemars p-value and an accuracy p-value. Each model went through this evaluation process.\n\n\nModel Summary\n# Create Function to Evaluate All Models\nmodstat <- function(model, test, \n                    target = \"TARGET_FLAG\", threshold = 0.5){\n  \n  # test model using predictions with test data\n  test$new <- ifelse(predict.glm(\n    model, test, \"response\") >= threshold, 1, 0) \n    \n    # create confusion matrix with stats\n    # shows true positive, false positive, and their inverse\n    cm <- confusionMatrix(factor(test$new), \n                          factor(test[[target]]), \"1\")\n    \n    # Organize information into data frame\n    df <- data.frame(obs = test$TARGET_FLAG, \n                     predicted = test$new, \n                     probs = predict(model, test))\n  \n  # Calculate performance and significance values\n  Pscores <- prediction(df$probs, \n                        df$obs)\n  \n  # AUC = \"Area Under the Curve\" \n  AUC <- performance(Pscores, \n                     measure = \"auc\")@y.values[[1]]\n    pscores <- performance(Pscores, \n                           \"tpr\", \"fpr\")\n  \n  # Plot the scores of true positive/ false positive\n    # This is a receiver operating characteristic (ROC) curve \n  plot(pscores,main=\"ROC Curve\", \n       sub = paste0(\"AUC: \", \n                    round(AUC, 3)))\n  \n  # Extract the F1 score \n    # place it below the plot for each model when run\n  results <- paste(cat(\"F1 = \", \n                       cm$byClass[7], \" \"), cm)\n  \n  # Output results with a ROC curve and all scores \n  return(results)\n}\n\n# Calculate and show ONLY model 6 for quick reference\nmodstat(model6, test)\n\n\n\n\n\nF1 =  0.4838057  \n\n\n[1] \" 1\"                                                                                                                                                                                                                                                                                                                                                                                             \n[2] \" c(1699, 113, 397, 239)\"                                                                                                                                                                                                                                                                                                                                                                        \n[3] \" c(Accuracy = 0.791666666666667, Kappa = 0.36653677545056, AccuracyLower = 0.775030346732191, AccuracyUpper = 0.807602177444679, AccuracyNull = 0.740196078431373, AccuracyPValue = 1.62575683811825e-09, McnemarPValue = 5.02353275353728e-36)\"                                                                                                                                                \n[4] \" c(Sensitivity = 0.375786163522013, Specificity = 0.937637969094923, `Pos Pred Value` = 0.678977272727273, `Neg Pred Value` = 0.810591603053435, Precision = 0.678977272727273, Recall = 0.375786163522013, F1 = 0.483805668016194, Prevalence = 0.259803921568627, `Detection Rate` = 0.0976307189542484, `Detection Prevalence` = 0.143790849673203, `Balanced Accuracy` = 0.656712066308468)\"\n[5] \" sens_spec\"                                                                                                                                                                                                                                                                                                                                                                                     \n[6] \" list()\""
  },
  {
    "objectID": "posts/Insurance Claim Prediction/index.html#conclusion",
    "href": "posts/Insurance Claim Prediction/index.html#conclusion",
    "title": "Claim Estimation",
    "section": "Conclusion",
    "text": "Conclusion\nRisk varies widely from customer to customer, and a deep understanding of different risk factors helps predict the likelihood and cost of insurance claims. Many factors contribute to the frequency and severity of car accidents including how, where and under what conditions people drive, as well as what they are driving. We developed 6 models. Half of these were multiple linear regression models and the other half were binomial logistic regression. Both have their benefits in the right context.\n\n\nComparison Table\nmod.stats.table <- data.frame(matrix(c(\n         \"Model 1\", 0.722, 0.757, 0.5, .01, \n         \"Model 2\", 0.732, 0.767, 0.58, 0.333,\n         \"Model 3\", 0.754, 0.788, 0.625, 0.422,\n         \"Model 4\", 0.998, 0.999, 0.999, 0.999, \n         \"Model 5\", 0.719, 0.753, 0.514, .090,\n         \"Model 6\", 0.775, 0.808, 0.657, 0.484),\n         nrow = 6, byrow = T))\ncolnames(mod.stats.table) <- c(\"ID\", \"Lower Bound\", \"Upper Bound\", \n                               \"Balanced Accuracy\", \"F1 Score\")\nmod.stats.table\n\n\n\n\n  \n\n\n\nWith an accuracy between 78 - 81%, the multidirectional stepwiseAIC model wins the contest between these two model types. It was also the most useful real-world model with a balanced accuracy at 65.7%. Its F1 score was 0.484, indicating the relationship between precision (how well it predicts true positives) and recall (ratio of correct positives in the predictions) is better than any other method of modeling.\nIn this table we reviewed the model’s lower and upper accuracy bounds, its balanced accuracy, and F1 scores. The results show Model 3, our other stepwiseAIC, is the runner up. Model 4 is the only model that appears to have been too effective, and is not realistic for a variety of reasons. Perhaps the most important being over-fitting.\nOf these factors in prediction, balanced accuracy is likely the best criteria to judge the models on in this scenario. This is because it finds arithmetic mean of sensitivity and specificity which tends to represent imbalanced data better than accuracy alone. Since our data set was highly imbalanced and the target class of claim amount appeared much less than the non-target class, this accuracy estimate helps balance expectations.\nOf course, these predictions could be improved. When building models, it may be a good idea to perform some feature engineering to better isolate the riskiest and least risky customers. Other model types may also offer new insights.\nFor more, view the full report on my GitHub page."
  },
  {
    "objectID": "posts/Loan Assessment/LendingApproval.html",
    "href": "posts/Loan Assessment/LendingApproval.html",
    "title": "Loan Approval",
    "section": "",
    "text": "The main source of income of lenders stems from their credit line. Can it be improved?"
  },
  {
    "objectID": "posts/Loan Assessment/LendingApproval.html#data-characteristics",
    "href": "posts/Loan Assessment/LendingApproval.html#data-characteristics",
    "title": "Loan Approval",
    "section": "Data Characteristics",
    "text": "Data Characteristics\nThere are 614 observations of 12 variables. Each observation is an applicant’s application for a loan with its corresponding variables of interest. Below is the description of the variables of interest in the data set.\n\n\n\nVARIABLE NAME\nDESCRIPTION\n\n\n\n\nLoan_ID\nUnique Loan ID\n\n\nGender\nMale/ Female\n\n\nMarried\nApplicant married (Y/N)\n\n\nDependents\nNumber of dependents\n\n\nEducation\nApplicant Education (Graduate/ Undergraduate)\n\n\nSelf_Employed\nSelf employed (Y/N)\n\n\nApplicantIncome\nApplicant income\n\n\nCoapplicantIncome\nCoapplicant income\n\n\nLoanAmount\nLoan amount in thousands\n\n\nLoan_Amount_Term\nTerm of loan in months\n\n\nCredit_History\ncredit history meets guidelines\n\n\nProperty_Area\nUrban/ Semi Urban/ Rural\n\n\nLoan_Status\nLoan approved (Y/N)\n\n\n\nThere are four numeric variables represented by loan amount, loan amount term, applicant and co-applicant income. Several of these variables appear to be factors with specific levels but are not coded as such. For example, Gender, Married, Dependents, Education, Self_Employed, Property_Area, Credit_History, and Loan_Status are character strings. We will need to fix this if we are to make use of them.\n\n\nCode\n# read data, change blank to NA and and remove loan_id\nloan_data <- read.csv('https://raw.githubusercontent.com/amit-kapoor/Data622Group2/main/Loan_approval.csv') %>% \n  na_if(\"\") %>%\n  dplyr::select(-1)\n\n# categorical columns as factors\nloan_data <- loan_data %>% \n  mutate(Gender=as.factor(Gender),\n         Married=as.factor(Married),\n         Dependents=as.factor(Dependents),\n         Education=as.factor(Education),\n         Self_Employed=as.factor(Self_Employed),\n         Property_Area=as.factor(Property_Area),\n         Credit_History=as.factor(Credit_History),\n         Loan_Status=as.factor(Loan_Status))"
  },
  {
    "objectID": "posts/Loan Assessment/LendingApproval.html#data-summary",
    "href": "posts/Loan Assessment/LendingApproval.html#data-summary",
    "title": "Loan Approval",
    "section": "Data summary",
    "text": "Data summary\nBelow is a summary of the loan approval dataset. For this process we have already adjusted the data types to their proper forms. This summarizing function quantifies each variable in a manner consistent with their types. We notice the levels of each factor in the ‘Stats/Values’ column, the frequency of valid (non-missing) observations per level of our factors, and the quantity and percent missing alongside them. We review these statistics to identify any issues with each variable.\n\n\nCode\ndfSummary(loan_data, style = 'grid', graph.col = FALSE)\n\n\n\n\n  \n\n\n\nThere are 7 columns that have missing values. The proportion of values for several columns shows significant differences and skew. For example, 97.9% of this dataset contains males applicants based on observations of the Gender variable, 99.5% of applicants are married people given the Married variable, and over 90% of our observations have longer Credit_History. Due to the disproportionate levels within the variables we should expect the data is not representative of a larger population unless that population happens to have similar proportions.\nOur numeric incomes variables show significant signs of skew through the differences in their mean and medians as well as their ranges. The lowest applicant income was 150, while the highest was 81000. A similar problem exists with our co-applicant income data having had individuals with 0 income on the lowest end of the range and 41667 on the highest.\nHowever, all of the observations contained an applicant and co-applicant income. Since some applicants may not have used a co-applicant on their applications, part of this skew could be caused by the data collection process. Additionally, we are only missing 3.6% of the observations of loan amount and 2.3% for loan terms.\nThere are regular intervals and commonality in the loan term amounts which indicates we may have been able to factorize their data types. We chose instead to leave it as a discrete numeric value since it represents the term length which could be any number of days or months. We note that 85.3% percent of these applicants applied for a loan term of 360 but we are unsure if that is due to the lending institutions standard practice or if applicants requested this specific term.\nFor exploratory purposes, we visualize the proportions to see just how skewed and disproportionate this dataset is. We include missing values to demonstrate their influence on the dataset as well. The chart below shows the distribution of all categorical variables, which includes the factors mentioned previously.\n\n\nCode\n# select categorical columns\ncat_cols = c()\nj <- 1\nfor (i in 1:ncol(loan_data)) {\n  if (class((loan_data[,i])) == 'factor') {\n      cat_cols[j]=names(loan_data[i])\n      j <- j+1\n  }\n}\n\nloan_fact <-  loan_data[cat_cols]\n# long format\nloan_factm <- melt(loan_fact, measure.vars = cat_cols, variable.name = 'metric', value.name = 'value')\n\n\nWarning: attributes are not identical across measure variables; they will be\ndropped\n\n\nCode\n# plot categorical columns\nggplot(loan_factm, aes(x = value)) + \n  geom_bar() + \n  scale_fill_brewer(palette = \"Set1\") + \n  facet_wrap( ~ metric, nrow = 5L, scales = 'free') + coord_flip()\n\n\n\n\n\nFrom this chart, it is very clear we have a dataset with mostly married male graduates with no dependents, a long credit history, and who are not self-employed. There is a relatively even mix of urban, suburban, and rural applicants and a small number of missing values. Applicants tend to be accepted more often than not and there are no missing observations for our target variable ‘Loan_Status’ nor the applicant’s property area or education. These are all of our categorical variables.\nWe also generate histograms with the count of each observation to assess our numeric variable distributions. This will let us know more about the skewness, average values, and where potential outliers may be found for our numeric variables. The graph below shows their distributions.\n\n\nCode\nplot_histogram(loan_data, geom_histogram_args = list(\"fill\" = \"tomato4\"))\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe applicant income and co-applicant income variables are highly right skewed with a smaller number of individual applicants stretching the distribution towards higher incomes. For analysis purposes, we must keep in mind that only a handful of applicants had higher incomes while the bulk of applicants were concentrated at the lower end of the income distribution. The loan amount term has one spike at 360. Meanwhile, the loan amount is the closest to normal. These results are consistent with our summary table.\nNext we will review the impact of the categorical variables’ proportions on loan approval in more detail by isolating the factor levels individually. Here again, we visualize the proportions as a bar chart without missing values and expand the size of the chart to see the nuances of each. These are placed alongside each variable’s frequency table by level to visualize their proportions. The results are as follows:\n\n\nCode\nloan_ch <- with(loan_data, table(Credit_History, Loan_Status)) %>% \n  prop.table(margin = 1) %>% as.data.frame() %>% filter(Loan_Status == 'Y')\n\nloan_ch\n\n\n\n\n  \n\n\n\n\n\nCode\nggplot(loan_ch, aes(x=Credit_History, y=Freq, fill=Credit_History)) + geom_bar(stat='identity') + labs(title = 'Approved Loans by Credit History', y = \"Percentage\", x = \"Credit History\")\n\n\n\n\n\n\n\nCode\nloan_gen <- with(loan_data, table(Gender, Loan_Status)) %>% \n  prop.table(margin = 1) %>% as.data.frame() %>% filter(Loan_Status == 'Y')\n\nloan_gen\n\n\n\n\n  \n\n\n\n\n\nCode\nggplot(loan_gen, aes(x=Gender, y=Freq, fill=Gender)) + geom_bar(stat='identity') + labs(title = 'Approved Loans by Gender', y = \"Percentage\", x = \"Gender\")\n\n\n\n\n\n\n\nCode\nloan_ed <- with(loan_data, table(Education, Loan_Status)) %>% \n  prop.table(margin = 1) %>% as.data.frame() %>% filter(Loan_Status == 'Y')\n\nloan_ed\n\n\n\n\n  \n\n\n\n\n\nCode\nggplot(loan_ed, aes(x=Education, y=Freq, fill=Education)) + geom_bar(stat='identity') + labs(title = 'Approved Loans by Education', y = \"Percentage\", x = \"Education\")\n\n\n\n\n\n\n\nCode\nloan_mar <- with(loan_data, table(Married, Loan_Status)) %>% \n  prop.table(margin = 1) %>% as.data.frame() %>% filter(Loan_Status == 'Y')\n\nloan_mar\n\n\n\n\n  \n\n\n\n\n\nCode\nggplot(loan_mar, aes(x=Married, y=Freq, fill=Married)) + geom_bar(stat='identity') + labs(title = 'Approved Loans by Married', y = \"Percentage\", x = \"Married\")\n\n\n\n\n\n\n\nCode\nloan_dep <- with(loan_data, table(Dependents, Loan_Status)) %>% \n  prop.table(margin = 1) %>% as.data.frame() %>% filter(Loan_Status == 'Y')\n\nloan_dep\n\n\n\n\n  \n\n\n\n\n\nCode\nggplot(loan_dep, aes(x=Dependents, y=Freq, fill=Dependents)) + geom_bar(stat='identity') + labs(title = 'Approved Loans by Dependents', y = \"Percentage\", x = \"Dependents\")\n\n\n\n\n\nThese bar charts confirm our thoughts about the dataset’s disproportionalities. Missing values have little effect on the overall proportions and so they can be removed. It remains male dominated with applicants who are married, have no dependents, are highly educated, and have a long credit history."
  },
  {
    "objectID": "posts/Loan Assessment/LendingApproval.html#correlations",
    "href": "posts/Loan Assessment/LendingApproval.html#correlations",
    "title": "Loan Approval",
    "section": "Correlations",
    "text": "Correlations\nTo determine how well each variable is correlated with our target variable and with one another, we construct a correlation plot. This plot contains the values of all correlation between variables represented by colors and numbers. The row we review the most is our target variable, ‘Loan_Status.’\n\n\nCode\nG = cor(loan_data[6:(length(loan_data)-3)])\ncorrplot(G, method = 'number') # colorful number\n\n\n\n\n\nThe numeric features do not seem to be strongly correlated with another so that is a factor that does not have to be dealt with.\n\n\nCode\nG = cor(loan_data[6:(length(loan_data)-3)])\ncorrplot(G, method = 'number') # colorful number\n\n\n\n\n\nGiven that our numeric features have correlation values near 0, they do not seem to be strongly correlated with our target. They also do not seem to have any correlation with one another so this is a factor that does not have to be dealt with."
  },
  {
    "objectID": "posts/Loan Assessment/LendingApproval.html#handling-missing-values",
    "href": "posts/Loan Assessment/LendingApproval.html#handling-missing-values",
    "title": "Loan Approval",
    "section": "Handling missing values",
    "text": "Handling missing values\n\n\nCode\n# plot missing values\nplot_missing(loan_data)\n\n\n\n\n\nWe can see above credit_history contributes to 8% of missing data along with self_employed that accounts for more than 5% of missing data. All records having missing categorical predictors will be removed. Next we will impute numeric values using MICE (Multivariate Imputation by Chained Equations).\n\n\nCode\n# Filter out the data which has missing categorical predictors\nloan_data <- loan_data %>% filter(!is.na(Credit_History) &\n                                  !is.na(Self_Employed) &  \n                                  !is.na(Dependents) & \n                                  !is.na(Gender) & \n                                  !is.na(Married))\n\n\n\n\nCode\n# impute numeric predictors using mice\nloan_data <- complete(mice(data=loan_data, method=\"pmm\", print=FALSE))\n\n\n\n\nCode\ndim(loan_data)\n\n\n[1] 511  12\n\n\nFinally our clean dataset contains 511 rows and 12 columns."
  },
  {
    "objectID": "posts/Loan Assessment/LendingApproval.html#preprocess-using-transformation",
    "href": "posts/Loan Assessment/LendingApproval.html#preprocess-using-transformation",
    "title": "Loan Approval",
    "section": "Preprocess using transformation",
    "text": "Preprocess using transformation\nWe have seen above that numeric features are right skewed so in this step we will use caret preprocess method using box cox, center and scale transformation.\n\n\nCode\n# library(e1071) - where this was used\nset.seed(622)\nloan_data <- loan_data %>% \n  dplyr::select(c(\"ApplicantIncome\", \"CoapplicantIncome\", \"LoanAmount\", \"Loan_Amount_Term\")) %>%\n  preProcess(method = c(\"BoxCox\",\"center\",\"scale\")) %>% \n  predict(loan_data)"
  },
  {
    "objectID": "posts/Loan Assessment/LendingApproval.html#training-and-test-partition",
    "href": "posts/Loan Assessment/LendingApproval.html#training-and-test-partition",
    "title": "Loan Approval",
    "section": "Training and Test Partition",
    "text": "Training and Test Partition\nIn this step for data preparation we will partition the training dataset in training and validation sets using createDataPartition method from caret package. We will reserve 75% for training and rest 25% for validation purpose.\n\n\nCode\nset.seed(622)\npartition <- createDataPartition(loan_data$Loan_Status, p=0.75, list = FALSE)\n\ntraining <- loan_data[partition,]\ntesting <- loan_data[-partition,]\n\n# training/validation partition for independent variables\n#X.train <- ld.clean[partition, ] %>% dplyr::select(-Loan_Status)\n#X.test <- ld.clean[-partition, ] %>% dplyr::select(-Loan_Status)\n\n# training/validation partition for dependent variable Loan_Status\n#y.train <- ld.clean$Loan_Status[partition]\n#y.test <- ld.clean$Loan_Status[-partition]"
  },
  {
    "objectID": "posts/Loan Assessment/LendingApproval.html#linear-discriminant-analysis-lda",
    "href": "posts/Loan Assessment/LendingApproval.html#linear-discriminant-analysis-lda",
    "title": "Loan Approval",
    "section": "Linear Discriminant Analysis (LDA)",
    "text": "Linear Discriminant Analysis (LDA)\n\n\nCode\n# LDA model\nlda_model <- lda(Loan_Status~., data = loan_data)\nlda_model\n\n\nCall:\nlda(Loan_Status ~ ., data = loan_data)\n\nPrior probabilities of groups:\n        N         Y \n0.3209393 0.6790607 \n\nGroup means:\n  GenderMale MarriedYes Dependents1 Dependents2 Dependents3+\nN  0.7926829  0.5792683   0.1829268   0.1341463   0.09756098\nY  0.8357349  0.6801153   0.1585014   0.1902017   0.08069164\n  EducationNot Graduate Self_EmployedYes ApplicantIncome CoapplicantIncome\nN             0.2682927        0.1463415     0.003576320         0.0571435\nY             0.1902017        0.1325648    -0.001690249        -0.0270073\n   LoanAmount Loan_Amount_Term Credit_History1 Property_AreaSemiurban\nN  0.07966414      0.016992352       0.5548780              0.2682927\nY -0.03765106     -0.008030968       0.9798271              0.4409222\n  Property_AreaUrban\nN          0.3719512\nY          0.2997118\n\nCoefficients of linear discriminants:\n                                LD1\nGenderMale              0.185159211\nMarriedYes              0.375755462\nDependents1            -0.209004726\nDependents2             0.137509542\nDependents3+            0.007142953\nEducationNot Graduate  -0.294391997\nSelf_EmployedYes       -0.025905262\nApplicantIncome        -0.012085555\nCoapplicantIncome      -0.106529320\nLoanAmount             -0.099136040\nLoan_Amount_Term       -0.049820158\nCredit_History1         3.073804026\nProperty_AreaSemiurban  0.616732100\nProperty_AreaUrban      0.066231320\n\n\n\n\nCode\n# prediction from lda model\nlda_predict <- lda_model %>% \n  predict(testing)\n\n\n\n\nCode\n# accuracy\nmean(lda_predict$class==testing$Loan_Status)\n\n\n[1] 0.8110236\n\n\nCode\nconfusionMatrix(lda_predict$class, testing$Loan_Status)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  N  Y\n         N 19  2\n         Y 22 84\n                                        \n               Accuracy : 0.811         \n                 95% CI : (0.732, 0.875)\n    No Information Rate : 0.6772        \n    P-Value [Acc > NIR] : 0.0005479     \n                                        \n                  Kappa : 0.5046        \n                                        \n Mcnemar's Test P-Value : 0.0001052     \n                                        \n            Sensitivity : 0.4634        \n            Specificity : 0.9767        \n         Pos Pred Value : 0.9048        \n         Neg Pred Value : 0.7925        \n             Prevalence : 0.3228        \n         Detection Rate : 0.1496        \n   Detection Prevalence : 0.1654        \n      Balanced Accuracy : 0.7201        \n                                        \n       'Positive' Class : N             \n                                        \n\n\nLDA model accuracy comes out as ~81%"
  },
  {
    "objectID": "posts/Loan Assessment/LendingApproval.html#k-nearest-neighbor-knn",
    "href": "posts/Loan Assessment/LendingApproval.html#k-nearest-neighbor-knn",
    "title": "Loan Approval",
    "section": "K-nearest neighbor (KNN)",
    "text": "K-nearest neighbor (KNN)\n\n\nCode\n# KNN model\nset.seed(622)\ntrain.knn <- training[, names(training) != \"Direction\"]\nprep <- preProcess(x = train.knn, method = c(\"center\", \"scale\"))\nprep\n\n\nCreated from 384 samples and 12 variables\n\nPre-processing:\n  - centered (4)\n  - ignored (8)\n  - scaled (4)\n\n\nCode\ncl <- trainControl(method=\"repeatedcv\", repeats = 5) \nknn_model <- train(Loan_Status ~ ., data = training, \n                method = \"knn\", \n                trControl = cl, \n                preProcess = c(\"center\",\"scale\"), \n                tuneLength = 20)\nknn_model \n\n\nk-Nearest Neighbors \n\n384 samples\n 11 predictor\n  2 classes: 'N', 'Y' \n\nPre-processing: centered (14), scaled (14) \nResampling: Cross-Validated (10 fold, repeated 5 times) \nSummary of sample sizes: 346, 346, 346, 345, 346, 345, ... \nResampling results across tuning parameters:\n\n  k   Accuracy   Kappa    \n   5  0.7590209  0.3575145\n   7  0.7689406  0.3751117\n   9  0.7725985  0.3746036\n  11  0.7782524  0.3884713\n  13  0.7829615  0.3994494\n  15  0.7787908  0.3849571\n  17  0.7689528  0.3503238\n  19  0.7642294  0.3309567\n  21  0.7647551  0.3299468\n  23  0.7548097  0.2947001\n  25  0.7454298  0.2607665\n  27  0.7412733  0.2448000\n  29  0.7402746  0.2423939\n  31  0.7365911  0.2319391\n  33  0.7292901  0.2050164\n  35  0.7251066  0.1885755\n  37  0.7178596  0.1615955\n  39  0.7126768  0.1383143\n  41  0.7100317  0.1282468\n  43  0.7084798  0.1212857\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was k = 13.\n\n\n\n\nCode\n# prediction from knn model\nplot(knn_model)\n\n\n\n\n\nCode\nknn_predict <- predict(knn_model,newdata = testing)\nmean(knn_predict == testing$Loan_Status) # accuracy\n\n\n[1] 0.7952756\n\n\nCode\nconfusionMatrix(knn_predict, testing$Loan_Status)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  N  Y\n         N 17  2\n         Y 24 84\n                                          \n               Accuracy : 0.7953          \n                 95% CI : (0.7146, 0.8617)\n    No Information Rate : 0.6772          \n    P-Value [Acc > NIR] : 0.002202        \n                                          \n                  Kappa : 0.4553          \n                                          \n Mcnemar's Test P-Value : 3.814e-05       \n                                          \n            Sensitivity : 0.4146          \n            Specificity : 0.9767          \n         Pos Pred Value : 0.8947          \n         Neg Pred Value : 0.7778          \n             Prevalence : 0.3228          \n         Detection Rate : 0.1339          \n   Detection Prevalence : 0.1496          \n      Balanced Accuracy : 0.6957          \n                                          \n       'Positive' Class : N               \n                                          \n\n\nKNN model accuracy comes out as ~80%"
  },
  {
    "objectID": "posts/Loan Assessment/LendingApproval.html#decision-trees",
    "href": "posts/Loan Assessment/LendingApproval.html#decision-trees",
    "title": "Loan Approval",
    "section": "Decision Trees",
    "text": "Decision Trees\n\n\nCode\n# Decision Trees model\nset.seed(622)\ntree.loans = tree(Loan_Status~., data=training)\nsummary(tree.loans)\n\n\n\nClassification tree:\ntree(formula = Loan_Status ~ ., data = training)\nVariables actually used in tree construction:\n[1] \"Credit_History\"    \"Property_Area\"     \"CoapplicantIncome\"\nNumber of terminal nodes:  5 \nResidual mean deviance:  0.921 = 349 / 379 \nMisclassification error rate: 0.1849 = 71 / 384 \n\n\nCode\nplot(tree.loans)\ntext(tree.loans, pretty = 0)\n\n\n\n\n\n\n\nCode\n# prediction from decision tree model\ntree.predict<-predict(tree.loans, testing, type = 'class')\nmean(tree.predict == testing$Loan_Status) # accuracy\n\n\n[1] 0.7952756\n\n\nCode\nconfusionMatrix(tree.predict, testing$Loan_Status)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  N  Y\n         N 19  4\n         Y 22 82\n                                          \n               Accuracy : 0.7953          \n                 95% CI : (0.7146, 0.8617)\n    No Information Rate : 0.6772          \n    P-Value [Acc > NIR] : 0.0022025       \n                                          \n                  Kappa : 0.471           \n                                          \n Mcnemar's Test P-Value : 0.0008561       \n                                          \n            Sensitivity : 0.4634          \n            Specificity : 0.9535          \n         Pos Pred Value : 0.8261          \n         Neg Pred Value : 0.7885          \n             Prevalence : 0.3228          \n         Detection Rate : 0.1496          \n   Detection Prevalence : 0.1811          \n      Balanced Accuracy : 0.7085          \n                                          \n       'Positive' Class : N               \n                                          \n\n\nDecision Tree model accuracy comes out as ~80%"
  },
  {
    "objectID": "posts/Loan Assessment/LendingApproval.html#random-forests",
    "href": "posts/Loan Assessment/LendingApproval.html#random-forests",
    "title": "Loan Approval",
    "section": "Random Forests",
    "text": "Random Forests\n\n\nCode\nset.seed(622)\n# Random Forest model\nrf.loans <- randomForest(Loan_Status~., data = training)\nrf.loans\n\n\n\nCall:\n randomForest(formula = Loan_Status ~ ., data = training) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 3\n\n        OOB estimate of  error rate: 20.57%\nConfusion matrix:\n   N   Y class.error\nN 59  64  0.52032520\nY 15 246  0.05747126\n\n\n\n\nCode\n# prediction from random forest model\nrf.predict <- predict(rf.loans, testing,type='class')\nmean(rf.predict == testing$Loan_Status) # accuracy\n\n\n[1] 0.8110236\n\n\nCode\nconfusionMatrix(rf.predict, testing$Loan_Status)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  N  Y\n         N 22  5\n         Y 19 81\n                                        \n               Accuracy : 0.811         \n                 95% CI : (0.732, 0.875)\n    No Information Rate : 0.6772        \n    P-Value [Acc > NIR] : 0.0005479     \n                                        \n                  Kappa : 0.5254        \n                                        \n Mcnemar's Test P-Value : 0.0079635     \n                                        \n            Sensitivity : 0.5366        \n            Specificity : 0.9419        \n         Pos Pred Value : 0.8148        \n         Neg Pred Value : 0.8100        \n             Prevalence : 0.3228        \n         Detection Rate : 0.1732        \n   Detection Prevalence : 0.2126        \n      Balanced Accuracy : 0.7392        \n                                        \n       'Positive' Class : N             \n                                        \n\n\nRandom Forest model accuracy comes out as ~80%"
  },
  {
    "objectID": "posts/Stock Prediction/arima.html",
    "href": "posts/Stock Prediction/arima.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "A good forecast is a blessing while the wrong forecast could prove to be dangerous"
  },
  {
    "objectID": "posts/Stock Prediction/arima.html#introduction",
    "href": "posts/Stock Prediction/arima.html#introduction",
    "title": "Time Series Analysis",
    "section": "Introduction",
    "text": "Introduction\nGiven an unknown data source with several groups, we attempt to predict the next 140 values of a times series data set based on 1622 entries provided on multiple events. Our predictions will be fine-tuned to reduce the mean absolute percentage error (MAPE) as much as possible. The packages we will be using and all associated code to produce the models can be found in the attached markdown file. The data with its first five rows, are shown below.\n\n\nCode\n# Packages\nlibrary(tidyverse)\nlibrary(kableExtra)\nlibrary(fpp2)\nlibrary(imputeTS)\nlibrary(forecast)\nlibrary(readxl)\nlibrary(fma)\nlibrary(tsoutliers)\nlibrary(psych)\nlibrary(kableExtra)\nlibrary(zoo)\nlibrary(xts)\nlibrary(urca)\nlibrary(ROCR)\nlibrary(TSstudio)\nlibrary(stringr)\n# Data source\ndata <- read.csv(\"https://raw.githubusercontent.com/palmorezm/msds/main/Predictive%20Analytics/Projects/Project1/project1data.csv\")\n# data <- data %>% \n#   rename(SeriesInd = ï..SeriesInd) \nhead(data, 5)\n\n\n\n\n  \n\n\n\nWe create forecasts for two preselected variables within each of six predetermined groups. These groups are denoted S01, S02, S03, S04, S05, and S06 respectively. There are five variables within each group that we have to work with. They are Var01, Var02, Var03, Var05, and Var07 respectively. Our date variable ‘SeriesInd,’ is displayed in its numeric serial number form calculated with Excel. Although we do not know what the variables stand for, we can develop models to try and forecast their behavior. This chart contains a breakdown of which variables are forecast in each group.\n\n\nCode\n# Chart\nvarsbygroup <- data.frame(matrix(c(\"S01\", \"S02\", \"S03\",\n                                   \"S04\", \"S05\", \"S06\", \n                                   \"Var01\", \"Var02\", \"Var05\",\n                                   \"Var01\", \"Var02\", \"Var05\",\n                                   \"Var02\", \"Var03\", \"Var07\",\n                                   \"Var02\", \"Var03\", \"Var07\"),\n                                 nrow = 6, ncol=3))\ncolnames(varsbygroup) <- c(\"Group\", \"Variable1\", \"Variable2\")\nvarsbygroup %>% \n  kbl(booktabs = T) %>% \n  kable_styling(latex_options = c(\"striped\", \"HOLD_position\", \"scale_down\"), full_width = T)\n\n\n\n\n \n  \n    Group \n    Variable1 \n    Variable2 \n  \n \n\n  \n    S01 \n    Var01 \n    Var02 \n  \n  \n    S02 \n    Var02 \n    Var03 \n  \n  \n    S03 \n    Var05 \n    Var07 \n  \n  \n    S04 \n    Var01 \n    Var02 \n  \n  \n    S05 \n    Var02 \n    Var03 \n  \n  \n    S06 \n    Var05 \n    Var07 \n  \n\n\n\n\n\nCode\n# Grouping\nS01 <- data %>% \n  filter(group == \"S01\")\nS02 <- data %>% \n  filter(group == \"S02\")\nS03 <- data %>% \n  filter(group == \"S03\")\nS04 <- data %>% \n  filter(group == \"S04\")\nS05 <- data %>% \n  filter(group == \"S05\")\nS06 <- data %>% \n  filter(group == \"S06\")\n\n# Imputation by function - missing something? lapply/sapply may work \nsoximp <- function(df){\n  for (i in colnames(df)){\n    if (sum(is.na(df[[i]])) !=0){\n      df[[i]][is.na(df[[i]])] <- median(df[[i]], na.rm=TRUE)\n    }\n  }\n}\n\n# Imputation loops for each group by median \nfor (i in colnames(S01)){\n  if (sum(is.na(S01[[i]])) != 0){\n    S01[[i]][is.na(S01[[i]])] <- median(S01[[i]], na.rm = TRUE)\n  } \n}\nfor (i in colnames(S02)){\n  if (sum(is.na(S02[[i]])) != 0){\n    S02[[i]][is.na(S02[[i]])] <- median(S02[[i]], na.rm = TRUE)\n  } \n}\nfor (i in colnames(S03)){\n  if (sum(is.na(S03[[i]])) != 0){\n    S03[[i]][is.na(S03[[i]])] <- median(S03[[i]], na.rm = TRUE)\n  } \n}\nfor (i in colnames(S04)){\n  if (sum(is.na(S04[[i]])) != 0){\n    S04[[i]][is.na(S04[[i]])] <- median(S04[[i]], na.rm = TRUE)\n  } \n}\nfor (i in colnames(S05)){\n  if (sum(is.na(S05[[i]])) != 0){\n    S05[[i]][is.na(S05[[i]])] <- median(S05[[i]], na.rm = TRUE)\n  } \n}\nfor (i in colnames(S06)){\n  if (sum(is.na(S06[[i]])) != 0){\n    S06[[i]][is.na(S06[[i]])] <- median(S06[[i]], na.rm = TRUE)\n  } \n}\n\n\nBefore we begin, the data is filtered to extract each time series by group. This isolates the Var01, Var02, Var03, Var05, and Var07 variables associated with groups S01, S02, and so on. Then, with each group and its respective variables’ behavior isolated, we clean and adjust the data to make use of it in the analysis. Once we determine the most appropriate models to forecast the proper variable in each group, we evaluate the results of our predictions. Our final forecasts are captured in the excel spreadsheet attached."
  },
  {
    "objectID": "posts/Stock Prediction/arima.html#analysis",
    "href": "posts/Stock Prediction/arima.html#analysis",
    "title": "Time Series Analysis",
    "section": "Analysis",
    "text": "Analysis\nWe began by addressing missing values. Given 10,572 observations, about 8% of each variable was missing. Several methods were tried to address this but the best were Kalman smoothing and simple imputation by the median of each ‘Var0X’ variable to fill in where appropriate. The ‘SeriesInd’ numeric date was also converted from its serial number form to a common date-time series. We then examined each group’s variables separately.\n\n\nCode\n# library(fpp2)\n\n#S01\nS01<-subset(data, group == \"S01\", select = c(SeriesInd, Var01, Var02))%>%\n  mutate(date=as.Date(SeriesInd, origin = \"1905-01-01\"))\nsummary(S01)\n\n\n   SeriesInd         Var01           Var02               date           \n Min.   :40669   Min.   :23.01   Min.   : 1339900   Min.   :2016-05-07  \n 1st Qu.:41304   1st Qu.:29.85   1st Qu.: 5347550   1st Qu.:2018-01-31  \n Median :41946   Median :35.66   Median : 7895050   Median :2019-11-05  \n Mean   :41945   Mean   :39.41   Mean   : 8907092   Mean   :2019-11-03  \n 3rd Qu.:42586   3rd Qu.:48.70   3rd Qu.:11321675   3rd Qu.:2021-08-06  \n Max.   :43221   Max.   :62.31   Max.   :48477500   Max.   :2023-05-03  \n                 NA's   :142     NA's   :140                            \n\n\nCode\n# Subset Var01 and Var02 from S01.\nS01_Var01<-S01 %>%select(Var01)\nS01_Var01<-S01_Var01[1:1625,]\n\n\nS01_Var02<-S01 %>%select(Var02)\nS01_Var02<-S01_Var02[1:1625,]\n\n\n#S02\nS02<-subset(data, group == \"S02\", select = c(SeriesInd, Var02, Var03))%>%\n  mutate(date=as.Date(SeriesInd, origin = \"1905-01-01\"))\nsummary(S02)\n\n\n   SeriesInd         Var02               Var03            date           \n Min.   :40669   Min.   :  7128800   Min.   : 8.82   Min.   :2016-05-07  \n 1st Qu.:41304   1st Qu.: 27880300   1st Qu.:11.82   1st Qu.:2018-01-31  \n Median :41946   Median : 39767500   Median :13.76   Median :2019-11-05  \n Mean   :41945   Mean   : 50633098   Mean   :13.68   Mean   :2019-11-03  \n 3rd Qu.:42586   3rd Qu.: 59050900   3rd Qu.:15.52   3rd Qu.:2021-08-06  \n Max.   :43221   Max.   :480879500   Max.   :38.28   Max.   :2023-05-03  \n                 NA's   :140         NA's   :144                         \n\n\nCode\n# Subset Var02 and Var03 from S02.\nS02_Var02<-S02 %>%select(Var02)\nS02_Var02<-S02_Var02[1:1625,]\n\n\nS02_Var03<-S02 %>%select(Var03)\nS02_Var03<-S02_Var03[1:1625,]\n\n\n\n#S03\nS03<-subset(data, group == \"S03\", select = c(SeriesInd, Var05, Var07))%>%\n  mutate(date=as.Date(SeriesInd, origin = \"1905-01-01\"))\nsummary(S03)\n\n\n   SeriesInd         Var05            Var07             date           \n Min.   :40669   Min.   : 27.48   Min.   : 27.44   Min.   :2016-05-07  \n 1st Qu.:41304   1st Qu.: 53.30   1st Qu.: 53.46   1st Qu.:2018-01-31  \n Median :41946   Median : 75.59   Median : 75.71   Median :2019-11-05  \n Mean   :41945   Mean   : 76.90   Mean   : 76.87   Mean   :2019-11-03  \n 3rd Qu.:42586   3rd Qu.: 98.55   3rd Qu.: 98.61   3rd Qu.:2021-08-06  \n Max.   :43221   Max.   :134.46   Max.   :133.00   Max.   :2023-05-03  \n                 NA's   :144      NA's   :144                          \n\n\nCode\n# Subset Var05 and Var07 from S03.\nS03_Var05<-S03 %>%select(Var05)\nS03_Var05<-S03_Var05[1:1625,]\n\n\nS03_Var07<-S03 %>%select(Var07)\nS03_Var07<-S03_Var07[1:1625,]\n\n\nStatistical summaries, box plots, and histograms were run on each group to evaluate where the average value of each variable was, if its distribution was skewed, determine whether outliers were present, and provide other descriptors of the data. These informed us that the average value (mean) of the variables are similar but their range varies widely with Var05 at 186.01 while Var02 covers a range of 479 million. Our analysis solves this potential problem by focusing on variables of the same scales as the intended target.\n\n\nCode\n# library(imputeTS)\n# Summarize the subset data.\n\nsummary(S01_Var01)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  23.01   29.85   35.66   39.41   48.70   62.31       5 \n\n\nCode\nsummary(S01_Var02)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's \n 1339900  5347550  7895050  8907092 11321675 48477500        3 \n\n\nCode\nsummary(S02_Var02)\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max.      NA's \n  7128800  27880300  39767500  50633098  59050900 480879500         3 \n\n\nCode\nsummary(S02_Var03)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   8.82   11.82   13.76   13.68   15.52   38.28       7 \n\n\nCode\nsummary(S03_Var07)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  27.44   53.46   75.71   76.87   98.61  133.00       7 \n\n\nCode\nsummary(S03_Var05)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  27.48   53.30   75.59   76.90   98.55  134.46       7 \n\n\nCode\n# according to the summary of subsets, \n# S01_Var01 has 5 NAs\n# S01_Var02 has 3 NAs\n# S02_Var02 has 3 NAs\n# S02_Var03 has 7 NAs\n# S03_Var07 has 7 NAs\n# S03_Var05 has 7 NAs\n\n# Using Kalman Smoothing to impute NAs.\nS01_Var01<-na_kalman(S01_Var01)\nS01_Var02<-na_kalman(S01_Var02)\nS02_Var02<-na_kalman(S02_Var02)\nS02_Var03<-na_kalman(S02_Var03)\nS03_Var05<-na_kalman(S03_Var05)\nS03_Var07<-na_kalman(S03_Var07)\n\nsummary(S01_Var01)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  23.01   29.85   35.72   39.47   48.76   62.38 \n\n\nCode\nsummary(S01_Var02)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n 1339900  5350400  7874900  8901799 11312300 48477500 \n\n\nCode\nsummary(S02_Var02)\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n  7128800  27829000  39737500  50581126  59017900 480879500 \n\n\nCode\nsummary(S02_Var03)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   8.82   11.82   13.75   13.68   15.52   38.28 \n\n\nCode\nsummary(S03_Var07)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  27.44   53.56   75.77   76.95   98.46  133.00 \n\n\nCode\nsummary(S03_Var05)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  27.48   53.35   75.70   76.99   98.57  134.46 \n\n\nCode\n# NA  no longer exists\nts_S01_Var01<-ts(S01_Var01)\nts_S01_Var02<-ts(S01_Var02)\nts_S02_Var02<-ts(S02_Var02)\nts_S02_Var03<-ts(S02_Var03)\nts_S03_Var05<-ts(S03_Var05)\nts_S03_Var07<-ts(S03_Var07)\n\nstr(ts_S01_Var01)\n\n\n Time-Series [1:1625] from 1 to 1625: 26.6 26.3 26 25.8 26.3 ...\n\n\nCode\nstr(ts_S01_Var02)\n\n\n Time-Series [1:1625] from 1 to 1625: 10369300 10943800 8933800 10775400 12875600 ...\n\n\nCode\nstr(ts_S02_Var02)\n\n\n Time-Series [1:1625] from 1 to 1625: 6.09e+07 2.16e+08 2.00e+08 1.30e+08 1.30e+08 ...\n\n\nCode\nstr(ts_S02_Var03)\n\n\n Time-Series [1:1625] from 1 to 1625: 10.1 10.4 11.1 11.3 11.5 ...\n\n\nCode\nstr(ts_S03_Var05)\n\n\n Time-Series [1:1625] from 1 to 1625: 30.5 30.7 30.6 30.2 30 ...\n\n\nCode\nstr(ts_S03_Var07)\n\n\n Time-Series [1:1625] from 1 to 1625: 30.6 30.6 30.1 30.1 30.3 ...\n\n\nCode\nautoplot(ts_S01_Var01)\n\n\n\n\n\nCode\nautoplot(ts_S01_Var02)\n\n\n\n\n\nCode\nautoplot(ts_S02_Var02)\n\n\n\n\n\nCode\nautoplot(ts_S02_Var03)\n\n\n\n\n\nCode\nautoplot(ts_S03_Var05)\n\n\n\n\n\nCode\nautoplot(ts_S03_Var07)\n\n\n\n\n\nCode\npar(mfrow = c(1,2))\nhist(ts_S01_Var01)\nboxplot(ts_S01_Var01)\n\n\n\n\n\nCode\npar(mfrow = c(1,2))\nhist(ts_S01_Var02)\nboxplot(ts_S01_Var02)\n\n\n\n\n\nCode\npar(mfrow = c(1,2))\nhist(ts_S02_Var02)\nboxplot(ts_S02_Var02)\n\n\n\n\n\nCode\npar(mfrow = c(1,2))\nhist(ts_S02_Var03)\nboxplot(ts_S02_Var03)\n\n\n\n\n\nCode\npar(mfrow = c(1,2))\nhist(ts_S03_Var05)\nboxplot(ts_S03_Var05)\n\n\n\n\n\nCode\npar(mfrow = c(1,2))\nhist(ts_S03_Var07)\nboxplot(ts_S03_Var07)\n\n\n\n\n\nAdditionally, all but group S03 of the histograms exhibited right skewness, and Var02 and Var03 had outliers. These were replaced using Friedman’s super smoothing method. Due to the randomness of these variables, determining outliers was difficult and there is a presence of additional overly influential points as determined using Cook’s distance formula. We acknowledge the presence of these points but are unable to alter them as they are likely intentional based on the patterns in the data. For reference, the observations are shown in the scatter plot with color coding by each group.\n\n\nCode\ndata[c(1:7)]%>%\n  gather(variable, value, -SeriesInd, -group) %>%\n  ggplot(., aes(value, SeriesInd, color = group)) + \n  geom_point(fill = \"white\",\n             size=1, \n             shape=21, \n             alpha = 0.75) + \n  coord_flip() + \n   facet_wrap(~variable, \n             scales =\"free\") + \n  labs(title = \"Variable Patterns\", \n       subtitle = \"Color Coded by Group\", \n       x=\"Value\", \n       y=\"Time\", \n       caption = \"Contains all non-null observations of the given data set\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust=0.5), \n        plot.subtitle = element_text(hjust=0.5),\n        legend.position = \"bottom\", \n        axis.ticks.x=element_blank(),\n        axis.text.x=element_blank(), \n        plot.caption = element_text(hjust=0.5)\n        )\n\n\nWarning: Removed 4294 rows containing missing values (geom_point).\n\n\n\n\n\nSeasonality was also considered. It is possible this data follows a weak seasonal trend that increases during summer months but there is not a lot of evidence to support regular fluctuations. Regular gaps were noticed in the time series on a weekly basis and several methods were used in attempts to fix this. However, the data appears randomly distributed and as such, acts randomly. For this reason, we left the gaps alone and any further adjustments made were minimal to avoid disturbing any existing patterns in the data.\n\n\nCode\nndiffs(ts_S01_Var01)\n\n\n[1] 1\n\n\nCode\nts_S01_Var01%>%diff()%>%ndiffs()\n\n\n[1] 0\n\n\nCode\n# ndiffs test for ts_S01_Var02.\nndiffs(ts_S01_Var02)\n\n\n[1] 1\n\n\nCode\nts_S01_Var02%>%diff()%>%ndiffs()\n\n\n[1] 0\n\n\nCode\n# ndiffs test for ts_S02_Var02\nndiffs(ts_S02_Var02)\n\n\n[1] 1\n\n\nCode\nts_S02_Var02%>%diff()%>%ndiffs()\n\n\n[1] 0\n\n\nCode\n# ndiffs test for ts_S02_Var03.\nndiffs(ts_S02_Var03)\n\n\n[1] 1\n\n\nCode\nts_S02_Var02%>%diff()%>%ndiffs()\n\n\n[1] 0\n\n\nCode\n# ndiffs test for ts_S03_Var05.\nndiffs(ts_S03_Var05)\n\n\n[1] 1\n\n\nCode\nts_S03_Var05%>%diff()%>%ndiffs()\n\n\n[1] 0\n\n\nCode\n# ndiffs test for ts_S03_Var07.\nndiffs(ts_S03_Var07)\n\n\n[1] 1\n\n\nCode\nts_S03_Var07%>%diff()%>%ndiffs()\n\n\n[1] 0\n\n\nCode\n# According to the ndiffs test, all the variables above require difference.\n\n\nWe determined that the best model type was an Auto Regressive Integrated Moving Average (ARIMA) with drift. Unfortunately, all variables required differencing to achieve stationarity. This indicates that any predictions made with these variables may be unrealistic because of inherent random changes in statistics like the mean and variance of these variables over time. We transform the data in our attempts to achieve stationarity but it should be noted that our review of stationarity is only a rough estimate using the aforementioned summary statistics so that we may apply this ARIMA method. Otherwise, we would have to conclude this data is inherently unpredictable and as such, render model forecasts useless. Rather, we focus on forecasting each variable individually and try to keep it simple."
  },
  {
    "objectID": "posts/Stock Prediction/arima.html#prediction",
    "href": "posts/Stock Prediction/arima.html#prediction",
    "title": "Time Series Analysis",
    "section": "Prediction",
    "text": "Prediction\n\n\nCode\ntrain0101<-window(ts_S01_Var01, end=as.integer(length(ts_S01_Var01)*0.7))\ntrain0102<-window(ts_S01_Var02, end=as.integer(length(ts_S01_Var02)*0.7))\ntrain0202<-window(ts_S02_Var02, end=as.integer(length(ts_S02_Var02)*0.7))\ntrain0203<-window(ts_S02_Var03, end=as.integer(length(ts_S02_Var03)*0.7))\ntrain0305<-window(ts_S03_Var05, end=as.integer(length(ts_S03_Var05)*0.7))\ntrain0307<-window(ts_S03_Var07, end=as.integer(length(ts_S03_Var07)*0.7))\nlength(ts_S01_Var01)*0.3\n\n\n[1] 487.5\n\n\nCode\n# library(dplyr)\n# library(forecast)\nAA_fit0101 <- train0101 %>% auto.arima(stepwise = FALSE, approximation =FALSE, seasonal = TRUE) %>% forecast(h=488)\nAA_fit0102 <- train0102 %>% auto.arima(stepwise = FALSE, approximation =FALSE, seasonal = TRUE) %>% forecast(h=488)\nAA_fit0202 <- train0202 %>% auto.arima(stepwise = FALSE, approximation =FALSE, seasonal = TRUE) %>% forecast(h=488)\nAA_fit0203 <- train0203 %>% auto.arima(stepwise = FALSE, approximation =FALSE, seasonal = TRUE) %>% forecast(h=488)\nAA_fit0305 <- train0305 %>% auto.arima(stepwise = FALSE, approximation =FALSE, seasonal = TRUE) %>% forecast(h=488)\nAA_fit0307 <- train0307 %>% auto.arima(stepwise = FALSE, approximation =FALSE, seasonal = TRUE) %>% forecast(h=488)\n\n\n\nmape0101<-accuracy(AA_fit0101,ts_S01_Var01)[\"Test set\", \"MAPE\"]\nmape0101\n\n\n[1] 5.7693\n\n\nCode\nmape0102<-accuracy(AA_fit0102,ts_S01_Var02)[\"Test set\",\"MAPE\"]\nmape0102\n\n\n[1] 30.40054\n\n\nCode\nmape0202<-accuracy(AA_fit0202,ts_S02_Var02)[\"Test set\", \"MAPE\"]\nmape0202\n\n\n[1] 78.81883\n\n\nCode\nmape0203<-accuracy(AA_fit0203,ts_S02_Var03)[\"Test set\", \"MAPE\"]\nmape0203\n\n\n[1] 19.28435\n\n\nCode\nmape0305<-accuracy(AA_fit0305,ts_S03_Var05)[\"Test set\", \"MAPE\"]\nmape0101\n\n\n[1] 5.7693\n\n\nCode\nmape0307<-accuracy(AA_fit0307,ts_S03_Var07)[\"Test set\", \"MAPE\"]\nmape0307\n\n\n[1] 11.09277\n\n\nCode\nAA0101<-auto.arima(ts_S01_Var01, stepwise = F, approximation = F, seasonal = T)\nfcast0101<-forecast(AA0101,h=140)\nplot(fcast0101)\n\n\n\n\n\nCode\nAA0102<-auto.arima(ts_S01_Var02, stepwise = F, approximation = F, seasonal = T)\nfcast0102<-forecast(AA0102,h=140)\nplot(fcast0102)\n\n\n\n\n\nCode\nAA0202<-auto.arima(ts_S02_Var02, stepwise = F, approximation = F, seasonal = T)\nfcast0202<-forecast(AA0202,h=140)\nplot(fcast0202)\n\n\n\n\n\nCode\nAA0203<-auto.arima(ts_S02_Var03, stepwise = F, approximation = F, seasonal = T)\nfcast0203<-forecast(AA0203,h=140)\nplot(fcast0203)\n\n\n\n\n\nCode\nAA0305<-auto.arima(ts_S03_Var05, stepwise = F, approximation = F, seasonal = T)\nfcast0305<-forecast(AA0305,h=140)\nplot(fcast0305)\n\n\n\n\n\nCode\nAA0307<-auto.arima(ts_S03_Var07, stepwise = F, approximation = F, seasonal = T)\nfcast0307<-forecast(AA0307,h=140)\nplot(fcast0307)\n\n\n\n\n\nCode\nfcast0101\n\n\n     Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n1626       62.39704 61.74274 63.05135 61.39637 63.39772\n1627       62.41922 61.45258 63.38587 60.94086 63.89758\n1628       62.44125 61.26868 63.61382 60.64795 64.23454\n1629       62.46327 61.11589 63.81065 60.40263 64.52392\n1630       62.48530 60.98331 63.98728 60.18821 64.78238\n1631       62.50732 60.86523 64.14942 59.99596 65.01869\n1632       62.52935 60.75819 64.30050 59.82060 65.23809\n1633       62.55137 60.65994 64.44280 59.65868 65.44406\n1634       62.57340 60.56890 64.57790 59.50778 65.63901\n1635       62.59542 60.48390 64.70695 59.36612 65.82472\n1636       62.61745 60.40407 64.83083 59.23237 66.00252\n1637       62.63947 60.32872 64.95022 59.10548 66.17346\n1638       62.66150 60.25732 65.06568 58.98462 66.33837\n1639       62.68352 60.18941 65.17763 58.86910 66.49794\n1640       62.70555 60.12463 65.28646 58.75838 66.65271\n1641       62.72757 60.06268 65.39246 58.65198 66.80317\n1642       62.74960 60.00330 65.49589 58.54950 66.94969\n1643       62.77162 59.94626 65.59698 58.45060 67.09264\n1644       62.79364 59.89137 65.69592 58.35500 67.23229\n1645       62.81567 59.83847 65.79286 58.26244 67.36890\n1646       62.83769 59.78741 65.88797 58.17269 67.50270\n1647       62.85972 59.73806 65.98137 58.08556 67.63388\n1648       62.88174 59.69031 66.07318 58.00087 67.76262\n1649       62.90377 59.64405 66.16349 57.91846 67.88908\n1650       62.92579 59.59919 66.25239 57.83820 68.01339\n1651       62.94782 59.55565 66.33998 57.75995 68.13569\n1652       62.96984 59.51336 66.42633 57.68360 68.25608\n1653       62.99187 59.47223 66.51150 57.60905 68.37468\n1654       63.01389 59.43223 66.59556 57.53621 68.49158\n1655       63.03592 59.39327 66.67856 57.46498 68.60686\n1656       63.05794 59.35533 66.76056 57.39528 68.72061\n1657       63.07997 59.31833 66.84160 57.32705 68.83289\n1658       63.10199 59.28225 66.92173 57.26021 68.94378\n1659       63.12402 59.24704 67.00099 57.19470 69.05333\n1660       63.14604 59.21267 67.07942 57.13047 69.16162\n1661       63.16807 59.17909 67.15704 57.06745 69.26868\n1662       63.19009 59.14627 67.23391 57.00561 69.37458\n1663       63.21212 59.11419 67.31004 56.94488 69.47935\n1664       63.23414 59.08282 67.38547 56.88524 69.58304\n1665       63.25617 59.05212 67.46021 56.82663 69.68570\n1666       63.27819 59.02207 67.53431 56.76902 69.78736\n1667       63.30022 58.99266 67.60777 56.71238 69.88805\n1668       63.32224 58.96385 67.68063 56.65666 69.98782\n1669       63.34426 58.93563 67.75290 56.60184 70.08669\n1670       63.36629 58.90797 67.82461 56.54788 70.18470\n1671       63.38831 58.88086 67.89576 56.49476 70.28186\n1672       63.41034 58.85429 67.96639 56.44246 70.37822\n1673       63.43236 58.82822 68.03651 56.39093 70.47379\n1674       63.45439 58.80265 68.10613 56.34017 70.56861\n1675       63.47641 58.77757 68.17526 56.29015 70.66268\n1676       63.49844 58.75295 68.24393 56.24084 70.75604\n1677       63.52046 58.72878 68.31214 56.19222 70.84871\n1678       63.54249 58.70506 68.37992 56.14428 70.94070\n1679       63.56451 58.68176 68.44726 56.09699 71.03203\n1680       63.58654 58.65889 68.51419 56.05035 71.12273\n1681       63.60856 58.63641 68.58071 56.00432 71.21281\n1682       63.63059 58.61434 68.64684 55.95889 71.30228\n1683       63.65261 58.59264 68.71258 55.91406 71.39117\n1684       63.67464 58.57132 68.77795 55.86979 71.47948\n1685       63.69666 58.55037 68.84295 55.82609 71.56724\n1686       63.71869 58.52977 68.90760 55.78293 71.65445\n1687       63.74071 58.50952 68.97190 55.74030 71.74113\n1688       63.76274 58.48961 69.03586 55.69818 71.82729\n1689       63.78476 58.47003 69.09949 55.65658 71.91294\n1690       63.80679 58.45077 69.16280 55.61547 71.99811\n1691       63.82881 58.43183 69.22579 55.57484 72.08278\n1692       63.85084 58.41319 69.28848 55.53468 72.16699\n1693       63.87286 58.39486 69.35086 55.49499 72.25073\n1694       63.89489 58.37683 69.41294 55.45574 72.33403\n1695       63.91691 58.35908 69.47474 55.41694 72.41688\n1696       63.93893 58.34161 69.53626 55.37857 72.49930\n1697       63.96096 58.32443 69.59749 55.34062 72.58129\n1698       63.98298 58.30751 69.65846 55.30309 72.66288\n1699       64.00501 58.29086 69.71916 55.26597 72.74405\n1700       64.02703 58.27446 69.77960 55.22924 72.82483\n1701       64.04906 58.25833 69.83979 55.19290 72.90522\n1702       64.07108 58.24244 69.89973 55.15694 72.98523\n1703       64.09311 58.22680 69.95942 55.12136 73.06486\n1704       64.11513 58.21139 70.01887 55.08614 73.14412\n1705       64.13716 58.19623 70.07809 55.05129 73.22303\n1706       64.15918 58.18129 70.13707 55.01679 73.30158\n1707       64.18121 58.16659 70.19583 54.98264 73.37978\n1708       64.20323 58.15210 70.25436 54.94883 73.45764\n1709       64.22526 58.13784 70.31268 54.91535 73.53517\n1710       64.24728 58.12378 70.37078 54.88220 73.61236\n1711       64.26931 58.10995 70.42867 54.84938 73.68924\n1712       64.29133 58.09631 70.48635 54.81687 73.76580\n1713       64.31336 58.08289 70.54383 54.78467 73.84204\n1714       64.33538 58.06966 70.60110 54.75278 73.91798\n1715       64.35741 58.05663 70.65818 54.72120 73.99361\n1716       64.37943 58.04379 70.71507 54.68991 74.06895\n1717       64.40146 58.03115 70.77176 54.65891 74.14400\n1718       64.42348 58.01869 70.82827 54.62820 74.21876\n1719       64.44551 58.00642 70.88459 54.59777 74.29324\n1720       64.46753 57.99433 70.94074 54.56762 74.36744\n1721       64.48956 57.98241 70.99670 54.53774 74.44137\n1722       64.51158 57.97068 71.05248 54.50813 74.51503\n1723       64.53360 57.95911 71.10810 54.47879 74.58842\n1724       64.55563 57.94772 71.16354 54.44971 74.66155\n1725       64.57765 57.93650 71.21881 54.42088 74.73443\n1726       64.59968 57.92544 71.27392 54.39231 74.80705\n1727       64.62170 57.91454 71.32886 54.36399 74.87942\n1728       64.64373 57.90381 71.38365 54.33591 74.95155\n1729       64.66575 57.89323 71.43827 54.30808 75.02343\n1730       64.68778 57.88281 71.49274 54.28048 75.09507\n1731       64.70980 57.87255 71.54706 54.25312 75.16648\n1732       64.73183 57.86243 71.60122 54.22600 75.23766\n1733       64.75385 57.85247 71.65524 54.19910 75.30861\n1734       64.77588 57.84265 71.70910 54.17243 75.37933\n1735       64.79790 57.83298 71.76282 54.14598 75.44983\n1736       64.81993 57.82345 71.81640 54.11975 75.52011\n1737       64.84195 57.81407 71.86984 54.09373 75.59017\n1738       64.86398 57.80482 71.92313 54.06793 75.66002\n1739       64.88600 57.79571 71.97629 54.04234 75.72966\n1740       64.90803 57.78674 72.02931 54.01696 75.79909\n1741       64.93005 57.77790 72.08220 53.99179 75.86832\n1742       64.95208 57.76920 72.13495 53.96681 75.93734\n1743       64.97410 57.76062 72.18758 53.94204 76.00616\n1744       64.99613 57.75218 72.24007 53.91747 76.07479\n1745       65.01815 57.74386 72.29244 53.89309 76.14322\n1746       65.04018 57.73567 72.34468 53.86890 76.21145\n1747       65.06220 57.72760 72.39680 53.84490 76.27950\n1748       65.08423 57.71966 72.44879 53.82109 76.34736\n1749       65.10625 57.71184 72.50066 53.79747 76.41503\n1750       65.12827 57.70413 72.55242 53.77403 76.48252\n1751       65.15030 57.69655 72.60405 53.75077 76.54982\n1752       65.17232 57.68908 72.65557 53.72770 76.61695\n1753       65.19435 57.68173 72.70697 53.70479 76.68390\n1754       65.21637 57.67450 72.75825 53.68207 76.75068\n1755       65.23840 57.66737 72.80943 53.65951 76.81728\n1756       65.26042 57.66036 72.86049 53.63713 76.88372\n1757       65.28245 57.65346 72.91144 53.61492 76.94998\n1758       65.30447 57.64667 72.96228 53.59287 77.01608\n1759       65.32650 57.63998 73.01301 53.57099 77.08201\n1760       65.34852 57.63341 73.06364 53.54927 77.14777\n1761       65.37055 57.62694 73.11416 53.52771 77.21338\n1762       65.39257 57.62057 73.16458 53.50632 77.27883\n1763       65.41460 57.61430 73.21489 53.48508 77.34412\n1764       65.43662 57.60814 73.26510 53.46400 77.40925\n1765       65.45865 57.60208 73.31521 53.44307 77.47422\n\n\nCode\nfcast0102\n\n\n     Point Forecast       Lo 80    Hi 80      Lo 95    Hi 95\n1626        5743772 1504215.083  9983329  -740070.7 12227615\n1627        5636139 1087600.319 10184677 -1320250.4 12592527\n1628        5577761  929998.521 10225524 -1530378.5 12685901\n1629        5534519  828946.217 10240091 -1662033.4 12731071\n1630        5497692  749190.107 10246193 -1764514.7 12759898\n1631        5464837  681814.076 10247859 -1850165.2 12779839\n1632        5435087  623542.207 10246631 -1923535.6 12793709\n1633        5407990  572592.367 10243388 -1987112.7 12803093\n1634        5383225  527714.803 10238736 -2042637.2 12809088\n1635        5360525  487934.056 10233116 -2091459.9 12812510\n1636        5339657  452459.011 10226856 -2134667.6 12813982\n1637        5320417  420638.335 10220196 -2173147.8 12813982\n1638        5302622  391931.556 10213312 -2207630.8 12812874\n1639        5286111  365887.695 10206333 -2238720.9 12810942\n1640        5270740  342128.662 10199350 -2266920.3 12808399\n1641        5256381  320336.094 10192427 -2292648.5 12805411\n1642        5242923  300240.835 10185605 -2316257.1 12802103\n1643        5230264  281614.449 10178913 -2338042.3 12798570\n1644        5218315  264262.359 10172367 -2358254.5 12794884\n1645        5206996  248018.257 10165973 -2377105.9 12791098\n1646        5196237  232739.539 10159735 -2394777.4 12787252\n1647        5185976  218303.555 10153648 -2411423.3 12783375\n1648        5176157  204604.519 10147709 -2427176.2 12779489\n1649        5166730  191550.947 10141909 -2442149.6 12775609\n1650        5157652  179063.533 10136240 -2456441.8 12771745\n1651        5148883  167073.369 10130693 -2470137.5 12767904\n1652        5140390  155520.457 10125259 -2483310.0 12764090\n1653        5132141  144352.456 10119929 -2496023.2 12760305\n1654        5124109  133523.627 10114694 -2508332.6 12756550\n1655        5116270  122993.931 10109545 -2520286.5 12752826\n1656        5108602  112728.272 10104475 -2531927.3 12749131\n1657        5101086  102695.843 10099476 -2543292.0 12745464\n1658        5093705   92869.570 10094541 -2554412.8 12741823\n1659        5086444   83225.634 10089663 -2565318.4 12738207\n1660        5079290   73743.056 10084837 -2576033.5 12734614\n1661        5072231   64403.347 10080058 -2586580.3 12731042\n1662        5065255   55190.196 10075321 -2596978.1 12727489\n1663        5058355   46089.202 10070620 -2607243.9 12723953\n1664        5051520   37087.645 10065953 -2617392.7 12720434\n1665        5044745   28174.281 10061316 -2627437.9 12716928\n1666        5038022   19339.168 10056705 -2637391.1 12713435\n1667        5031346   10573.508 10052118 -2647262.7 12709954\n1668        5024710    1869.514 10047551 -2657061.9 12706483\n1669        5018112   -6779.708 10043004 -2666796.7 12703021\n1670        5011546  -15380.266 10038473 -2676474.4 12699567\n1671        5005009  -23937.570 10033956 -2686101.2 12696120\n1672        4998498  -32456.417 10029452 -2695682.8 12692679\n1673        4992010  -40941.058 10024960 -2705224.2 12689243\n1674        4985542  -49395.262 10020478 -2714729.8 12685813\n1675        4979091  -57822.372 10016005 -2724203.5 12682386\n1676        4972657  -66225.353 10011540 -2733648.7 12678963\n1677        4966238  -74606.836 10007082 -2743068.7 12675544\n1678        4959830  -82969.154 10002630 -2752465.9 12672127\n1679        4953434  -91314.378  9998183 -2761843.0 12668712\n1680        4947048  -99644.346  9993741 -2771202.1 12665299\n1681        4940671 -107960.688  9989303 -2780544.9 12661887\n1682        4934302 -116264.854  9984868 -2789873.3 12658477\n1683        4927939 -124558.127  9980437 -2799188.8 12655068\n1684        4921583 -132841.649  9976008 -2808492.6 12651659\n1685        4915233 -141116.432  9971582 -2817786.0 12648251\n1686        4908887 -149383.377  9967157 -2827070.0 12644844\n1687        4902546 -157643.281  9962735 -2836345.5 12641437\n1688        4896208 -165896.853  9958313 -2845613.4 12638030\n1689        4889874 -174144.724  9953893 -2854874.3 12634622\n1690        4883543 -182387.452  9949473 -2864129.0 12631215\n1691        4877215 -190625.535  9945055 -2873378.1 12627807\n1692        4870889 -198859.412  9940637 -2882622.0 12624399\n1693        4864565 -207089.476  9936219 -2891861.1 12620991\n1694        4858243 -215316.075  9931802 -2901096.0 12617582\n1695        4851923 -223539.518  9927385 -2910327.0 12614173\n1696        4845604 -231760.080  9922968 -2919554.3 12610763\n1697        4839287 -239978.003  9918551 -2928778.3 12607352\n1698        4832971 -248193.506  9914135 -2937999.3 12603940\n1699        4826655 -256406.780  9909718 -2947217.3 12600528\n1700        4820341 -264617.997  9905300 -2956432.8 12597115\n1701        4814028 -272827.309  9900883 -2965645.7 12593701\n1702        4807715 -281034.850  9896465 -2974856.3 12590286\n1703        4801403 -289240.742  9892047 -2984064.8 12586871\n1704        4795092 -297445.089  9887628 -2993271.2 12583454\n1705        4788781 -305647.988  9883209 -3002475.6 12580037\n1706        4782470 -313849.523  9878790 -3011678.3 12576619\n1707        4776160 -322049.769  9874370 -3020879.1 12573200\n1708        4769851 -330248.791  9869950 -3030078.3 12569779\n1709        4763541 -338446.651  9865529 -3039275.8 12566358\n1710        4757232 -346643.400  9861108 -3048471.9 12562936\n1711        4750923 -354839.085  9856686 -3057666.4 12559513\n1712        4744615 -363033.748  9852263 -3066859.5 12556089\n1713        4738306 -371227.427  9847840 -3076051.2 12552664\n1714        4731998 -379420.154  9843416 -3085241.5 12549237\n1715        4725690 -387611.959  9838992 -3094430.5 12545810\n1716        4719382 -395802.868  9834567 -3103618.2 12542382\n1717        4713074 -403992.904  9830141 -3112804.6 12538953\n1718        4706766 -412182.089  9825715 -3121989.8 12535523\n1719        4700459 -420370.441  9821288 -3131173.7 12532091\n1720        4694151 -428557.977  9816861 -3140356.5 12528659\n1721        4687844 -436744.712  9812433 -3149538.1 12525226\n1722        4681537 -444930.658  9808004 -3158718.5 12521792\n1723        4675229 -453115.828  9803574 -3167897.7 12518356\n1724        4668922 -461300.233  9799144 -3177075.8 12514920\n1725        4662615 -469483.881  9794713 -3186252.8 12511482\n1726        4656308 -477666.782  9790282 -3195428.7 12508044\n1727        4650001 -485848.943  9785850 -3204603.4 12504604\n1728        4643693 -494030.371  9781417 -3213777.1 12501164\n1729        4637386 -502211.072  9776984 -3222949.6 12497722\n1730        4631079 -510391.052  9772550 -3232121.1 12494280\n1731        4624772 -518570.315  9768115 -3241291.4 12490836\n1732        4618465 -526748.868  9763680 -3250460.7 12487391\n1733        4612158 -534926.712  9759243 -3259629.0 12483946\n1734        4605851 -543103.853  9754807 -3268796.1 12480499\n1735        4599544 -551280.294  9750369 -3277962.2 12477051\n1736        4593238 -559456.038  9745931 -3287127.3 12473602\n1737        4586931 -567631.087  9741492 -3296291.2 12470153\n1738        4580624 -575805.445  9737053 -3305454.2 12466702\n1739        4574317 -583979.113  9732613 -3314616.0 12463250\n1740        4568010 -592152.095  9728172 -3323776.9 12459797\n1741        4561703 -600324.391  9723731 -3332936.7 12456343\n1742        4555396 -608496.004  9719288 -3342095.4 12452888\n1743        4549089 -616666.936  9714846 -3351253.1 12449432\n1744        4542782 -624837.188  9710402 -3360409.8 12445975\n1745        4536476 -633006.762  9705958 -3369565.4 12442517\n1746        4530169 -641175.660  9701513 -3378720.0 12439058\n1747        4523862 -649343.881  9697068 -3387873.6 12435597\n1748        4517555 -657511.429  9692622 -3397026.2 12432136\n1749        4511248 -665678.304  9688175 -3406177.7 12428674\n1750        4504941 -673844.508  9683727 -3415328.2 12425211\n1751        4498635 -682010.041  9679279 -3424477.6 12421747\n1752        4492328 -690174.904  9674830 -3433626.1 12418282\n1753        4486021 -698339.099  9670381 -3442773.5 12414815\n1754        4479714 -706502.627  9665931 -3451919.9 12411348\n1755        4473407 -714665.488  9661480 -3461065.3 12407880\n1756        4467100 -722827.684  9657028 -3470209.6 12404410\n1757        4460794 -730989.215  9652576 -3479353.0 12400940\n1758        4454487 -739150.082  9648124 -3488495.3 12397469\n1759        4448180 -747310.287  9643670 -3497636.7 12393996\n1760        4441873 -755469.829  9639216 -3506777.0 12390523\n1761        4435566 -763628.711  9634761 -3515916.3 12387049\n1762        4429259 -771786.932  9630306 -3525054.6 12383573\n1763        4422953 -779944.493  9625850 -3534191.8 12380097\n1764        4416646 -788101.396  9621393 -3543328.1 12376620\n1765        4410339 -796257.640  9616936 -3552463.4 12373141\n\n\nCode\nfcast0202\n\n\n     Point Forecast     Lo 80    Hi 80     Lo 95    Hi 95\n1626       24510562  -7744678 56765803 -24819571 73840695\n1627       25459984 -10852111 61772079 -30074573 80994542\n1628       25920865 -11633627 63475357 -31513774 83355504\n1629       26157614 -11915239 64230466 -32069790 84385017\n1630       26288281 -12065931 64642494 -32369425 84945988\n1631       26365940 -12174203 64906083 -32576123 85308002\n1632       26414767 -12264725 65094260 -32740412 85569947\n1633       26446063 -12345775 65237901 -32880933 85773060\n1634       26465352 -12420851 65351556 -33005964 85936668\n1635       26475543 -12491833 65442919 -33119916 86071002\n1636       26478335 -12559926 65516595 -33225533 86182202\n1637       26474846 -12625986 65575678 -33324716 86274408\n1638       26465898 -12690648 65622445 -33418872 86350669\n1639       26452146 -12754391 65658682 -33509078 86413369\n1640       26434137 -12817572 65685845 -33596171 86464445\n1641       26412345 -12880457 65705148 -33680811 86505502\n1642       26387188 -12943243 65717618 -33763515 86537890\n1643       26359031 -13006069 65724130 -33844694 86562755\n1644       26328201 -13069034 65725436 -33924670 86581072\n1645       26294989 -13132204 65722182 -34003699 86593678\n1646       26259654 -13195622 65714929 -34081983 86601291\n1647       26222425 -13259312 65704162 -34159682 86604532\n1648       26183508 -13323286 65690303 -34236920 86603937\n1649       26143087 -13387545 65673719 -34313797 86599971\n1650       26101325 -13452082 65654731 -34390391 86593040\n1651       26058366 -13516889 65633621 -34466763 86583495\n1652       26014341 -13581951 65610633 -34542962 86571644\n1653       25969365 -13647254 65585985 -34619026 86557757\n1654       25923542 -13712781 65559866 -34694984 86542068\n1655       25876964 -13778516 65532444 -34770859 86524786\n1656       25829712 -13844441 65503865 -34846669 86506093\n1657       25781859 -13910542 65474260 -34922430 86486148\n1658       25733471 -13976802 65443744 -34998151 86465093\n1659       25684606 -14043207 65412419 -35073840 86443052\n1660       25635315 -14109743 65380373 -35149506 86420136\n1661       25585645 -14176398 65347688 -35225152 86396442\n1662       25535636 -14243160 65314432 -35300782 86372055\n1663       25485326 -14310017 65280670 -35376400 86347053\n1664       25434748 -14376961 65246456 -35452006 86321502\n1665       25383929 -14443982 65211840 -35527604 86295462\n1666       25332897 -14511071 65176865 -35603194 86268988\n1667       25281674 -14578222 65141570 -35678776 86242125\n1668       25230282 -14645427 65105991 -35754352 86214916\n1669       25178738 -14712680 65070156 -35829921 86187397\n1670       25127059 -14779976 65034094 -35905484 86159603\n1671       25075260 -14847310 64997830 -35981041 86131562\n1672       25023354 -14914677 64961384 -36056593 86103300\n1673       24971352 -14982073 64924776 -36132138 86074841\n1674       24919264 -15049494 64888023 -36207677 86046205\n1675       24867101 -15116938 64851140 -36283210 86017412\n1676       24814870 -15184402 64814141 -36358737 85988476\n1677       24762578 -15251882 64777037 -36434257 85959413\n1678       24710232 -15319376 64739841 -36509771 85930236\n1679       24657839 -15386883 64702560 -36585278 85900955\n1680       24605403 -15454400 64665205 -36660778 85871583\n1681       24552928 -15521925 64627781 -36736271 85842127\n1682       24500419 -15589458 64590297 -36811757 85812596\n1683       24447881 -15656997 64552758 -36887236 85782997\n1684       24395315 -15724540 64515169 -36962708 85753337\n1685       24342725 -15792087 64477536 -37038172 85723622\n1686       24290113 -15859636 64439862 -37113629 85693855\n1687       24237482 -15927187 64402152 -37189079 85664043\n1688       24184834 -15994739 64364408 -37264521 85634190\n1689       24132171 -16062291 64326634 -37339955 85604298\n1690       24079495 -16129843 64288833 -37415381 85574371\n1691       24026806 -16197395 64251007 -37490800 85544412\n1692       23974107 -16264944 64213157 -37566211 85514424\n1693       23921397 -16332492 64175287 -37641614 85484409\n1694       23868680 -16400038 64137397 -37717009 85454369\n1695       23815954 -16467581 64099490 -37792396 85424305\n1696       23763222 -16535121 64061565 -37867776 85394220\n1697       23710484 -16602659 64023626 -37943147 85364115\n1698       23657740 -16670192 63985673 -38018510 85333991\n1699       23604992 -16737723 63947706 -38093866 85303849\n1700       23552239 -16805249 63909727 -38169213 85273690\n1701       23499482 -16872772 63871736 -38244552 85243516\n1702       23446722 -16940290 63833734 -38319883 85213327\n1703       23393959 -17007805 63795722 -38395207 85183124\n1704       23341193 -17075315 63757700 -38470522 85152907\n1705       23288424 -17142820 63719669 -38545829 85122678\n1706       23235654 -17210321 63681629 -38621127 85092435\n1707       23182882 -17277818 63643581 -38696418 85062181\n1708       23130107 -17345309 63605524 -38771701 85031916\n1709       23077332 -17412796 63567460 -38846976 85001639\n1710       23024555 -17480279 63529388 -38922242 84971352\n1711       22971777 -17547756 63491309 -38997501 84941054\n1712       22918997 -17615228 63453223 -39072751 84910745\n1713       22866217 -17682696 63415129 -39147993 84880427\n1714       22813436 -17750158 63377029 -39223227 84850099\n1715       22760654 -17817615 63338923 -39298454 84819761\n1716       22707871 -17885068 63300810 -39373672 84789414\n1717       22655088 -17952515 63262691 -39448882 84759058\n1718       22602304 -18019957 63224566 -39524084 84728692\n1719       22549520 -18087394 63186434 -39599278 84698317\n1720       22496735 -18154827 63148297 -39674463 84667934\n1721       22443950 -18222253 63110154 -39749641 84637541\n1722       22391165 -18289675 63072005 -39824811 84607140\n1723       22338379 -18357092 63033850 -39899973 84576731\n1724       22285593 -18424503 62995689 -39975126 84546312\n1725       22232807 -18491910 62957523 -40050272 84515885\n1726       22180020 -18559311 62919351 -40125410 84485450\n1727       22127234 -18626707 62881174 -40200539 84455006\n1728       22074447 -18694097 62842991 -40275661 84424554\n1729       22021660 -18761483 62804803 -40350775 84394094\n1730       21968873 -18828864 62766609 -40425881 84363626\n1731       21916085 -18896239 62728410 -40500978 84333149\n1732       21863298 -18963609 62690205 -40576068 84302664\n1733       21810510 -19030974 62651995 -40651150 84272171\n1734       21757723 -19098334 62613780 -40726224 84241670\n1735       21704935 -19165688 62575559 -40801290 84211160\n1736       21652148 -19233038 62537333 -40876348 84180643\n1737       21599360 -19300382 62499102 -40951398 84150118\n1738       21546572 -19367721 62460865 -41026440 84119584\n1739       21493784 -19435055 62422624 -41101474 84089043\n1740       21440996 -19502384 62384377 -41176501 84058493\n1741       21388208 -19569708 62346125 -41251519 84027936\n1742       21335420 -19637026 62307867 -41326530 83997370\n1743       21282632 -19704340 62269605 -41401532 83966797\n1744       21229844 -19771648 62231337 -41476527 83936216\n1745       21177056 -19838951 62193064 -41551514 83905627\n1746       21124268 -19906249 62154786 -41626493 83875030\n1747       21071480 -19973542 62116502 -41701464 83844425\n1748       21018692 -20040830 62078214 -41776428 83813812\n1749       20965904 -20108113 62039920 -41851384 83783191\n1750       20913116 -20175390 62001622 -41926331 83752563\n1751       20860328 -20242663 61963318 -42001271 83721926\n1752       20807539 -20309930 61925009 -42076204 83691282\n1753       20754751 -20377192 61886695 -42151128 83660630\n1754       20701963 -20444449 61848375 -42226045 83629970\n1755       20649175 -20511702 61810051 -42300953 83599303\n1756       20596387 -20578949 61771722 -42375854 83568628\n1757       20543598 -20646191 61733387 -42450748 83537944\n1758       20490810 -20713427 61695048 -42525633 83507254\n1759       20438022 -20780659 61656703 -42600511 83476555\n1760       20385234 -20847886 61618353 -42675381 83445848\n1761       20332445 -20915108 61579999 -42750243 83415134\n1762       20279657 -20982324 61541639 -42825098 83384412\n1763       20226869 -21049536 61503274 -42899945 83353683\n1764       20174081 -21116743 61464904 -42974784 83322946\n1765       20121293 -21183944 61426529 -43049616 83292201\n\n\nCode\nfcast0203\n\n\n     Point Forecast     Lo 80    Hi 80     Lo 95    Hi 95\n1626       13.07911 12.081253 14.07697 11.553019 14.60520\n1627       13.07911 12.028014 14.13021 11.471598 14.68662\n1628       13.07911 11.977345 14.18087 11.394106 14.76411\n1629       13.07911 11.928906 14.22931 11.320025 14.83820\n1630       13.07911 11.882426 14.27579 11.248940 14.90928\n1631       13.07911 11.837685 14.32053 11.180515 14.97771\n1632       13.07911 11.794502 14.36372 11.114471 15.04375\n1633       13.07911 11.752723 14.40550 11.050576 15.10764\n1634       13.07911 11.712221 14.44600 10.988634 15.16959\n1635       13.07911 11.672885 14.48534 10.928474 15.22975\n1636       13.07911 11.634620 14.52360 10.869953 15.28827\n1637       13.07911 11.597342 14.56088 10.812942 15.34528\n1638       13.07911 11.560980 14.59724 10.757331 15.40089\n1639       13.07911 11.525468 14.63275 10.703020 15.45520\n1640       13.07911 11.490751 14.66747 10.649924 15.50830\n1641       13.07911 11.456776 14.70144 10.597964 15.56026\n1642       13.07911 11.423498 14.73472 10.547070 15.61115\n1643       13.07911 11.390876 14.76734 10.497178 15.66104\n1644       13.07911 11.358872 14.79935 10.448233 15.70999\n1645       13.07911 11.327453 14.83077 10.400182 15.75804\n1646       13.07911 11.296588 14.86163 10.352978 15.80524\n1647       13.07911 11.266248 14.89197 10.306577 15.85164\n1648       13.07911 11.236408 14.92181 10.260940 15.89728\n1649       13.07911 11.207043 14.95118 10.216030 15.94219\n1650       13.07911 11.178132 14.98009 10.171815 15.98641\n1651       13.07911 11.149654 15.00857 10.128261 16.02996\n1652       13.07911 11.121590 15.03663 10.085341 16.07288\n1653       13.07911 11.093923 15.06430 10.043028 16.11519\n1654       13.07911 11.066636 15.09158 10.001297 16.15692\n1655       13.07911 11.039714 15.11851  9.960123 16.19810\n1656       13.07911 11.013143 15.14508  9.919487 16.23873\n1657       13.07911 10.986910 15.17131  9.879366 16.27885\n1658       13.07911 10.961001 15.19722  9.839742 16.31848\n1659       13.07911 10.935406 15.22281  9.800597 16.35762\n1660       13.07911 10.910112 15.24811  9.761914 16.39631\n1661       13.07911 10.885110 15.27311  9.723677 16.43454\n1662       13.07911 10.860390 15.29783  9.685871 16.47235\n1663       13.07911 10.835942 15.32228  9.648481 16.50974\n1664       13.07911 10.811758 15.34646  9.611494 16.54673\n1665       13.07911 10.787829 15.37039  9.574898 16.58332\n1666       13.07911 10.764147 15.39407  9.538680 16.61954\n1667       13.07911 10.740706 15.41751  9.502829 16.65539\n1668       13.07911 10.717497 15.44072  9.467334 16.69089\n1669       13.07911 10.694513 15.46371  9.432184 16.72604\n1670       13.07911 10.671749 15.48647  9.397370 16.76085\n1671       13.07911 10.649199 15.50902  9.362882 16.79534\n1672       13.07911 10.626856 15.53136  9.328711 16.82951\n1673       13.07911 10.604714 15.55351  9.294848 16.86337\n1674       13.07911 10.582769 15.57545  9.261286 16.89693\n1675       13.07911 10.561015 15.59720  9.228016 16.93020\n1676       13.07911 10.539448 15.61877  9.195032 16.96319\n1677       13.07911 10.518062 15.64016  9.162325 16.99590\n1678       13.07911 10.496853 15.66137  9.129889 17.02833\n1679       13.07911 10.475817 15.68240  9.097717 17.06050\n1680       13.07911 10.454950 15.70327  9.065803 17.09242\n1681       13.07911 10.434247 15.72397  9.034141 17.12408\n1682       13.07911 10.413705 15.74452  9.002725 17.15550\n1683       13.07911 10.393320 15.76490  8.971549 17.18667\n1684       13.07911 10.373089 15.78513  8.940608 17.21761\n1685       13.07911 10.353008 15.80521  8.909896 17.24832\n1686       13.07911 10.333073 15.82515  8.879409 17.27881\n1687       13.07911 10.313283 15.84494  8.849142 17.30908\n1688       13.07911 10.293633 15.86459  8.819090 17.33913\n1689       13.07911 10.274120 15.88410  8.789248 17.36897\n1690       13.07911 10.254743 15.90348  8.759613 17.39861\n1691       13.07911 10.235497 15.92272  8.730179 17.42804\n1692       13.07911 10.216381 15.94184  8.700944 17.45728\n1693       13.07911 10.197392 15.96083  8.671902 17.48632\n1694       13.07911 10.178527 15.97969  8.643050 17.51517\n1695       13.07911 10.159783 15.99844  8.614385 17.54383\n1696       13.07911 10.141160 16.01706  8.585903 17.57232\n1697       13.07911 10.122654 16.03557  8.557600 17.60062\n1698       13.07911 10.104263 16.05396  8.529473 17.62875\n1699       13.07911 10.085984 16.07224  8.501519 17.65670\n1700       13.07911 10.067817 16.09040  8.473735 17.68448\n1701       13.07911 10.049759 16.10846  8.446117 17.71210\n1702       13.07911 10.031808 16.12641  8.418663 17.73956\n1703       13.07911 10.013962 16.14426  8.391370 17.76685\n1704       13.07911  9.996219 16.16200  8.364235 17.79399\n1705       13.07911  9.978578 16.17964  8.337255 17.82097\n1706       13.07911  9.961036 16.19718  8.310428 17.84779\n1707       13.07911  9.943593 16.21463  8.283750 17.87447\n1708       13.07911  9.926246 16.23197  8.257221 17.90100\n1709       13.07911  9.908994 16.24923  8.230836 17.92738\n1710       13.07911  9.891836 16.26638  8.204595 17.95363\n1711       13.07911  9.874769 16.28345  8.178493 17.97973\n1712       13.07911  9.857793 16.30043  8.152531 18.00569\n1713       13.07911  9.840906 16.31731  8.126704 18.03152\n1714       13.07911  9.824106 16.33411  8.101011 18.05721\n1715       13.07911  9.807393 16.35083  8.075450 18.08277\n1716       13.07911  9.790764 16.36746  8.050019 18.10820\n1717       13.07911  9.774220 16.38400  8.024716 18.13350\n1718       13.07911  9.757757 16.40046  7.999539 18.15868\n1719       13.07911  9.741376 16.41684  7.974487 18.18373\n1720       13.07911  9.725075 16.43315  7.949556 18.20866\n1721       13.07911  9.708853 16.44937  7.924746 18.23347\n1722       13.07911  9.692708 16.46551  7.900055 18.25816\n1723       13.07911  9.676640 16.48158  7.875482 18.28274\n1724       13.07911  9.660648 16.49757  7.851023 18.30720\n1725       13.07911  9.644730 16.51349  7.826679 18.33154\n1726       13.07911  9.628885 16.52933  7.802447 18.35577\n1727       13.07911  9.613113 16.54511  7.778325 18.37989\n1728       13.07911  9.597413 16.56081  7.754313 18.40391\n1729       13.07911  9.581782 16.57644  7.730409 18.42781\n1730       13.07911  9.566222 16.59200  7.706611 18.45161\n1731       13.07911  9.550730 16.60749  7.682918 18.47530\n1732       13.07911  9.535306 16.62291  7.659329 18.49889\n1733       13.07911  9.519948 16.63827  7.635842 18.52238\n1734       13.07911  9.504657 16.65356  7.612455 18.54576\n1735       13.07911  9.489430 16.66879  7.589169 18.56905\n1736       13.07911  9.474268 16.68395  7.565980 18.59224\n1737       13.07911  9.459170 16.69905  7.542889 18.61533\n1738       13.07911  9.444134 16.71409  7.519894 18.63833\n1739       13.07911  9.429160 16.72906  7.496994 18.66123\n1740       13.07911  9.414248 16.74397  7.474187 18.68403\n1741       13.07911  9.399395 16.75882  7.451472 18.70675\n1742       13.07911  9.384603 16.77362  7.428849 18.72937\n1743       13.07911  9.369869 16.78835  7.406316 18.75190\n1744       13.07911  9.355194 16.80303  7.383872 18.77435\n1745       13.07911  9.340576 16.81764  7.361516 18.79670\n1746       13.07911  9.326016 16.83220  7.339248 18.81897\n1747       13.07911  9.311511 16.84671  7.317065 18.84116\n1748       13.07911  9.297063 16.86116  7.294967 18.86325\n1749       13.07911  9.282669 16.87555  7.272954 18.88527\n1750       13.07911  9.268329 16.88989  7.251024 18.90720\n1751       13.07911  9.254044 16.90418  7.229176 18.92904\n1752       13.07911  9.239811 16.91841  7.207409 18.95081\n1753       13.07911  9.225631 16.93259  7.185723 18.97250\n1754       13.07911  9.211503 16.94672  7.164116 18.99410\n1755       13.07911  9.197427 16.96079  7.142587 19.01563\n1756       13.07911  9.183401 16.97482  7.121137 19.03708\n1757       13.07911  9.169425 16.98879  7.099764 19.05846\n1758       13.07911  9.155500 17.00272  7.078466 19.07975\n1759       13.07911  9.141624 17.01660  7.057244 19.10098\n1760       13.07911  9.127796 17.03042  7.036097 19.12212\n1761       13.07911  9.114017 17.04420  7.015023 19.14320\n1762       13.07911  9.100285 17.05794  6.994022 19.16420\n1763       13.07911  9.086600 17.07162  6.973093 19.18513\n1764       13.07911  9.072963 17.08526  6.952236 19.20598\n1765       13.07911  9.059371 17.09885  6.931450 19.22677\n\n\nCode\nfcast0305\n\n\n     Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n1626       98.86793 96.95649 100.7794 95.94463 101.7912\n1627       98.98513 96.48601 101.4843 95.16305 102.8072\n1628       98.82444 95.79562 101.8533 94.19225 103.4566\n1629       98.81411 95.33921 102.2890 93.49971 104.1285\n1630       98.97102 95.12694 102.8151 93.09201 104.8500\n1631       98.88669 94.69043 103.0830 92.46906 105.3043\n1632       98.78962 94.25608 103.3232 91.85617 105.7231\n1633       98.92720 94.09955 103.7548 91.54394 106.3104\n1634       98.93601 93.83026 104.0418 91.12745 106.7446\n1635       98.80128 93.41562 104.1869 90.56462 107.0379\n1636       98.87380 93.23216 104.5154 90.24566 107.5019\n1637       98.95707 93.07863 104.8355 89.96677 107.9474\n1638       98.83889 92.71948 104.9583 89.48006 108.1977\n1639       98.83142 92.48083 105.1820 89.11903 108.5438\n1640       98.94710 92.38476 105.5094 88.91087 108.9833\n1641       98.88474 92.11016 105.6593 88.52391 109.2456\n1642       98.81330 91.82702 105.7996 88.12870 109.4979\n1643       98.91483 91.73287 106.0968 87.93097 109.8987\n1644       98.92115 91.54823 106.2941 87.64524 110.1971\n1645       98.82183 91.25429 106.3894 87.24828 110.3954\n1646       98.87546 91.12359 106.6273 87.02001 110.7309\n1647       98.93674 91.00924 106.8642 86.81268 111.0608\n1648       98.84953 90.74277 106.9563 86.45131 111.2477\n1649       98.84417 90.56242 107.1259 86.17832 111.5100\n1650       98.92945 90.48279 107.3761 86.01140 111.8475\n1651       98.88333 90.27080 107.4959 85.71160 112.0551\n1652       98.83076 90.05203 107.6095 85.40485 112.2567\n1653       98.90568 89.96965 107.8417 85.23920 112.5722\n1654       98.91021 89.81930 108.0011 85.00687 112.8136\n1655       98.83700 89.58870 108.0853 84.69294 112.9811\n1656       98.87665 89.47689 108.2764 84.50096 113.2523\n1657       98.92175 89.37563 108.4679 84.32222 113.5213\n1658       98.85739 89.16253 108.5523 84.03038 113.6844\n1659       98.85356 89.01245 108.6947 83.80288 113.9042\n1660       98.91642 88.93524 108.8976 83.65153 114.1813\n1661       98.88232 88.76042 109.0042 83.40221 114.3624\n1662       98.84363 88.58076 109.1065 83.14792 114.5393\n1663       98.89892 88.50067 109.2972 82.99617 114.8017\n1664       98.90216 88.37008 109.4342 82.79473 115.0096\n1665       98.84819 88.18069 109.5157 82.53366 115.1627\n1666       98.87751 88.07841 109.6766 82.36171 115.3933\n1667       98.91070 87.98332 109.8381 82.19871 115.6227\n1668       98.86321 87.80606 109.9204 81.95277 115.7736\n1669       98.86047 87.67523 110.0457 81.75412 115.9668\n1670       98.90680 87.59755 110.2160 81.61080 116.2028\n1671       98.88159 87.44797 110.3152 81.39537 116.3678\n1672       98.85311 87.29504 110.4112 81.17657 116.5297\n1673       98.89392 87.21514 110.5727 81.03277 116.7551\n1674       98.89623 87.09785 110.6946 80.85217 116.9403\n1675       98.85645 86.93752 110.7754 80.62801 117.0849\n1676       98.87813 86.84125 110.9150 80.46931 117.2870\n1677       98.90256 86.75003 111.0551 80.31686 117.4883\n1678       98.86751 86.59844 111.1366 80.10358 117.6314\n1679       98.86555 86.48115 111.2499 79.92525 117.8059\n1680       98.89970 86.40280 111.3966 79.78734 118.0121\n1681       98.88106 86.27151 111.4906 79.59641 118.1657\n1682       98.86011 86.13796 111.5823 79.40326 118.3170\n1683       98.89022 86.05808 111.7224 79.26516 118.5153\n1684       98.89187 85.95059 111.8332 79.09990 118.6839\n1685       98.86255 85.81159 111.9135 78.90283 118.8223\n1686       98.87858 85.71981 112.0374 78.75398 119.0032\n1687       98.89656 85.63162 112.1615 78.60959 119.1835\n1688       98.87069 85.49910 112.2423 78.42060 119.3208\n1689       98.86929 85.39196 112.3466 78.25749 119.4811\n1690       98.89447 85.31341 112.4755 78.12404 119.6649\n1691       98.88068 85.19591 112.5655 77.95163 119.8097\n1692       98.86526 85.07691 112.6536 77.77780 119.9527\n1693       98.88748 84.99744 112.7775 77.64449 120.1305\n1694       98.88866 84.89759 112.8797 77.49117 120.2862\n1695       98.86705 84.77469 112.9594 77.31465 120.4194\n1696       98.87890 84.68663 113.0712 77.17369 120.5841\n1697       98.89213 84.60117 113.1831 77.03599 120.7483\n1698       98.87304 84.48315 113.2629 76.86561 120.8805\n1699       98.87204 84.38396 113.3601 76.71443 121.0297\n1700       98.89060 84.30580 113.4754 76.58507 121.1961\n1701       98.88041 84.19899 113.5618 76.42711 121.3337\n1702       98.86906 84.09121 113.6469 76.26829 121.4698\n1703       98.88546 84.01257 113.7583 76.13934 121.6316\n1704       98.88630 83.91893 113.8537 75.99568 121.7769\n1705       98.87037 83.80842 113.9323 75.83511 121.9056\n1706       98.87913 83.72366 114.0346 75.70083 122.0574\n1707       98.88887 83.64080 114.1369 75.56896 122.2088\n1708       98.87478 83.53405 114.2155 75.41315 122.3364\n1709       98.87407 83.44126 114.3069 75.27162 122.4765\n1710       98.88775 83.36398 114.4115 75.14619 122.6293\n1711       98.88021 83.26564 114.4948 74.99978 122.7606\n1712       98.87186 83.16670 114.5770 74.85290 122.8908\n1713       98.88396 83.08927 114.6787 74.72807 123.0399\n1714       98.88456 83.00081 114.7683 74.59246 123.1767\n1715       98.87282 82.90002 114.8456 74.44454 123.3011\n1716       98.87930 82.81827 114.9403 74.31608 123.4425\n1717       98.88646 82.73794 115.0350 74.18943 123.5835\n1718       98.87607 82.64007 115.1121 74.04526 123.7069\n1719       98.87556 82.55260 115.1985 73.91174 123.8394\n1720       98.88565 82.47657 115.2947 73.79012 123.9812\n1721       98.88007 82.38507 115.3751 73.65314 124.1070\n1722       98.87393 82.29323 115.4546 73.51593 124.2319\n1723       98.88286 82.21727 115.5484 73.39505 124.3707\n1724       98.88328 82.13322 115.6333 73.26628 124.5003\n1725       98.87463 82.04018 115.7091 73.12856 124.6207\n1726       98.87942 81.96123 115.7976 73.00528 124.7536\n1727       98.88469 81.88335 115.8860 72.88338 124.8860\n1728       98.87702 81.79262 115.9614 72.74868 125.0054\n1729       98.87666 81.70962 116.0437 72.62194 125.1314\n1730       98.88409 81.63509 116.1331 72.50402 125.2642\n1731       98.87997 81.54921 116.2107 72.37486 125.3851\n1732       98.87545 81.46316 116.2877 72.24565 125.5052\n1733       98.88204 81.38887 116.3752 72.12854 125.6355\n1734       98.88234 81.30864 116.4560 72.00569 125.7590\n1735       98.87596 81.22186 116.5301 71.87635 125.8756\n1736       98.87950 81.14553 116.6135 71.75773 126.0013\n1737       98.88338 81.07002 116.6967 71.64019 126.1266\n1738       98.87772 80.98510 116.7703 71.51331 126.2421\n1739       98.87747 80.90595 116.8490 71.39240 126.3625\n1740       98.88295 80.83307 116.9328 71.27804 126.4878\n1741       98.87990 80.75188 117.0079 71.15548 126.6043\n1742       98.87657 80.67063 117.0825 71.03299 126.7201\n1743       98.88143 80.59810 117.1648 70.91948 126.8434\n1744       98.88165 80.52123 117.2421 70.80181 126.9615\n1745       98.87694 80.43959 117.3143 70.67945 127.0744\n1746       98.87956 80.36572 117.3934 70.56508 127.1941\n1747       98.88242 80.29247 117.4724 70.45155 127.3133\n1748       98.87824 80.21236 117.5441 70.33123 127.4253\n1749       98.87806 80.13655 117.6196 70.21540 127.5407\n1750       98.88210 80.06541 117.6988 70.10446 127.6597\n1751       98.87985 79.98819 117.7715 69.98755 127.7722\n1752       98.87740 79.91099 117.8438 69.87077 127.8840\n1753       98.88099 79.84025 117.9217 69.76069 128.0013\n1754       98.88114 79.76635 117.9959 69.64760 128.1147\n1755       98.87767 79.68901 118.0663 69.53114 128.2242\n1756       98.87961 79.61743 118.1418 69.42065 128.3386\n1757       98.88171 79.54636 118.2171 69.31084 128.4526\n1758       98.87863 79.47026 118.2870 69.19610 128.5612\n1759       98.87850 79.39741 118.3596 69.08474 128.6723\n1760       98.88148 79.32803 118.4349 68.97706 128.7859\n1761       98.87981 79.25420 118.5054 68.86503 128.8946\n1762       98.87801 79.18046 118.5756 68.75320 129.0028\n1763       98.88066 79.11151 118.6498 68.64636 129.1150\n1764       98.88076 79.04028 118.7212 68.53736 129.2242\n1765       98.87821 78.96656 118.7899 68.42597 129.3304\n\n\nCode\nfcast0307\n\n\n     Point Forecast    Lo 80     Hi 80    Lo 95    Hi 95\n1626       97.46326 95.74322  99.18330 94.83268 100.0938\n1627       97.46326 95.03075  99.89576 93.74307 101.1835\n1628       97.46326 94.48406 100.44245 92.90697 102.0195\n1629       97.46326 94.02318 100.90334 92.20211 102.7244\n1630       97.46326 93.61713 101.30938 91.58112 103.3454\n1631       97.46326 93.25004 101.67648 91.01969 103.9068\n1632       97.46326 92.91246 102.01406 90.50341 104.4231\n1633       97.46326 92.59825 102.32827 90.02287 104.9036\n1634       97.46326 92.30314 102.62338 89.57154 105.3550\n1635       97.46326 92.02401 102.90250 89.14465 105.7819\n1636       97.46326 91.75853 103.16799 88.73863 106.1879\n1637       97.46326 91.50487 103.42165 88.35068 106.5758\n1638       97.46326 91.26157 103.66495 87.97859 106.9479\n1639       97.46326 91.02746 103.89906 87.62055 107.3060\n1640       97.46326 90.80157 104.12494 87.27509 107.6514\n1641       97.46326 90.58310 104.34342 86.94096 107.9856\n1642       97.46326 90.37135 104.55516 86.61712 108.3094\n1643       97.46326 90.16575 104.76077 86.30268 108.6238\n1644       97.46326 89.96578 104.96074 85.99685 108.9297\n1645       97.46326 89.77101 105.15551 85.69897 109.2275\n1646       97.46326 89.58105 105.34547 85.40845 109.5181\n1647       97.46326 89.39556 105.53096 85.12477 109.8017\n1648       97.46326 89.21424 105.71228 84.84747 110.0790\n1649       97.46326 89.03682 105.88970 84.57613 110.3504\n1650       97.46326 88.86306 106.06346 84.31039 110.6161\n1651       97.46326 88.69274 106.23378 84.04991 110.8766\n1652       97.46326 88.52567 106.40085 83.79440 111.1321\n1653       97.46326 88.36166 106.56485 83.54357 111.3829\n1654       97.46326 88.20056 106.72596 83.29718 111.6293\n1655       97.46326 88.04221 106.88430 83.05501 111.8715\n1656       97.46326 87.88648 107.04004 82.81684 112.1097\n1657       97.46326 87.73324 107.19327 82.58248 112.3440\n1658       97.46326 87.58238 107.34414 82.35176 112.5748\n1659       97.46326 87.43379 107.49273 82.12451 112.8020\n1660       97.46326 87.28737 107.63915 81.90057 113.0259\n1661       97.46326 87.14302 107.78350 81.67981 113.2467\n1662       97.46326 87.00066 107.92585 81.46210 113.4644\n1663       97.46326 86.86022 108.06630 81.24731 113.6792\n1664       97.46326 86.72161 108.20490 81.03533 113.8912\n1665       97.46326 86.58477 108.34175 80.82605 114.1005\n1666       97.46326 86.44963 108.47689 80.61937 114.3072\n1667       97.46326 86.31613 108.61039 80.41519 114.5113\n1668       97.46326 86.18420 108.74231 80.21343 114.7131\n1669       97.46326 86.05380 108.87271 80.01400 114.9125\n1670       97.46326 85.92488 109.00164 79.81683 115.1097\n1671       97.46326 85.79738 109.12914 79.62184 115.3047\n1672       97.46326 85.67126 109.25526 79.42895 115.4976\n1673       97.46326 85.54647 109.38004 79.23811 115.6884\n1674       97.46326 85.42298 109.50354 79.04924 115.8773\n1675       97.46326 85.30074 109.62578 78.86229 116.0642\n1676       97.46326 85.17972 109.74680 78.67720 116.2493\n1677       97.46326 85.05987 109.86664 78.49392 116.4326\n1678       97.46326 84.94118 109.98534 78.31239 116.6141\n1679       97.46326 84.82360 110.10292 78.13257 116.7940\n1680       97.46326 84.70710 110.21942 77.95440 116.9721\n1681       97.46326 84.59166 110.33486 77.77785 117.1487\n1682       97.46326 84.47724 110.44927 77.60286 117.3237\n1683       97.46326 84.36383 110.56269 77.42940 117.4971\n1684       97.46326 84.25138 110.67514 77.25744 117.6691\n1685       97.46326 84.13989 110.78663 77.08692 117.8396\n1686       97.46326 84.02932 110.89720 76.91782 118.0087\n1687       97.46326 83.91965 111.00687 76.75010 118.1764\n1688       97.46326 83.81087 111.11565 76.58372 118.3428\n1689       97.46326 83.70294 111.22358 76.41867 118.5079\n1690       97.46326 83.59585 111.33066 76.25489 118.6716\n1691       97.46326 83.48959 111.43693 76.09237 118.8341\n1692       97.46326 83.38413 111.54239 75.93108 118.9954\n1693       97.46326 83.27945 111.64707 75.77099 119.1555\n1694       97.46326 83.17553 111.75098 75.61207 119.3144\n1695       97.46326 83.07237 111.85414 75.45430 119.4722\n1696       97.46326 82.96994 111.95657 75.29765 119.6289\n1697       97.46326 82.86824 112.05828 75.14210 119.7844\n1698       97.46326 82.76723 112.15929 74.98762 119.9389\n1699       97.46326 82.66692 112.25960 74.83421 120.0923\n1700       97.46326 82.56728 112.35924 74.68182 120.2447\n1701       97.46326 82.46830 112.45822 74.53045 120.3961\n1702       97.46326 82.36997 112.55655 74.38007 120.5465\n1703       97.46326 82.27228 112.65424 74.23066 120.6959\n1704       97.46326 82.17521 112.75131 74.08221 120.8443\n1705       97.46326 82.07875 112.84776 73.93469 120.9918\n1706       97.46326 81.98290 112.94362 73.78809 121.1384\n1707       97.46326 81.88763 113.03888 73.64240 121.2841\n1708       97.46326 81.79295 113.13357 73.49759 121.4289\n1709       97.46326 81.69883 113.22768 73.35365 121.5729\n1710       97.46326 81.60527 113.32124 73.21056 121.7160\n1711       97.46326 81.51226 113.41425 73.06832 121.8582\n1712       97.46326 81.41979 113.50672 72.92690 121.9996\n1713       97.46326 81.32785 113.59866 72.78629 122.1402\n1714       97.46326 81.23643 113.69008 72.64647 122.2800\n1715       97.46326 81.14553 113.78099 72.50744 122.4191\n1716       97.46326 81.05512 113.87139 72.36918 122.5573\n1717       97.46326 80.96522 113.96130 72.23168 122.6948\n1718       97.46326 80.87579 114.05072 72.09492 122.8316\n1719       97.46326 80.78685 114.13966 71.95890 122.9676\n1720       97.46326 80.69838 114.22813 71.82359 123.1029\n1721       97.46326 80.61038 114.31614 71.68900 123.2375\n1722       97.46326 80.52283 114.40369 71.55511 123.3714\n1723       97.46326 80.43573 114.49078 71.42190 123.5046\n1724       97.46326 80.34908 114.57744 71.28938 123.6371\n1725       97.46326 80.26286 114.66366 71.15752 123.7690\n1726       97.46326 80.17707 114.74945 71.02632 123.9002\n1727       97.46326 80.09171 114.83481 70.89576 124.0308\n1728       97.46326 80.00676 114.91976 70.76585 124.1607\n1729       97.46326 79.92222 115.00429 70.63656 124.2900\n1730       97.46326 79.83809 115.08842 70.50790 124.4186\n1731       97.46326 79.75436 115.17215 70.37984 124.5467\n1732       97.46326 79.67103 115.25549 70.25239 124.6741\n1733       97.46326 79.58808 115.33844 70.12553 124.8010\n1734       97.46326 79.50551 115.42100 69.99926 124.9273\n1735       97.46326 79.42333 115.50319 69.87357 125.0530\n1736       97.46326 79.34151 115.58500 69.74844 125.1781\n1737       97.46326 79.26007 115.66645 69.62388 125.3026\n1738       97.46326 79.17898 115.74753 69.49987 125.4266\n1739       97.46326 79.09826 115.82826 69.37641 125.5501\n1740       97.46326 79.01789 115.90863 69.25350 125.6730\n1741       97.46326 78.93786 115.98865 69.13111 125.7954\n1742       97.46326 78.85818 116.06833 69.00925 125.9173\n1743       97.46326 78.77884 116.14767 68.88791 126.0386\n1744       97.46326 78.69984 116.22668 68.76708 126.1594\n1745       97.46326 78.62117 116.30535 68.64676 126.2798\n1746       97.46326 78.54282 116.38370 68.52694 126.3996\n1747       97.46326 78.46480 116.46172 68.40762 126.5189\n1748       97.46326 78.38709 116.53942 68.28878 126.6377\n1749       97.46326 78.30971 116.61681 68.17043 126.7561\n1750       97.46326 78.23263 116.69389 68.05255 126.8740\n1751       97.46326 78.15586 116.77066 67.93514 126.9914\n1752       97.46326 78.07939 116.84712 67.81820 127.1083\n1753       97.46326 78.00323 116.92329 67.70171 127.2248\n1754       97.46326 77.92736 116.99916 67.58568 127.3408\n1755       97.46326 77.85179 117.07473 67.47010 127.4564\n1756       97.46326 77.77650 117.15001 67.35496 127.5716\n1757       97.46326 77.70150 117.22501 67.24026 127.6863\n1758       97.46326 77.62679 117.29973 67.12600 127.8005\n1759       97.46326 77.55236 117.37416 67.01216 127.9144\n1760       97.46326 77.47820 117.44832 66.89875 128.0278\n1761       97.46326 77.40432 117.52220 66.78576 128.1408\n1762       97.46326 77.33071 117.59581 66.67318 128.2533\n1763       97.46326 77.25737 117.66915 66.56101 128.3655\n1764       97.46326 77.18429 117.74223 66.44925 128.4773\n1765       97.46326 77.11147 117.81504 66.33789 128.5886\n\n\nCode\nS0101 <- fcast0101$mean\nS0102 <- fcast0102$mean\nS0202 <- fcast0202$mean\nS0203 <- fcast0203$mean\nS0305 <- fcast0305$mean\nS0307 <- fcast0307$mean\nS0101_preds <- S0101[1:140]\nS0102_preds <- S0102[1:140]\nS0202_preds <- S0202[1:140]\nS0203_preds <- S0203[1:140]\nS0305_preds <- S0305[1:140]\nS0307_preds <- S0307[1:140]\ncsv <- data.frame(cbind(S0101_preds, S0102_preds, S0202_preds, S0203_preds,S0305_preds, S0307_preds))\n# write.csv(csv, file = \"C:/data/csv.csv\")"
  }
]